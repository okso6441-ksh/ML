{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, is_train=True):\n",
    "    \n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "\n",
    "    if is_train==True:          \n",
    "    \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill')\n",
    "        temp = temp.dropna()\n",
    "        \n",
    "        return temp.iloc[:-96]\n",
    "\n",
    "    elif is_train==False:\n",
    "        \n",
    "        temp = temp[['Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "                              \n",
    "        return temp.iloc[-48:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train/train.csv')\n",
    "\n",
    "test = []\n",
    "\n",
    "for i in range(81):\n",
    "    file_path = './data/test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    test.append(temp)\n",
    "\n",
    "df_test = pd.concat(test)\n",
    "\n",
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52464, 9), (3888, 7))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = preprocess_data(train)\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = df_train[['Hour','DHI','DNI','WS','RH','T']].min()\n",
    "max  = df_train[['Hour','DHI','DNI','WS','RH','T']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(['Hour','DHI','DNI','WS','RH','T']):\n",
    "    df_train[col] = (df_train[col] - min[i]) / (max[i] - min[i])\n",
    "    df_test[col] = (df_test[col] - min[i]) / (max[i] - min[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Day  = df_train.iloc[:, :-2]\n",
    "Day7 = df_train.iloc[:, -2]\n",
    "Day8 = df_train.iloc[:, -1]\n",
    "Day78 = df_train.iloc[:, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(q, y, pred):\n",
    "    err = (y-pred)\n",
    "    return mean(maximum(q*err, (q-1)*err), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_lst = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39348, 7), (13116, 7), (39348, 2), (13116, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(Day, Day78, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train_1, X_valid_1, Y_train_1, Y_valid_1 = train_test_split(Day, Day7, test_size=0.25, random_state=42)\n",
    "X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(Day, Day8, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import mean, maximum\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(Day.shape)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "594/615 [===========================>..] - ETA: 0s - loss: 291.6936- ETA: 0s - loss: 3WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 288.9031 - val_loss: 175.3714\n",
      "Epoch 2/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 167.2054 - val_loss: 157.4137\n",
      "Epoch 3/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 151.8488 - val_loss: 149.4633\n",
      "Epoch 4/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 152.9240 - val_loss: 147.5404\n",
      "Epoch 5/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 147.1542 - val_loss: 147.0453\n",
      "Epoch 6/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 148.7233 - val_loss: 150.9186\n",
      "Epoch 7/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 148.6483 - val_loss: 143.8951\n",
      "Epoch 8/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 140.5893 - val_loss: 150.6173\n",
      "Epoch 9/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 146.3554 - val_loss: 144.0072\n",
      "Epoch 10/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 146.2887 - val_loss: 139.8799\n",
      "Epoch 11/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 150.1109 - val_loss: 146.7265\n",
      "Epoch 12/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 142.5601 - val_loss: 139.6149\n",
      "Epoch 13/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 141.1732 - val_loss: 137.6176\n",
      "Epoch 14/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 142.5086 - val_loss: 136.3390\n",
      "Epoch 15/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 135.8996 - val_loss: 136.7445\n",
      "Epoch 16/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 142.1282 - val_loss: 138.0264\n",
      "Epoch 17/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 144.0342 - val_loss: 138.8077\n",
      "Epoch 00017: early stopping\n",
      "410/410 [==============================] - 0s 592us/step - loss: 136.8887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136.88865661621094"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "hist = model.fit(X_train, Y_train, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "model.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'], 'b-', label='loss')\n",
    "plt.plot(hist.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "818/820 [============================>.] - ETA: 0s - loss: 1.4682WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4681 - val_loss: 1.6004\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4143 - val_loss: 1.6053\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4049 - val_loss: 1.6004\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3980 - val_loss: 1.5957\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4069 - val_loss: 1.6062\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4008 - val_loss: 1.6007\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4062 - val_loss: 1.6079\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "776/820 [===========================>..] - ETA: 0s - loss: 2.2852WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.2845 - val_loss: 2.6155\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2590 - val_loss: 2.6059\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2588 - val_loss: 2.6173\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2487 - val_loss: 2.6073\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2652 - val_loss: 2.6031\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2613 - val_loss: 2.5965\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2715 - val_loss: 2.6025\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2820 - val_loss: 2.6522\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2628 - val_loss: 2.5925\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2913 - val_loss: 2.5932\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2269 - val_loss: 2.5923\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2307 - val_loss: 2.6007\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2542 - val_loss: 2.6361\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2387 - val_loss: 2.5853\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2591 - val_loss: 2.5926\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2591 - val_loss: 2.6142\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2550 - val_loss: 2.6211\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "784/820 [===========================>..] - ETA: 0s - loss: 2.6888WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.6885 - val_loss: 3.0402\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6644 - val_loss: 3.0814\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6628 - val_loss: 3.1096\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6544 - val_loss: 3.0558\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "801/820 [============================>.] - ETA: 0s - loss: 2.8013WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.8011 - val_loss: 3.1546\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7635 - val_loss: 3.1592\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7690 - val_loss: 3.2131\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7584 - val_loss: 3.1577\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "805/820 [============================>.] - ETA: 0s - loss: 2.6597WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.6596 - val_loss: 2.9887\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6307 - val_loss: 3.0048\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6354 - val_loss: 3.0164\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6357 - val_loss: 3.0659\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "788/820 [===========================>..] - ETA: 0s - loss: 2.3519WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.3520 - val_loss: 2.6637\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3270 - val_loss: 2.6938\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3314 - val_loss: 2.6977\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3360 - val_loss: 2.6881\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "806/820 [============================>.] - ETA: 0s - loss: 1.9273WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 1.9274 - val_loss: 2.1777\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9148 - val_loss: 2.1820\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9134 - val_loss: 2.1690\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9166 - val_loss: 2.1630\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9197 - val_loss: 2.1873\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9121 - val_loss: 2.1579\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9359 - val_loss: 2.1958\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9544 - val_loss: 2.2247\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9017 - val_loss: 2.1967\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "790/820 [===========================>..] - ETA: 0s - loss: 1.3976WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 1.3978 - val_loss: 1.6126\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3921 - val_loss: 1.6408\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3953 - val_loss: 1.5795\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3955 - val_loss: 1.5719\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3875 - val_loss: 1.5851\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3854 - val_loss: 1.5637\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4060 - val_loss: 1.5873\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4203 - val_loss: 1.5981\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3803 - val_loss: 1.5624\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4100 - val_loss: 1.5634\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3603 - val_loss: 1.5826\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3830 - val_loss: 1.5755\n",
      "Epoch 00012: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "815/820 [============================>.] - ETA: 0s - loss: 0.7764WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7764 - val_loss: 0.8627\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7749 - val_loss: 0.9403\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7780 - val_loss: 0.8891\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7742 - val_loss: 0.8783\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_10_input'), name='dense_10_input', description=\"created by layer 'dense_10_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3888, 18)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    model.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model.fit(Day, Day78, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred = pd.DataFrame(model.predict(df_test))\n",
    "    results = pd.concat([results, pred], axis=1)\n",
    "\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(             0         0         0         0         0         0         0  \\\n",
       " 0    -0.007208 -0.007240  0.005783  0.016488  0.009341  0.004064  0.029597   \n",
       " 1    -0.006766 -0.007369  0.005188  0.016327  0.009297  0.003761  0.029509   \n",
       " 2    -0.005782 -0.005189  0.003775  0.016418  0.011415  0.006870  0.029207   \n",
       " 3    -0.005958 -0.005104  0.003940  0.016418  0.011611  0.007090  0.029363   \n",
       " 4    -0.008584 -0.002786  0.003562  0.016114  0.013767  0.010487  0.029069   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 3883 -0.014827  0.004310  0.000880  0.014228 -0.002549  0.013747  0.020386   \n",
       " 3884 -0.014616  0.002194  0.000956  0.015063 -0.003505  0.014016  0.019748   \n",
       " 3885 -0.015728  0.002448  0.001592  0.015489 -0.003203  0.014439  0.019969   \n",
       " 3886 -0.015695  0.000360  0.011812  0.017918 -0.000447  0.019413  0.023538   \n",
       " 3887 -0.017725  0.000719  0.010601  0.017597 -0.000041  0.019596  0.023822   \n",
       " \n",
       "              0         0  \n",
       " 0    -0.019634  0.012591  \n",
       " 1    -0.019493  0.012424  \n",
       " 2    -0.015663  0.012966  \n",
       " 3    -0.015648  0.012735  \n",
       " 4    -0.011966  0.013513  \n",
       " ...        ...       ...  \n",
       " 3883 -0.000713  0.012145  \n",
       " 3884 -0.000044  0.010139  \n",
       " 3885 -0.000204  0.010528  \n",
       " 3886  0.000802  0.009252  \n",
       " 3887  0.000563  0.010392  \n",
       " \n",
       " [3888 rows x 9 columns],\n",
       "              1         1         1         1         1         1         1  \\\n",
       " 0     0.011690 -0.017089  0.029111  0.006058  0.004984  0.010467  0.021861   \n",
       " 1     0.011878 -0.017034  0.028693  0.006178  0.004879  0.010237  0.022031   \n",
       " 2     0.010963 -0.014039  0.024870  0.007491  0.007486  0.013313  0.024795   \n",
       " 3     0.011021 -0.014077  0.025049  0.007227  0.007699  0.013488  0.024837   \n",
       " 4     0.007468 -0.011199  0.021855  0.008137  0.010685  0.016720  0.027206   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 3883 -0.016284  0.000994  0.009535  0.010912  0.000382  0.012936  0.014531   \n",
       " 3884 -0.014964 -0.002996  0.007766  0.008940 -0.004833  0.009918  0.012189   \n",
       " 3885 -0.016066 -0.002625  0.008751  0.009568 -0.004169  0.010625  0.012632   \n",
       " 3886 -0.014873 -0.006587  0.021567  0.007743 -0.005774  0.011624  0.015271   \n",
       " 3887 -0.016788 -0.006065  0.019511  0.006606 -0.004857  0.012325  0.015873   \n",
       " \n",
       "              1         1  \n",
       " 0    -0.031724  0.013497  \n",
       " 1    -0.031348  0.013104  \n",
       " 2    -0.023865  0.014404  \n",
       " 3    -0.023961  0.014138  \n",
       " 4    -0.016948  0.016449  \n",
       " ...        ...       ...  \n",
       " 3883 -0.003726  0.017738  \n",
       " 3884 -0.006119  0.014837  \n",
       " 3885 -0.006326  0.015168  \n",
       " 3886 -0.007895  0.013508  \n",
       " 3887 -0.008173  0.014990  \n",
       " \n",
       " [3888 rows x 9 columns])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0], results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34889\n",
      "Early stopping, best iteration is:\n",
      "[418]\tvalid_0's quantile: 1.34812\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.14466\n",
      "[1000]\tvalid_0's quantile: 2.13764\n",
      "[1500]\tvalid_0's quantile: 2.13582\n",
      "[2000]\tvalid_0's quantile: 2.1334\n",
      "Early stopping, best iteration is:\n",
      "[1749]\tvalid_0's quantile: 2.13312\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.53565\n",
      "[1000]\tvalid_0's quantile: 2.50726\n",
      "[1500]\tvalid_0's quantile: 2.49215\n",
      "Early stopping, best iteration is:\n",
      "[1604]\tvalid_0's quantile: 2.48959\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.66191\n",
      "[1000]\tvalid_0's quantile: 2.62846\n",
      "[1500]\tvalid_0's quantile: 2.61266\n",
      "[2000]\tvalid_0's quantile: 2.6059\n",
      "[2500]\tvalid_0's quantile: 2.59923\n",
      "[3000]\tvalid_0's quantile: 2.59644\n",
      "Early stopping, best iteration is:\n",
      "[2707]\tvalid_0's quantile: 2.59598\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.55537\n",
      "[1000]\tvalid_0's quantile: 2.54183\n",
      "[1500]\tvalid_0's quantile: 2.52395\n",
      "[2000]\tvalid_0's quantile: 2.5191\n",
      "[2500]\tvalid_0's quantile: 2.51606\n",
      "[3000]\tvalid_0's quantile: 2.51386\n",
      "[3500]\tvalid_0's quantile: 2.5086\n",
      "[4000]\tvalid_0's quantile: 2.50447\n",
      "[4500]\tvalid_0's quantile: 2.50257\n",
      "[5000]\tvalid_0's quantile: 2.50037\n",
      "[5500]\tvalid_0's quantile: 2.49801\n",
      "[6000]\tvalid_0's quantile: 2.49694\n",
      "[6500]\tvalid_0's quantile: 2.49596\n",
      "Early stopping, best iteration is:\n",
      "[6563]\tvalid_0's quantile: 2.49582\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.28838\n",
      "[1000]\tvalid_0's quantile: 2.26798\n",
      "[1500]\tvalid_0's quantile: 2.25775\n",
      "[2000]\tvalid_0's quantile: 2.25273\n",
      "[2500]\tvalid_0's quantile: 2.24936\n",
      "[3000]\tvalid_0's quantile: 2.24642\n",
      "[3500]\tvalid_0's quantile: 2.24451\n",
      "[4000]\tvalid_0's quantile: 2.24427\n",
      "Early stopping, best iteration is:\n",
      "[3735]\tvalid_0's quantile: 2.24391\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.87856\n",
      "[1000]\tvalid_0's quantile: 1.86236\n",
      "[1500]\tvalid_0's quantile: 1.85737\n",
      "[2000]\tvalid_0's quantile: 1.85491\n",
      "Early stopping, best iteration is:\n",
      "[1840]\tvalid_0's quantile: 1.85404\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34949\n",
      "[1000]\tvalid_0's quantile: 1.34333\n",
      "Early stopping, best iteration is:\n",
      "[898]\tvalid_0's quantile: 1.34271\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.748569\n",
      "[1000]\tvalid_0's quantile: 0.746233\n",
      "[1500]\tvalid_0's quantile: 0.74548\n",
      "Early stopping, best iteration is:\n",
      "[1284]\tvalid_0's quantile: 0.745153\n",
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.39392\n",
      "Early stopping, best iteration is:\n",
      "[331]\tvalid_0's quantile: 1.3933\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.19349\n",
      "Early stopping, best iteration is:\n",
      "[616]\tvalid_0's quantile: 2.18577\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.61892\n",
      "[1000]\tvalid_0's quantile: 2.57944\n",
      "[1500]\tvalid_0's quantile: 2.578\n",
      "Early stopping, best iteration is:\n",
      "[1380]\tvalid_0's quantile: 2.57785\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.7433\n",
      "[1000]\tvalid_0's quantile: 2.70695\n",
      "[1500]\tvalid_0's quantile: 2.70092\n",
      "[2000]\tvalid_0's quantile: 2.69464\n",
      "[2500]\tvalid_0's quantile: 2.68927\n",
      "[3000]\tvalid_0's quantile: 2.68204\n",
      "[3500]\tvalid_0's quantile: 2.67218\n",
      "[4000]\tvalid_0's quantile: 2.66515\n",
      "[4500]\tvalid_0's quantile: 2.66079\n",
      "Early stopping, best iteration is:\n",
      "[4475]\tvalid_0's quantile: 2.66079\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.6458\n",
      "[1000]\tvalid_0's quantile: 2.62814\n",
      "[1500]\tvalid_0's quantile: 2.61626\n",
      "[2000]\tvalid_0's quantile: 2.60023\n",
      "[2500]\tvalid_0's quantile: 2.58706\n",
      "[3000]\tvalid_0's quantile: 2.58403\n",
      "Early stopping, best iteration is:\n",
      "[2807]\tvalid_0's quantile: 2.58403\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.36786\n",
      "[1000]\tvalid_0's quantile: 2.35343\n",
      "[1500]\tvalid_0's quantile: 2.33476\n",
      "[2000]\tvalid_0's quantile: 2.32426\n",
      "[2500]\tvalid_0's quantile: 2.31984\n",
      "[3000]\tvalid_0's quantile: 2.31736\n",
      "Early stopping, best iteration is:\n",
      "[2871]\tvalid_0's quantile: 2.31729\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.96038\n",
      "[1000]\tvalid_0's quantile: 1.93778\n",
      "[1500]\tvalid_0's quantile: 1.92682\n",
      "[2000]\tvalid_0's quantile: 1.91583\n",
      "[2500]\tvalid_0's quantile: 1.91245\n",
      "Early stopping, best iteration is:\n",
      "[2560]\tvalid_0's quantile: 1.91213\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.40785\n",
      "[1000]\tvalid_0's quantile: 1.39944\n",
      "[1500]\tvalid_0's quantile: 1.39438\n",
      "[2000]\tvalid_0's quantile: 1.39224\n",
      "[2500]\tvalid_0's quantile: 1.39124\n",
      "Early stopping, best iteration is:\n",
      "[2308]\tvalid_0's quantile: 1.39107\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.780444\n",
      "[1000]\tvalid_0's quantile: 0.777171\n",
      "[1500]\tvalid_0's quantile: 0.775623\n",
      "[2000]\tvalid_0's quantile: 0.775235\n",
      "Early stopping, best iteration is:\n",
      "[1831]\tvalid_0's quantile: 0.774894\n"
     ]
    }
   ],
   "source": [
    "def LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "    \n",
    "    # (a) Modeling  \n",
    "    model = LGBMRegressor(objective='quantile', alpha=q,\n",
    "                         n_estimators=10000, bagging_fraction=0.7, learning_rate=0.027, subsample=0.7)                   \n",
    "                         \n",
    "                         \n",
    "    model.fit(X_train, Y_train, eval_metric = ['quantile'], \n",
    "          eval_set=[(X_valid, Y_valid)], early_stopping_rounds=300, verbose=500)\n",
    "\n",
    "    # (b) Predictions\n",
    "    pred = pd.Series(model.predict(X_test).round(2))\n",
    "    return pred, model\n",
    "\n",
    "def train_data(X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "\n",
    "    LGBM_models=[]\n",
    "    LGBM_actual_pred = pd.DataFrame()\n",
    "\n",
    "    for q in q_lst:\n",
    "        print(q)\n",
    "        pred , model = LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test)\n",
    "        LGBM_models.append(model)\n",
    "        LGBM_actual_pred = pd.concat([LGBM_actual_pred,pred],axis=1)\n",
    "\n",
    "    LGBM_actual_pred.columns=q_lst\n",
    "    \n",
    "    return LGBM_models, LGBM_actual_pred\n",
    "\n",
    "models_1, results_1 = train_data(X_train_1, Y_train_1, X_valid_1, Y_valid_1, df_test)\n",
    "models_2, results_2 = train_data(X_train_2, Y_train_2, X_valid_2, Y_valid_2, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_L0 = pd.DataFrame(results_1.sort_index())\n",
    "res_L0.columns = ['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']\n",
    "res_L1 = pd.DataFrame(results_1.sort_index())\n",
    "res_L1.columns = ['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']\n",
    "\n",
    "res_D0 = pd.DataFrame(results[0].sort_index())\n",
    "res_D0.columns = ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9']\n",
    "res_D1 = pd.DataFrame(results[1].sort_index())\n",
    "res_D1.columns = ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0 = pd.concat([res_L0, res_D0], axis=1)\n",
    "res_1 = pd.concat([res_L1, res_D1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0.loc[res_0[res_0['L00.1'] == 0].index, ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9']] = 0\n",
    "res_1.loc[res_1[res_1['L10.1'] == 0].index, ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    res_0[\"L00.\"+str(i)] = (res_0[\"L00.\"+str(i)] + res_0[\"D00.\"+str(i)])/2\n",
    "    res_1[\"L10.\"+str(i)] = (res_1[\"L10.\"+str(i)] + res_1[\"D10.\"+str(i)])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.csv_Day7_2h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.csv_Day7_3h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.csv_Day7_3h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.csv_Day7_4h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.csv_Day7_4h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.csv_Day7_5h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.csv_Day7_5h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.csv_Day7_6h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.csv_Day7_6h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.csv_Day7_7h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.csv_Day7_7h30m</td>\n",
       "      <td>0.710383</td>\n",
       "      <td>1.257184</td>\n",
       "      <td>1.129719</td>\n",
       "      <td>1.532850</td>\n",
       "      <td>2.895748</td>\n",
       "      <td>3.065999</td>\n",
       "      <td>5.617217</td>\n",
       "      <td>6.877531</td>\n",
       "      <td>9.315586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.csv_Day7_8h00m</td>\n",
       "      <td>2.052315</td>\n",
       "      <td>5.234605</td>\n",
       "      <td>5.293047</td>\n",
       "      <td>6.756347</td>\n",
       "      <td>7.644313</td>\n",
       "      <td>9.619443</td>\n",
       "      <td>12.553109</td>\n",
       "      <td>15.351528</td>\n",
       "      <td>20.223162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.csv_Day7_8h30m</td>\n",
       "      <td>2.360454</td>\n",
       "      <td>6.674595</td>\n",
       "      <td>7.101134</td>\n",
       "      <td>8.538914</td>\n",
       "      <td>9.383190</td>\n",
       "      <td>11.101244</td>\n",
       "      <td>15.096229</td>\n",
       "      <td>18.767547</td>\n",
       "      <td>24.087223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.csv_Day7_9h00m</td>\n",
       "      <td>5.690312</td>\n",
       "      <td>14.135590</td>\n",
       "      <td>14.797799</td>\n",
       "      <td>17.117285</td>\n",
       "      <td>18.087472</td>\n",
       "      <td>19.916203</td>\n",
       "      <td>19.556289</td>\n",
       "      <td>24.038186</td>\n",
       "      <td>32.756288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.csv_Day7_9h30m</td>\n",
       "      <td>7.609080</td>\n",
       "      <td>17.354929</td>\n",
       "      <td>15.910891</td>\n",
       "      <td>19.909402</td>\n",
       "      <td>21.147041</td>\n",
       "      <td>21.717200</td>\n",
       "      <td>21.970226</td>\n",
       "      <td>25.045669</td>\n",
       "      <td>33.014029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.csv_Day7_10h00m</td>\n",
       "      <td>13.111211</td>\n",
       "      <td>26.205452</td>\n",
       "      <td>25.541880</td>\n",
       "      <td>28.464625</td>\n",
       "      <td>30.016293</td>\n",
       "      <td>33.439690</td>\n",
       "      <td>33.028609</td>\n",
       "      <td>31.949986</td>\n",
       "      <td>35.883344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.csv_Day7_10h30m</td>\n",
       "      <td>14.130914</td>\n",
       "      <td>26.119877</td>\n",
       "      <td>27.354091</td>\n",
       "      <td>29.851715</td>\n",
       "      <td>32.408141</td>\n",
       "      <td>34.544084</td>\n",
       "      <td>36.468533</td>\n",
       "      <td>34.963052</td>\n",
       "      <td>35.888322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.csv_Day7_11h00m</td>\n",
       "      <td>16.502776</td>\n",
       "      <td>29.080038</td>\n",
       "      <td>33.301503</td>\n",
       "      <td>34.422623</td>\n",
       "      <td>37.070670</td>\n",
       "      <td>39.784595</td>\n",
       "      <td>40.010417</td>\n",
       "      <td>38.511805</td>\n",
       "      <td>43.658671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.csv_Day7_11h30m</td>\n",
       "      <td>13.978701</td>\n",
       "      <td>25.308782</td>\n",
       "      <td>27.829533</td>\n",
       "      <td>29.827054</td>\n",
       "      <td>31.469281</td>\n",
       "      <td>36.108546</td>\n",
       "      <td>39.561488</td>\n",
       "      <td>34.717557</td>\n",
       "      <td>44.077841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.csv_Day7_12h00m</td>\n",
       "      <td>14.969767</td>\n",
       "      <td>25.358585</td>\n",
       "      <td>29.464869</td>\n",
       "      <td>30.719707</td>\n",
       "      <td>33.475804</td>\n",
       "      <td>34.474189</td>\n",
       "      <td>34.574149</td>\n",
       "      <td>33.119050</td>\n",
       "      <td>39.952225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.csv_Day7_12h30m</td>\n",
       "      <td>15.549378</td>\n",
       "      <td>27.316133</td>\n",
       "      <td>32.297145</td>\n",
       "      <td>33.099210</td>\n",
       "      <td>33.566296</td>\n",
       "      <td>37.018753</td>\n",
       "      <td>37.806076</td>\n",
       "      <td>35.811545</td>\n",
       "      <td>40.854097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.csv_Day7_13h00m</td>\n",
       "      <td>13.833743</td>\n",
       "      <td>22.767601</td>\n",
       "      <td>28.076743</td>\n",
       "      <td>30.160896</td>\n",
       "      <td>32.907303</td>\n",
       "      <td>34.082884</td>\n",
       "      <td>34.068159</td>\n",
       "      <td>33.226633</td>\n",
       "      <td>37.396584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.csv_Day7_13h30m</td>\n",
       "      <td>11.554113</td>\n",
       "      <td>21.972812</td>\n",
       "      <td>26.052547</td>\n",
       "      <td>28.918272</td>\n",
       "      <td>29.945225</td>\n",
       "      <td>31.722059</td>\n",
       "      <td>32.717139</td>\n",
       "      <td>32.793216</td>\n",
       "      <td>43.367675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.csv_Day7_14h00m</td>\n",
       "      <td>9.193644</td>\n",
       "      <td>18.507741</td>\n",
       "      <td>21.873290</td>\n",
       "      <td>23.948612</td>\n",
       "      <td>25.113868</td>\n",
       "      <td>26.961390</td>\n",
       "      <td>26.355809</td>\n",
       "      <td>27.880537</td>\n",
       "      <td>33.477349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.csv_Day7_14h30m</td>\n",
       "      <td>9.398386</td>\n",
       "      <td>19.491702</td>\n",
       "      <td>22.583577</td>\n",
       "      <td>23.928516</td>\n",
       "      <td>24.588163</td>\n",
       "      <td>25.571531</td>\n",
       "      <td>27.874150</td>\n",
       "      <td>28.465745</td>\n",
       "      <td>32.549082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.csv_Day7_15h00m</td>\n",
       "      <td>6.006129</td>\n",
       "      <td>13.911311</td>\n",
       "      <td>17.483428</td>\n",
       "      <td>18.319894</td>\n",
       "      <td>20.408498</td>\n",
       "      <td>20.889166</td>\n",
       "      <td>23.721348</td>\n",
       "      <td>25.075890</td>\n",
       "      <td>27.736509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.csv_Day7_15h30m</td>\n",
       "      <td>5.058706</td>\n",
       "      <td>13.668221</td>\n",
       "      <td>13.222881</td>\n",
       "      <td>15.059928</td>\n",
       "      <td>15.400882</td>\n",
       "      <td>16.931569</td>\n",
       "      <td>18.929443</td>\n",
       "      <td>20.511927</td>\n",
       "      <td>25.164407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.csv_Day7_16h00m</td>\n",
       "      <td>2.937015</td>\n",
       "      <td>6.494632</td>\n",
       "      <td>7.203380</td>\n",
       "      <td>7.781691</td>\n",
       "      <td>9.075378</td>\n",
       "      <td>9.404808</td>\n",
       "      <td>9.795269</td>\n",
       "      <td>12.808344</td>\n",
       "      <td>15.968061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.csv_Day7_16h30m</td>\n",
       "      <td>1.462753</td>\n",
       "      <td>4.136407</td>\n",
       "      <td>3.499740</td>\n",
       "      <td>3.323693</td>\n",
       "      <td>3.601853</td>\n",
       "      <td>4.701259</td>\n",
       "      <td>6.053561</td>\n",
       "      <td>7.905532</td>\n",
       "      <td>15.046466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.csv_Day7_17h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.csv_Day7_17h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.csv_Day7_18h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.csv_Day7_18h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.csv_Day7_19h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.csv_Day7_19h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.csv_Day7_20h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.csv_Day7_20h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.csv_Day7_21h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.csv_Day7_21h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.csv_Day7_22h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.csv_Day7_22h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.csv_Day7_23h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.csv_Day7_23h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id      q_0.1      q_0.2      q_0.3      q_0.4      q_0.5  \\\n",
       "0    0.csv_Day7_0h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1    0.csv_Day7_0h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2    0.csv_Day7_1h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3    0.csv_Day7_1h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4    0.csv_Day7_2h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5    0.csv_Day7_2h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6    0.csv_Day7_3h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7    0.csv_Day7_3h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8    0.csv_Day7_4h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9    0.csv_Day7_4h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10   0.csv_Day7_5h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11   0.csv_Day7_5h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12   0.csv_Day7_6h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13   0.csv_Day7_6h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14   0.csv_Day7_7h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15   0.csv_Day7_7h30m   0.710383   1.257184   1.129719   1.532850   2.895748   \n",
       "16   0.csv_Day7_8h00m   2.052315   5.234605   5.293047   6.756347   7.644313   \n",
       "17   0.csv_Day7_8h30m   2.360454   6.674595   7.101134   8.538914   9.383190   \n",
       "18   0.csv_Day7_9h00m   5.690312  14.135590  14.797799  17.117285  18.087472   \n",
       "19   0.csv_Day7_9h30m   7.609080  17.354929  15.910891  19.909402  21.147041   \n",
       "20  0.csv_Day7_10h00m  13.111211  26.205452  25.541880  28.464625  30.016293   \n",
       "21  0.csv_Day7_10h30m  14.130914  26.119877  27.354091  29.851715  32.408141   \n",
       "22  0.csv_Day7_11h00m  16.502776  29.080038  33.301503  34.422623  37.070670   \n",
       "23  0.csv_Day7_11h30m  13.978701  25.308782  27.829533  29.827054  31.469281   \n",
       "24  0.csv_Day7_12h00m  14.969767  25.358585  29.464869  30.719707  33.475804   \n",
       "25  0.csv_Day7_12h30m  15.549378  27.316133  32.297145  33.099210  33.566296   \n",
       "26  0.csv_Day7_13h00m  13.833743  22.767601  28.076743  30.160896  32.907303   \n",
       "27  0.csv_Day7_13h30m  11.554113  21.972812  26.052547  28.918272  29.945225   \n",
       "28  0.csv_Day7_14h00m   9.193644  18.507741  21.873290  23.948612  25.113868   \n",
       "29  0.csv_Day7_14h30m   9.398386  19.491702  22.583577  23.928516  24.588163   \n",
       "30  0.csv_Day7_15h00m   6.006129  13.911311  17.483428  18.319894  20.408498   \n",
       "31  0.csv_Day7_15h30m   5.058706  13.668221  13.222881  15.059928  15.400882   \n",
       "32  0.csv_Day7_16h00m   2.937015   6.494632   7.203380   7.781691   9.075378   \n",
       "33  0.csv_Day7_16h30m   1.462753   4.136407   3.499740   3.323693   3.601853   \n",
       "34  0.csv_Day7_17h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  0.csv_Day7_17h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  0.csv_Day7_18h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  0.csv_Day7_18h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  0.csv_Day7_19h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  0.csv_Day7_19h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  0.csv_Day7_20h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  0.csv_Day7_20h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  0.csv_Day7_21h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  0.csv_Day7_21h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  0.csv_Day7_22h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  0.csv_Day7_22h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  0.csv_Day7_23h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  0.csv_Day7_23h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        q_0.6      q_0.7      q_0.8      q_0.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   3.065999   5.617217   6.877531   9.315586  \n",
       "16   9.619443  12.553109  15.351528  20.223162  \n",
       "17  11.101244  15.096229  18.767547  24.087223  \n",
       "18  19.916203  19.556289  24.038186  32.756288  \n",
       "19  21.717200  21.970226  25.045669  33.014029  \n",
       "20  33.439690  33.028609  31.949986  35.883344  \n",
       "21  34.544084  36.468533  34.963052  35.888322  \n",
       "22  39.784595  40.010417  38.511805  43.658671  \n",
       "23  36.108546  39.561488  34.717557  44.077841  \n",
       "24  34.474189  34.574149  33.119050  39.952225  \n",
       "25  37.018753  37.806076  35.811545  40.854097  \n",
       "26  34.082884  34.068159  33.226633  37.396584  \n",
       "27  31.722059  32.717139  32.793216  43.367675  \n",
       "28  26.961390  26.355809  27.880537  33.477349  \n",
       "29  25.571531  27.874150  28.465745  32.549082  \n",
       "30  20.889166  23.721348  25.075890  27.736509  \n",
       "31  16.931569  18.929443  20.511927  25.164407  \n",
       "32   9.404808   9.795269  12.808344  15.968061  \n",
       "33   4.701259   6.053561   7.905532  15.046466  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = res_0[['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']].values\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = res_1[['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']].values\n",
    "submission[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission/submission_20210118-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45\n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51\n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11\n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34\n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51\n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17\n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03\n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17\n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82\n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14\n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61\n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92\n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20\n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73\n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94\n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20\n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59\n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49\n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13\n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_1.sort_index()[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.054383</td>\n",
       "      <td>-0.077396</td>\n",
       "      <td>0.017636</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>-0.017260</td>\n",
       "      <td>-0.023554</td>\n",
       "      <td>0.023951</td>\n",
       "      <td>0.034546</td>\n",
       "      <td>0.015895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.054989</td>\n",
       "      <td>-0.078058</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>-0.017288</td>\n",
       "      <td>-0.023530</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>0.035055</td>\n",
       "      <td>0.015970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.056124</td>\n",
       "      <td>-0.076407</td>\n",
       "      <td>0.012608</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>-0.017544</td>\n",
       "      <td>-0.025520</td>\n",
       "      <td>0.019945</td>\n",
       "      <td>0.026681</td>\n",
       "      <td>0.018218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.056871</td>\n",
       "      <td>-0.076160</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>-0.017368</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>0.019986</td>\n",
       "      <td>0.026664</td>\n",
       "      <td>0.017985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.054549</td>\n",
       "      <td>-0.075217</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>-0.017373</td>\n",
       "      <td>-0.025870</td>\n",
       "      <td>0.015958</td>\n",
       "      <td>0.012309</td>\n",
       "      <td>0.015488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.054482</td>\n",
       "      <td>-0.075539</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>-0.017683</td>\n",
       "      <td>-0.025759</td>\n",
       "      <td>0.015988</td>\n",
       "      <td>0.012422</td>\n",
       "      <td>0.015672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.052897</td>\n",
       "      <td>-0.071629</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>-0.003630</td>\n",
       "      <td>-0.017824</td>\n",
       "      <td>-0.025654</td>\n",
       "      <td>0.013116</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.012949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.050297</td>\n",
       "      <td>-0.071649</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>-0.005247</td>\n",
       "      <td>-0.018835</td>\n",
       "      <td>-0.025467</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>-0.001571</td>\n",
       "      <td>0.013606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.043220</td>\n",
       "      <td>-0.067566</td>\n",
       "      <td>0.006367</td>\n",
       "      <td>-0.009477</td>\n",
       "      <td>-0.019822</td>\n",
       "      <td>-0.025210</td>\n",
       "      <td>0.010773</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>0.013564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.046112</td>\n",
       "      <td>-0.067906</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>-0.020418</td>\n",
       "      <td>-0.023944</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>-0.000724</td>\n",
       "      <td>0.013921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.044195</td>\n",
       "      <td>-0.063328</td>\n",
       "      <td>0.014459</td>\n",
       "      <td>-0.008567</td>\n",
       "      <td>-0.020768</td>\n",
       "      <td>-0.023074</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.014361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.044968</td>\n",
       "      <td>-0.063955</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>-0.007284</td>\n",
       "      <td>-0.020191</td>\n",
       "      <td>-0.022290</td>\n",
       "      <td>0.006718</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.014304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.045505</td>\n",
       "      <td>-0.064582</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>-0.008212</td>\n",
       "      <td>-0.021068</td>\n",
       "      <td>-0.025874</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.012829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.045458</td>\n",
       "      <td>-0.061935</td>\n",
       "      <td>0.011022</td>\n",
       "      <td>-0.007144</td>\n",
       "      <td>-0.020495</td>\n",
       "      <td>-0.025431</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.012875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.053835</td>\n",
       "      <td>-0.064915</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>-0.008344</td>\n",
       "      <td>-0.021174</td>\n",
       "      <td>-0.028270</td>\n",
       "      <td>0.007496</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>0.012505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.480766</td>\n",
       "      <td>0.804367</td>\n",
       "      <td>1.009437</td>\n",
       "      <td>1.405700</td>\n",
       "      <td>1.691496</td>\n",
       "      <td>2.841998</td>\n",
       "      <td>4.194435</td>\n",
       "      <td>6.345063</td>\n",
       "      <td>9.181172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.074630</td>\n",
       "      <td>5.189209</td>\n",
       "      <td>3.766095</td>\n",
       "      <td>4.612694</td>\n",
       "      <td>4.448627</td>\n",
       "      <td>9.298885</td>\n",
       "      <td>10.746218</td>\n",
       "      <td>13.193055</td>\n",
       "      <td>16.936323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.360908</td>\n",
       "      <td>8.229190</td>\n",
       "      <td>6.112268</td>\n",
       "      <td>7.307828</td>\n",
       "      <td>7.376381</td>\n",
       "      <td>12.782488</td>\n",
       "      <td>14.812457</td>\n",
       "      <td>17.435095</td>\n",
       "      <td>21.064445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.710624</td>\n",
       "      <td>14.561180</td>\n",
       "      <td>12.065598</td>\n",
       "      <td>14.274570</td>\n",
       "      <td>15.324944</td>\n",
       "      <td>22.272406</td>\n",
       "      <td>25.762577</td>\n",
       "      <td>28.166372</td>\n",
       "      <td>32.172577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.728159</td>\n",
       "      <td>16.829859</td>\n",
       "      <td>13.981781</td>\n",
       "      <td>16.238804</td>\n",
       "      <td>18.034082</td>\n",
       "      <td>25.094400</td>\n",
       "      <td>27.990452</td>\n",
       "      <td>29.321339</td>\n",
       "      <td>32.518059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.032422</td>\n",
       "      <td>23.690905</td>\n",
       "      <td>21.573759</td>\n",
       "      <td>23.289249</td>\n",
       "      <td>26.672586</td>\n",
       "      <td>33.469379</td>\n",
       "      <td>36.227219</td>\n",
       "      <td>37.029972</td>\n",
       "      <td>38.596687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.351828</td>\n",
       "      <td>23.249754</td>\n",
       "      <td>22.878181</td>\n",
       "      <td>26.063431</td>\n",
       "      <td>30.046282</td>\n",
       "      <td>35.618168</td>\n",
       "      <td>38.187065</td>\n",
       "      <td>36.416103</td>\n",
       "      <td>37.746643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.415552</td>\n",
       "      <td>25.810076</td>\n",
       "      <td>27.963005</td>\n",
       "      <td>30.775246</td>\n",
       "      <td>34.041340</td>\n",
       "      <td>41.889191</td>\n",
       "      <td>43.480835</td>\n",
       "      <td>42.513611</td>\n",
       "      <td>50.147343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.217402</td>\n",
       "      <td>24.167564</td>\n",
       "      <td>25.379066</td>\n",
       "      <td>27.624107</td>\n",
       "      <td>31.148561</td>\n",
       "      <td>40.437092</td>\n",
       "      <td>41.202976</td>\n",
       "      <td>40.425114</td>\n",
       "      <td>48.335682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.109533</td>\n",
       "      <td>23.117170</td>\n",
       "      <td>26.679737</td>\n",
       "      <td>29.879414</td>\n",
       "      <td>31.591608</td>\n",
       "      <td>38.458378</td>\n",
       "      <td>40.798298</td>\n",
       "      <td>38.298100</td>\n",
       "      <td>45.764450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.798756</td>\n",
       "      <td>23.242266</td>\n",
       "      <td>27.564289</td>\n",
       "      <td>30.948420</td>\n",
       "      <td>32.562592</td>\n",
       "      <td>39.057507</td>\n",
       "      <td>41.612152</td>\n",
       "      <td>39.613091</td>\n",
       "      <td>45.098194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.717486</td>\n",
       "      <td>20.775202</td>\n",
       "      <td>25.443485</td>\n",
       "      <td>28.751791</td>\n",
       "      <td>30.554605</td>\n",
       "      <td>35.555767</td>\n",
       "      <td>38.046318</td>\n",
       "      <td>37.313267</td>\n",
       "      <td>42.873169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.678226</td>\n",
       "      <td>21.505625</td>\n",
       "      <td>24.105095</td>\n",
       "      <td>27.986544</td>\n",
       "      <td>29.820450</td>\n",
       "      <td>34.604118</td>\n",
       "      <td>39.064278</td>\n",
       "      <td>38.436432</td>\n",
       "      <td>46.535351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.547288</td>\n",
       "      <td>17.725483</td>\n",
       "      <td>20.116579</td>\n",
       "      <td>23.347223</td>\n",
       "      <td>25.457735</td>\n",
       "      <td>28.782780</td>\n",
       "      <td>32.501617</td>\n",
       "      <td>33.851074</td>\n",
       "      <td>38.224697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.166773</td>\n",
       "      <td>17.433405</td>\n",
       "      <td>20.277153</td>\n",
       "      <td>23.937033</td>\n",
       "      <td>26.356325</td>\n",
       "      <td>29.223063</td>\n",
       "      <td>33.168301</td>\n",
       "      <td>34.101490</td>\n",
       "      <td>38.158165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.892257</td>\n",
       "      <td>12.332622</td>\n",
       "      <td>15.076857</td>\n",
       "      <td>19.009789</td>\n",
       "      <td>22.126995</td>\n",
       "      <td>24.418331</td>\n",
       "      <td>26.842695</td>\n",
       "      <td>27.721781</td>\n",
       "      <td>30.273018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.207412</td>\n",
       "      <td>12.266441</td>\n",
       "      <td>13.195762</td>\n",
       "      <td>16.659857</td>\n",
       "      <td>18.211763</td>\n",
       "      <td>20.173138</td>\n",
       "      <td>22.898886</td>\n",
       "      <td>25.183853</td>\n",
       "      <td>27.738813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.364031</td>\n",
       "      <td>4.959263</td>\n",
       "      <td>6.216760</td>\n",
       "      <td>7.283382</td>\n",
       "      <td>8.400757</td>\n",
       "      <td>10.049616</td>\n",
       "      <td>12.040539</td>\n",
       "      <td>14.636688</td>\n",
       "      <td>16.446121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.475507</td>\n",
       "      <td>3.872814</td>\n",
       "      <td>3.739480</td>\n",
       "      <td>3.767387</td>\n",
       "      <td>4.253705</td>\n",
       "      <td>5.282517</td>\n",
       "      <td>7.747121</td>\n",
       "      <td>11.481064</td>\n",
       "      <td>12.962932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.008009</td>\n",
       "      <td>-0.049788</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>-0.011683</td>\n",
       "      <td>-0.025514</td>\n",
       "      <td>-0.024529</td>\n",
       "      <td>0.017446</td>\n",
       "      <td>0.008195</td>\n",
       "      <td>0.019340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.010483</td>\n",
       "      <td>-0.049493</td>\n",
       "      <td>-0.005264</td>\n",
       "      <td>-0.015214</td>\n",
       "      <td>-0.029195</td>\n",
       "      <td>-0.026546</td>\n",
       "      <td>0.017653</td>\n",
       "      <td>0.007123</td>\n",
       "      <td>0.018086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.011386</td>\n",
       "      <td>-0.043699</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>-0.019729</td>\n",
       "      <td>-0.039483</td>\n",
       "      <td>-0.049527</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.016109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.011307</td>\n",
       "      <td>-0.043807</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>-0.019798</td>\n",
       "      <td>-0.039323</td>\n",
       "      <td>-0.048894</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.008803</td>\n",
       "      <td>0.015715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.002113</td>\n",
       "      <td>-0.042092</td>\n",
       "      <td>-0.006321</td>\n",
       "      <td>-0.016995</td>\n",
       "      <td>-0.043729</td>\n",
       "      <td>-0.053568</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.014582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.002532</td>\n",
       "      <td>-0.041698</td>\n",
       "      <td>-0.007065</td>\n",
       "      <td>-0.017063</td>\n",
       "      <td>-0.043602</td>\n",
       "      <td>-0.053440</td>\n",
       "      <td>0.017552</td>\n",
       "      <td>0.008758</td>\n",
       "      <td>0.014786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.005869</td>\n",
       "      <td>-0.054863</td>\n",
       "      <td>-0.004829</td>\n",
       "      <td>-0.012315</td>\n",
       "      <td>-0.044470</td>\n",
       "      <td>-0.056016</td>\n",
       "      <td>0.016332</td>\n",
       "      <td>0.008738</td>\n",
       "      <td>0.015297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.006571</td>\n",
       "      <td>-0.054277</td>\n",
       "      <td>-0.004157</td>\n",
       "      <td>-0.012108</td>\n",
       "      <td>-0.044326</td>\n",
       "      <td>-0.055941</td>\n",
       "      <td>0.017792</td>\n",
       "      <td>0.008218</td>\n",
       "      <td>0.015598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.006715</td>\n",
       "      <td>-0.055998</td>\n",
       "      <td>0.009565</td>\n",
       "      <td>-0.009299</td>\n",
       "      <td>-0.045285</td>\n",
       "      <td>-0.058515</td>\n",
       "      <td>0.019625</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>0.014584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.007768</td>\n",
       "      <td>-0.055059</td>\n",
       "      <td>0.011419</td>\n",
       "      <td>-0.008992</td>\n",
       "      <td>-0.044945</td>\n",
       "      <td>-0.058379</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>0.014763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.013778</td>\n",
       "      <td>-0.057453</td>\n",
       "      <td>0.015920</td>\n",
       "      <td>-0.012567</td>\n",
       "      <td>-0.054702</td>\n",
       "      <td>-0.066809</td>\n",
       "      <td>0.024914</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.013134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.014101</td>\n",
       "      <td>-0.057169</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>-0.012388</td>\n",
       "      <td>-0.054410</td>\n",
       "      <td>-0.066495</td>\n",
       "      <td>0.025159</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.013202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.009479</td>\n",
       "      <td>-0.059496</td>\n",
       "      <td>0.017075</td>\n",
       "      <td>-0.014544</td>\n",
       "      <td>-0.047033</td>\n",
       "      <td>-0.058775</td>\n",
       "      <td>0.029621</td>\n",
       "      <td>0.016625</td>\n",
       "      <td>0.011564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.009479</td>\n",
       "      <td>-0.059496</td>\n",
       "      <td>0.017075</td>\n",
       "      <td>-0.014544</td>\n",
       "      <td>-0.047033</td>\n",
       "      <td>-0.058775</td>\n",
       "      <td>0.029621</td>\n",
       "      <td>0.016625</td>\n",
       "      <td>0.011564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.054383  -0.077396   0.017636   0.001934  -0.017260  -0.023554   \n",
       "1   -0.054989  -0.078058   0.017619   0.002192  -0.017288  -0.023530   \n",
       "2   -0.056124  -0.076407   0.012608   0.001101  -0.017544  -0.025520   \n",
       "3   -0.056871  -0.076160   0.012578   0.001377  -0.017368  -0.025321   \n",
       "4   -0.054549  -0.075217   0.007305  -0.000571  -0.017373  -0.025870   \n",
       "5   -0.054482  -0.075539   0.007546  -0.001123  -0.017683  -0.025759   \n",
       "6   -0.052897  -0.071629   0.002485  -0.003630  -0.017824  -0.025654   \n",
       "7   -0.050297  -0.071649   0.002458  -0.005247  -0.018835  -0.025467   \n",
       "8   -0.043220  -0.067566   0.006367  -0.009477  -0.019822  -0.025210   \n",
       "9   -0.046112  -0.067906   0.004724  -0.009587  -0.020418  -0.023944   \n",
       "10  -0.044195  -0.063328   0.014459  -0.008567  -0.020768  -0.023074   \n",
       "11  -0.044968  -0.063955   0.016717  -0.007284  -0.020191  -0.022290   \n",
       "12  -0.045505  -0.064582   0.008952  -0.008212  -0.021068  -0.025874   \n",
       "13  -0.045458  -0.061935   0.011022  -0.007144  -0.020495  -0.025431   \n",
       "14  -0.053835  -0.064915   0.000935  -0.008344  -0.021174  -0.028270   \n",
       "15   0.480766   0.804367   1.009437   1.405700   1.691496   2.841998   \n",
       "16   1.074630   5.189209   3.766095   4.612694   4.448627   9.298885   \n",
       "17   1.360908   8.229190   6.112268   7.307828   7.376381  12.782488   \n",
       "18   2.710624  14.561180  12.065598  14.274570  15.324944  22.272406   \n",
       "19   3.728159  16.829859  13.981781  16.238804  18.034082  25.094400   \n",
       "20   7.032422  23.690905  21.573759  23.289249  26.672586  33.469379   \n",
       "21   8.351828  23.249754  22.878181  26.063431  30.046282  35.618168   \n",
       "22  10.415552  25.810076  27.963005  30.775246  34.041340  41.889191   \n",
       "23   9.217402  24.167564  25.379066  27.624107  31.148561  40.437092   \n",
       "24  10.109533  23.117170  26.679737  29.879414  31.591608  38.458378   \n",
       "25   9.798756  23.242266  27.564289  30.948420  32.562592  39.057507   \n",
       "26   8.717486  20.775202  25.443485  28.751791  30.554605  35.555767   \n",
       "27   9.678226  21.505625  24.105095  27.986544  29.820450  34.604118   \n",
       "28   6.547288  17.725483  20.116579  23.347223  25.457735  28.782780   \n",
       "29   6.166773  17.433405  20.277153  23.937033  26.356325  29.223063   \n",
       "30   3.892257  12.332622  15.076857  19.009789  22.126995  24.418331   \n",
       "31   3.207412  12.266441  13.195762  16.659857  18.211763  20.173138   \n",
       "32   1.364031   4.959263   6.216760   7.283382   8.400757  10.049616   \n",
       "33   1.475507   3.872814   3.739480   3.767387   4.253705   5.282517   \n",
       "34  -0.008009  -0.049788   0.004635  -0.011683  -0.025514  -0.024529   \n",
       "35  -0.010483  -0.049493  -0.005264  -0.015214  -0.029195  -0.026546   \n",
       "36  -0.011386  -0.043699  -0.000705  -0.019729  -0.039483  -0.049527   \n",
       "37  -0.011307  -0.043807  -0.001383  -0.019798  -0.039323  -0.048894   \n",
       "38  -0.002113  -0.042092  -0.006321  -0.016995  -0.043729  -0.053568   \n",
       "39  -0.002532  -0.041698  -0.007065  -0.017063  -0.043602  -0.053440   \n",
       "40  -0.005869  -0.054863  -0.004829  -0.012315  -0.044470  -0.056016   \n",
       "41  -0.006571  -0.054277  -0.004157  -0.012108  -0.044326  -0.055941   \n",
       "42  -0.006715  -0.055998   0.009565  -0.009299  -0.045285  -0.058515   \n",
       "43  -0.007768  -0.055059   0.011419  -0.008992  -0.044945  -0.058379   \n",
       "44  -0.013778  -0.057453   0.015920  -0.012567  -0.054702  -0.066809   \n",
       "45  -0.014101  -0.057169   0.016026  -0.012388  -0.054410  -0.066495   \n",
       "46  -0.009479  -0.059496   0.017075  -0.014544  -0.047033  -0.058775   \n",
       "47  -0.009479  -0.059496   0.017075  -0.014544  -0.047033  -0.058775   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.023951   0.034546   0.015895  \n",
       "1    0.023965   0.035055   0.015970  \n",
       "2    0.019945   0.026681   0.018218  \n",
       "3    0.019986   0.026664   0.017985  \n",
       "4    0.015958   0.012309   0.015488  \n",
       "5    0.015988   0.012422   0.015672  \n",
       "6    0.013116   0.000668   0.012949  \n",
       "7    0.013325  -0.001571   0.013606  \n",
       "8    0.010773  -0.001386   0.013564  \n",
       "9    0.010917  -0.000724   0.013921  \n",
       "10   0.006784   0.003000   0.014361  \n",
       "11   0.006718   0.003357   0.014304  \n",
       "12   0.001670   0.006274   0.012829  \n",
       "13   0.001301   0.006596   0.012875  \n",
       "14   0.007496   0.009397   0.012505  \n",
       "15   4.194435   6.345063   9.181172  \n",
       "16  10.746218  13.193055  16.936323  \n",
       "17  14.812457  17.435095  21.064445  \n",
       "18  25.762577  28.166372  32.172577  \n",
       "19  27.990452  29.321339  32.518059  \n",
       "20  36.227219  37.029972  38.596687  \n",
       "21  38.187065  36.416103  37.746643  \n",
       "22  43.480835  42.513611  50.147343  \n",
       "23  41.202976  40.425114  48.335682  \n",
       "24  40.798298  38.298100  45.764450  \n",
       "25  41.612152  39.613091  45.098194  \n",
       "26  38.046318  37.313267  42.873169  \n",
       "27  39.064278  38.436432  46.535351  \n",
       "28  32.501617  33.851074  38.224697  \n",
       "29  33.168301  34.101490  38.158165  \n",
       "30  26.842695  27.721781  30.273018  \n",
       "31  22.898886  25.183853  27.738813  \n",
       "32  12.040539  14.636688  16.446121  \n",
       "33   7.747121  11.481064  12.962932  \n",
       "34   0.017446   0.008195   0.019340  \n",
       "35   0.017653   0.007123   0.018086  \n",
       "36   0.014197   0.009481   0.016109  \n",
       "37   0.014508   0.008803   0.015715  \n",
       "38   0.017293   0.009763   0.014582  \n",
       "39   0.017552   0.008758   0.014786  \n",
       "40   0.016332   0.008738   0.015297  \n",
       "41   0.017792   0.008218   0.015598  \n",
       "42   0.019625   0.005610   0.014584  \n",
       "43   0.021314   0.005486   0.014763  \n",
       "44   0.024914   0.003055   0.013134  \n",
       "45   0.025159   0.003132   0.013202  \n",
       "46   0.029621   0.016625   0.011564  \n",
       "47   0.029621   0.016625   0.011564  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].sort_index()[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.710383"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.94+0.480766)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.315586"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9.45 + 9.181172)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.793529</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.800130</td>\n",
       "      <td>0.298148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.782599</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.782599</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.753273</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.765826</td>\n",
       "      <td>0.292593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.748728</td>\n",
       "      <td>0.288889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.761173</td>\n",
       "      <td>0.285185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.749919</td>\n",
       "      <td>0.281481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241667</td>\n",
       "      <td>0.768856</td>\n",
       "      <td>0.275926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.753382</td>\n",
       "      <td>0.272222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.773509</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.726653</td>\n",
       "      <td>0.261111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.745157</td>\n",
       "      <td>0.255556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.669408</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.469267</td>\n",
       "      <td>0.009470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.663673</td>\n",
       "      <td>0.251852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0.347826</td>\n",
       "      <td>1.877069</td>\n",
       "      <td>0.037879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.605021</td>\n",
       "      <td>0.251852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.347826</td>\n",
       "      <td>3.191010</td>\n",
       "      <td>0.064394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.594633</td>\n",
       "      <td>0.255556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>4.692657</td>\n",
       "      <td>0.094697</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.580781</td>\n",
       "      <td>0.257407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>6.382006</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.575803</td>\n",
       "      <td>0.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>12.576279</td>\n",
       "      <td>0.225379</td>\n",
       "      <td>0.038716</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.572990</td>\n",
       "      <td>0.262963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>15.954963</td>\n",
       "      <td>0.268939</td>\n",
       "      <td>0.067044</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.568012</td>\n",
       "      <td>0.264815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.478261</td>\n",
       "      <td>13.890188</td>\n",
       "      <td>0.259470</td>\n",
       "      <td>0.024551</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.563467</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.478261</td>\n",
       "      <td>10.886904</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.563467</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>11.356167</td>\n",
       "      <td>0.210227</td>\n",
       "      <td>0.021719</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.563143</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>14.453320</td>\n",
       "      <td>0.259470</td>\n",
       "      <td>0.035883</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.568012</td>\n",
       "      <td>0.264815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>12.200868</td>\n",
       "      <td>0.221591</td>\n",
       "      <td>0.027384</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.569852</td>\n",
       "      <td>0.262963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>7.695957</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.584893</td>\n",
       "      <td>0.257407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>9.573041</td>\n",
       "      <td>0.176136</td>\n",
       "      <td>0.022663</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.583595</td>\n",
       "      <td>0.253704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>10.605451</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.593767</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.652174</td>\n",
       "      <td>10.230059</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.037771</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.583378</td>\n",
       "      <td>0.246296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.652174</td>\n",
       "      <td>6.757493</td>\n",
       "      <td>0.128788</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.599610</td>\n",
       "      <td>0.240741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.695652</td>\n",
       "      <td>3.754175</td>\n",
       "      <td>0.071970</td>\n",
       "      <td>0.015109</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.589006</td>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.938544</td>\n",
       "      <td>0.018939</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.589006</td>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.588681</td>\n",
       "      <td>0.225926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.593767</td>\n",
       "      <td>0.224074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.577751</td>\n",
       "      <td>0.220370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.582837</td>\n",
       "      <td>0.218519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.559680</td>\n",
       "      <td>0.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.564658</td>\n",
       "      <td>0.214815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.537929</td>\n",
       "      <td>0.212963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.547668</td>\n",
       "      <td>0.209259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.515529</td>\n",
       "      <td>0.207407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.524835</td>\n",
       "      <td>0.203704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.497241</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.501028</td>\n",
       "      <td>0.198148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.480143</td>\n",
       "      <td>0.196296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.480143</td>\n",
       "      <td>0.196296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Hour     TARGET       DHI       DNI        WS        RH         T\n",
       "288  0.000000   0.000000  0.000000  0.000000  0.066667  0.793529  0.300000\n",
       "289  0.000000   0.000000  0.000000  0.000000  0.075000  0.800130  0.298148\n",
       "290  0.043478   0.000000  0.000000  0.000000  0.083333  0.782599  0.296296\n",
       "291  0.043478   0.000000  0.000000  0.000000  0.075000  0.782599  0.296296\n",
       "292  0.086957   0.000000  0.000000  0.000000  0.075000  0.753273  0.296296\n",
       "293  0.086957   0.000000  0.000000  0.000000  0.091667  0.765826  0.292593\n",
       "294  0.130435   0.000000  0.000000  0.000000  0.100000  0.748728  0.288889\n",
       "295  0.130435   0.000000  0.000000  0.000000  0.150000  0.761173  0.285185\n",
       "296  0.173913   0.000000  0.000000  0.000000  0.200000  0.749919  0.281481\n",
       "297  0.173913   0.000000  0.000000  0.000000  0.241667  0.768856  0.275926\n",
       "298  0.217391   0.000000  0.000000  0.000000  0.291667  0.753382  0.272222\n",
       "299  0.217391   0.000000  0.000000  0.000000  0.300000  0.773509  0.266667\n",
       "300  0.260870   0.000000  0.000000  0.000000  0.316667  0.726653  0.261111\n",
       "301  0.260870   0.000000  0.000000  0.000000  0.325000  0.745157  0.255556\n",
       "302  0.304348   0.000000  0.000000  0.000000  0.341667  0.669408  0.250000\n",
       "303  0.304348   0.469267  0.009470  0.000000  0.375000  0.663673  0.251852\n",
       "304  0.347826   1.877069  0.037879  0.000000  0.416667  0.605021  0.251852\n",
       "305  0.347826   3.191010  0.064394  0.000000  0.441667  0.594633  0.255556\n",
       "306  0.391304   4.692657  0.094697  0.000944  0.466667  0.580781  0.257407\n",
       "307  0.391304   6.382006  0.125000  0.006610  0.483333  0.575803  0.259259\n",
       "308  0.434783  12.576279  0.225379  0.038716  0.491667  0.572990  0.262963\n",
       "309  0.434783  15.954963  0.268939  0.067044  0.500000  0.568012  0.264815\n",
       "310  0.478261  13.890188  0.259470  0.024551  0.508333  0.563467  0.266667\n",
       "311  0.478261  10.886904  0.204545  0.016997  0.508333  0.563467  0.266667\n",
       "312  0.521739  11.356167  0.210227  0.021719  0.516667  0.563143  0.266667\n",
       "313  0.521739  14.453320  0.259470  0.035883  0.525000  0.568012  0.264815\n",
       "314  0.565217  12.200868  0.221591  0.027384  0.533333  0.569852  0.262963\n",
       "315  0.565217   7.695957  0.147727  0.008499  0.541667  0.584893  0.257407\n",
       "316  0.608696   9.573041  0.176136  0.022663  0.550000  0.583595  0.253704\n",
       "317  0.608696  10.605451  0.196970  0.028329  0.541667  0.593767  0.250000\n",
       "318  0.652174  10.230059  0.187500  0.037771  0.541667  0.583378  0.246296\n",
       "319  0.652174   6.757493  0.128788  0.020774  0.525000  0.599610  0.240741\n",
       "320  0.695652   3.754175  0.071970  0.015109  0.500000  0.589006  0.235185\n",
       "321  0.695652   0.938544  0.018939  0.004721  0.500000  0.589006  0.235185\n",
       "322  0.739130   0.000000  0.000000  0.000000  0.450000  0.588681  0.225926\n",
       "323  0.739130   0.000000  0.000000  0.000000  0.425000  0.593767  0.224074\n",
       "324  0.782609   0.000000  0.000000  0.000000  0.408333  0.577751  0.220370\n",
       "325  0.782609   0.000000  0.000000  0.000000  0.391667  0.582837  0.218519\n",
       "326  0.826087   0.000000  0.000000  0.000000  0.375000  0.559680  0.216667\n",
       "327  0.826087   0.000000  0.000000  0.000000  0.358333  0.564658  0.214815\n",
       "328  0.869565   0.000000  0.000000  0.000000  0.341667  0.537929  0.212963\n",
       "329  0.869565   0.000000  0.000000  0.000000  0.333333  0.547668  0.209259\n",
       "330  0.913043   0.000000  0.000000  0.000000  0.316667  0.515529  0.207407\n",
       "331  0.913043   0.000000  0.000000  0.000000  0.300000  0.524835  0.203704\n",
       "332  0.956522   0.000000  0.000000  0.000000  0.283333  0.497241  0.200000\n",
       "333  0.956522   0.000000  0.000000  0.000000  0.283333  0.501028  0.198148\n",
       "334  1.000000   0.000000  0.000000  0.000000  0.283333  0.480143  0.196296\n",
       "335  1.000000   0.000000  0.000000  0.000000  0.283333  0.480143  0.196296"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[:48]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
