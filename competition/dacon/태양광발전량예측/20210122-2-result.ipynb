{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, is_train=True):\n",
    "    \n",
    "    temp = data.copy()\n",
    "    temp = temp[['Day','Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "\n",
    "    if is_train==True:          \n",
    "    \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill')\n",
    "        temp = temp.dropna()\n",
    "        \n",
    "        return temp.iloc[:-96]\n",
    "\n",
    "    elif is_train==False:\n",
    "        \n",
    "        temp = temp[['Day','Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "                              \n",
    "        return temp.iloc[-48:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train/train.csv')\n",
    "\n",
    "test = []\n",
    "\n",
    "for i in range(81):\n",
    "    file_path = './data/test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp['Day'] = i\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    test.append(temp)\n",
    "\n",
    "df_test = pd.concat(test)\n",
    "\n",
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52464, 10), (3888, 8))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = preprocess_data(train)\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.WS = np.log1p(df_train.WS)\n",
    "df_test.WS = np.log1p(df_test.WS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = df_train[['Day','Hour','DHI','DNI','WS','RH','T']].min()\n",
    "max  = df_train[['Day','Hour','DHI','DNI','WS','RH','T']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(['Day','Hour','DHI','DNI','WS','RH','T']):\n",
    "    df_train[col] = (df_train[col] - min[i]) / (max[i] - min[i])\n",
    "    df_test[col] = (df_test[col] - min[i]) / (max[i] - min[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Day0 = df_train.iloc[:, :-2]\n",
    "Day  = df_train.iloc[:, 1:-2]\n",
    "Day7 = df_train.iloc[:, -2]\n",
    "Day8 = df_train.iloc[:, -1]\n",
    "Day78 = df_train.iloc[:, -2:]\n",
    "\n",
    "df_test0 = df_test.copy()\n",
    "df_test = df_test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(q, y, pred):\n",
    "    err = (y-pred)\n",
    "    return mean(maximum(q*err, (q-1)*err), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_lst = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39348, 7), (13116, 7), (39348, 2), (13116, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(Day, Day78, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train_1, X_valid_1, Y_train_1, Y_valid_1 = train_test_split(Day, Day7, test_size=0.25, random_state=42)\n",
    "X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(Day, Day8, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import mean, maximum\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "582/615 [===========================>..] - ETA: 0s - loss: 374.8886WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "615/615 [==============================] - 2s 2ms/step - loss: 367.3883 - val_loss: 178.8931\n",
      "Epoch 2/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 171.6994 - val_loss: 158.0998\n",
      "Epoch 3/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 153.0226 - val_loss: 150.6760\n",
      "Epoch 4/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 154.1150 - val_loss: 148.6838\n",
      "Epoch 5/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 148.2095 - val_loss: 147.8098\n",
      "Epoch 6/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 150.1184 - val_loss: 149.6413\n",
      "Epoch 7/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 149.8693 - val_loss: 146.2519\n",
      "Epoch 8/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 142.4574 - val_loss: 147.2933\n",
      "Epoch 9/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 148.2808 - val_loss: 146.2809\n",
      "Epoch 10/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 148.6767 - val_loss: 143.0798\n",
      "Epoch 11/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 153.2889 - val_loss: 148.2513\n",
      "Epoch 12/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 145.3421 - val_loss: 143.6833\n",
      "Epoch 13/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 144.2798 - val_loss: 140.5431\n",
      "Epoch 14/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 145.5635 - val_loss: 140.3103\n",
      "Epoch 15/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 138.7699 - val_loss: 140.5931\n",
      "Epoch 16/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 144.6258 - val_loss: 139.9224\n",
      "Epoch 17/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 145.7918 - val_loss: 139.8964\n",
      "Epoch 18/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 139.0772 - val_loss: 138.8293\n",
      "Epoch 19/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 141.4712 - val_loss: 137.0847\n",
      "Epoch 20/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 134.7885 - val_loss: 137.7627\n",
      "Epoch 21/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 138.8287 - val_loss: 145.7656\n",
      "Epoch 22/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 140.1953 - val_loss: 135.3615\n",
      "Epoch 23/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 139.0165 - val_loss: 147.1748\n",
      "Epoch 24/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 137.6333 - val_loss: 135.4534\n",
      "Epoch 25/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 134.3128 - val_loss: 134.4454\n",
      "Epoch 26/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 138.8196 - val_loss: 137.6373\n",
      "Epoch 27/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 140.2207 - val_loss: 133.6471\n",
      "Epoch 28/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 133.9257 - val_loss: 135.3104\n",
      "Epoch 29/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 137.1697 - val_loss: 136.1895\n",
      "Epoch 30/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 136.1091 - val_loss: 139.0966\n",
      "Epoch 00030: early stopping\n",
      "410/410 [==============================] - 0s 627us/step - loss: 136.9035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136.9035186767578"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(Day.shape)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(2)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "hist = model.fit(X_train, Y_train, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "model.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'], 'b-', label='loss')\n",
    "plt.plot(hist.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "776/820 [===========================>..] - ETA: 0s - loss: 1.4485WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 1.4466 - val_loss: 1.5857\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4015 - val_loss: 1.5911\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3885 - val_loss: 1.5875\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3851 - val_loss: 1.5817\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3950 - val_loss: 1.5888\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3919 - val_loss: 1.5936\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3963 - val_loss: 1.5751\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4032 - val_loss: 1.5890\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3972 - val_loss: 1.5817\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4083 - val_loss: 1.5709\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3770 - val_loss: 1.5754\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3782 - val_loss: 1.6085\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3934 - val_loss: 1.5765\n",
      "Epoch 00013: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "809/820 [============================>.] - ETA: 0s - loss: 2.2647WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2645 - val_loss: 2.5744\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2360 - val_loss: 2.5675\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2304 - val_loss: 2.5755\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2185 - val_loss: 2.5578\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2407 - val_loss: 2.5563\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2340 - val_loss: 2.5562\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2462 - val_loss: 2.5812\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2530 - val_loss: 2.5990\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2311 - val_loss: 2.5576\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "772/820 [===========================>..] - ETA: 0s - loss: 2.6757WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6749 - val_loss: 3.0544\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6461 - val_loss: 3.0846\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6499 - val_loss: 3.0732\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6338 - val_loss: 3.0341\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6545 - val_loss: 3.0784\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6478 - val_loss: 3.0433\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6685 - val_loss: 3.0382\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "809/820 [============================>.] - ETA: 0s - loss: 2.7761WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.7760 - val_loss: 3.1536\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7462 - val_loss: 3.1848\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7543 - val_loss: 3.1604\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7447 - val_loss: 3.1620\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "784/820 [===========================>..] - ETA: 0s - loss: 2.6437WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6437 - val_loss: 2.9995\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6233 - val_loss: 3.0241\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6223 - val_loss: 3.0258\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6248 - val_loss: 3.0458\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "797/820 [============================>.] - ETA: 0s - loss: 2.3430WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3431 - val_loss: 2.6631\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3206 - val_loss: 2.6859\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3236 - val_loss: 2.6837\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3227 - val_loss: 2.6837\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "808/820 [============================>.] - ETA: 0s - loss: 1.9176WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9177 - val_loss: 2.1826\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9031 - val_loss: 2.2155\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9060 - val_loss: 2.1876\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9062 - val_loss: 2.1855\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "778/820 [===========================>..] - ETA: 0s - loss: 1.4000WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4000 - val_loss: 1.5825\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3874 - val_loss: 1.6115\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3928 - val_loss: 1.5962\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3905 - val_loss: 1.5850\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "764/820 [==========================>...] - ETA: 0s - loss: 0.7856WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7854 - val_loss: 0.8803\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7706 - val_loss: 0.9529\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7775 - val_loss: 0.9230\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7760 - val_loss: 0.8663\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7671 - val_loss: 0.8712\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7696 - val_loss: 0.8625\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7781 - val_loss: 0.8868\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7832 - val_loss: 0.8882\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7624 - val_loss: 0.8661\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3888, 18)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    model.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model.fit(Day, Day78, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred = pd.DataFrame(model.predict(df_test))\n",
    "    results = pd.concat([results, pred], axis=1)\n",
    "\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.012377</td>\n",
       "      <td>-0.008408</td>\n",
       "      <td>-0.002669</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.008699</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>-0.005510</td>\n",
       "      <td>-0.004215</td>\n",
       "      <td>0.008374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.011908</td>\n",
       "      <td>-0.008797</td>\n",
       "      <td>-0.002492</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>-0.005051</td>\n",
       "      <td>-0.004180</td>\n",
       "      <td>0.007388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.010526</td>\n",
       "      <td>-0.004697</td>\n",
       "      <td>-0.000590</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>-0.002543</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.003742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.010701</td>\n",
       "      <td>-0.004678</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.004220</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>-0.002043</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.004744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.011175</td>\n",
       "      <td>-0.002744</td>\n",
       "      <td>-0.001799</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>-0.002546</td>\n",
       "      <td>-0.003118</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.002712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.010739</td>\n",
       "      <td>-0.002702</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>-0.002185</td>\n",
       "      <td>-0.003657</td>\n",
       "      <td>-0.001281</td>\n",
       "      <td>0.001608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.011099</td>\n",
       "      <td>-0.003457</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>-0.002362</td>\n",
       "      <td>-0.001750</td>\n",
       "      <td>-0.001075</td>\n",
       "      <td>0.002287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.009779</td>\n",
       "      <td>-0.004186</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>0.004732</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>-0.002142</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.000815</td>\n",
       "      <td>0.002213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.009821</td>\n",
       "      <td>-0.003191</td>\n",
       "      <td>-0.001087</td>\n",
       "      <td>0.005724</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>-0.002203</td>\n",
       "      <td>-0.000805</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.002826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.011221</td>\n",
       "      <td>-0.003830</td>\n",
       "      <td>-0.001033</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>-0.000982</td>\n",
       "      <td>-0.001051</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.003127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.012145</td>\n",
       "      <td>-0.002385</td>\n",
       "      <td>-0.003248</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>-0.000704</td>\n",
       "      <td>-0.001115</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.004341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.012509</td>\n",
       "      <td>-0.002297</td>\n",
       "      <td>-0.003261</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.006177</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>-0.000928</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.004327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.010746</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.002549</td>\n",
       "      <td>0.007414</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.009221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.011147</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.002534</td>\n",
       "      <td>0.007370</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.009349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.007546</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.006869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.639051</td>\n",
       "      <td>0.635898</td>\n",
       "      <td>0.843498</td>\n",
       "      <td>1.982780</td>\n",
       "      <td>2.256428</td>\n",
       "      <td>2.603778</td>\n",
       "      <td>5.363832</td>\n",
       "      <td>6.449433</td>\n",
       "      <td>15.617063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.814343</td>\n",
       "      <td>3.295753</td>\n",
       "      <td>2.746976</td>\n",
       "      <td>5.817867</td>\n",
       "      <td>6.423006</td>\n",
       "      <td>8.212381</td>\n",
       "      <td>12.193195</td>\n",
       "      <td>14.353599</td>\n",
       "      <td>26.773714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.588575</td>\n",
       "      <td>6.609897</td>\n",
       "      <td>5.656024</td>\n",
       "      <td>9.344378</td>\n",
       "      <td>10.357342</td>\n",
       "      <td>12.644737</td>\n",
       "      <td>16.442818</td>\n",
       "      <td>19.096739</td>\n",
       "      <td>31.644506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.595252</td>\n",
       "      <td>13.879253</td>\n",
       "      <td>13.842419</td>\n",
       "      <td>19.580328</td>\n",
       "      <td>21.464272</td>\n",
       "      <td>25.057659</td>\n",
       "      <td>28.468496</td>\n",
       "      <td>32.681065</td>\n",
       "      <td>45.594498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.647614</td>\n",
       "      <td>16.832405</td>\n",
       "      <td>17.848009</td>\n",
       "      <td>23.733877</td>\n",
       "      <td>25.369984</td>\n",
       "      <td>29.109535</td>\n",
       "      <td>31.853228</td>\n",
       "      <td>34.154743</td>\n",
       "      <td>45.717892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12.565716</td>\n",
       "      <td>26.434645</td>\n",
       "      <td>26.116669</td>\n",
       "      <td>30.763998</td>\n",
       "      <td>32.340950</td>\n",
       "      <td>37.603115</td>\n",
       "      <td>38.784103</td>\n",
       "      <td>40.021847</td>\n",
       "      <td>41.573956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.355133</td>\n",
       "      <td>29.459236</td>\n",
       "      <td>29.044569</td>\n",
       "      <td>32.611523</td>\n",
       "      <td>35.292900</td>\n",
       "      <td>38.157532</td>\n",
       "      <td>38.711216</td>\n",
       "      <td>40.273335</td>\n",
       "      <td>41.185211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16.214230</td>\n",
       "      <td>30.014963</td>\n",
       "      <td>30.131413</td>\n",
       "      <td>37.141304</td>\n",
       "      <td>40.878246</td>\n",
       "      <td>46.543797</td>\n",
       "      <td>47.951195</td>\n",
       "      <td>46.542347</td>\n",
       "      <td>51.413349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15.461583</td>\n",
       "      <td>30.050043</td>\n",
       "      <td>29.642187</td>\n",
       "      <td>35.482990</td>\n",
       "      <td>38.248829</td>\n",
       "      <td>44.175770</td>\n",
       "      <td>45.887787</td>\n",
       "      <td>44.448658</td>\n",
       "      <td>50.929131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15.829645</td>\n",
       "      <td>26.572556</td>\n",
       "      <td>27.212975</td>\n",
       "      <td>38.487041</td>\n",
       "      <td>39.427395</td>\n",
       "      <td>43.737904</td>\n",
       "      <td>45.538689</td>\n",
       "      <td>43.334656</td>\n",
       "      <td>49.250668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15.413681</td>\n",
       "      <td>25.731159</td>\n",
       "      <td>27.238661</td>\n",
       "      <td>38.078571</td>\n",
       "      <td>40.145130</td>\n",
       "      <td>44.641834</td>\n",
       "      <td>46.135239</td>\n",
       "      <td>44.152813</td>\n",
       "      <td>47.106987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.965651</td>\n",
       "      <td>21.169024</td>\n",
       "      <td>23.215572</td>\n",
       "      <td>34.138832</td>\n",
       "      <td>37.506283</td>\n",
       "      <td>40.659565</td>\n",
       "      <td>44.352531</td>\n",
       "      <td>41.669754</td>\n",
       "      <td>45.997246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14.931929</td>\n",
       "      <td>22.830767</td>\n",
       "      <td>23.891876</td>\n",
       "      <td>34.956406</td>\n",
       "      <td>35.964188</td>\n",
       "      <td>40.592705</td>\n",
       "      <td>46.225418</td>\n",
       "      <td>43.423851</td>\n",
       "      <td>52.211876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.454762</td>\n",
       "      <td>16.772087</td>\n",
       "      <td>19.008219</td>\n",
       "      <td>28.416666</td>\n",
       "      <td>31.030113</td>\n",
       "      <td>34.061485</td>\n",
       "      <td>37.703865</td>\n",
       "      <td>36.401390</td>\n",
       "      <td>39.311161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11.479879</td>\n",
       "      <td>16.782578</td>\n",
       "      <td>19.353642</td>\n",
       "      <td>28.391935</td>\n",
       "      <td>31.379198</td>\n",
       "      <td>34.174431</td>\n",
       "      <td>37.964203</td>\n",
       "      <td>36.769184</td>\n",
       "      <td>37.942368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.403148</td>\n",
       "      <td>12.403229</td>\n",
       "      <td>15.980728</td>\n",
       "      <td>23.602724</td>\n",
       "      <td>26.707657</td>\n",
       "      <td>27.987970</td>\n",
       "      <td>30.370726</td>\n",
       "      <td>29.519817</td>\n",
       "      <td>30.116423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8.652231</td>\n",
       "      <td>12.185276</td>\n",
       "      <td>14.748415</td>\n",
       "      <td>21.813589</td>\n",
       "      <td>22.833496</td>\n",
       "      <td>25.308094</td>\n",
       "      <td>28.803246</td>\n",
       "      <td>27.512985</td>\n",
       "      <td>29.924093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.323413</td>\n",
       "      <td>7.650569</td>\n",
       "      <td>8.590864</td>\n",
       "      <td>12.570680</td>\n",
       "      <td>12.143576</td>\n",
       "      <td>13.096897</td>\n",
       "      <td>15.412798</td>\n",
       "      <td>15.768902</td>\n",
       "      <td>20.099825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.997223</td>\n",
       "      <td>4.491141</td>\n",
       "      <td>3.348555</td>\n",
       "      <td>6.742145</td>\n",
       "      <td>5.155645</td>\n",
       "      <td>6.307911</td>\n",
       "      <td>10.580891</td>\n",
       "      <td>9.936776</td>\n",
       "      <td>15.877209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.013756</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>-0.001695</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.014364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.015333</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>-0.001510</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.004647</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.013583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.012758</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>0.002037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.012845</td>\n",
       "      <td>0.005026</td>\n",
       "      <td>-0.002002</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>0.001957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.019962</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>-0.002069</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>-0.001355</td>\n",
       "      <td>-0.000359</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>-0.001453</td>\n",
       "      <td>0.001843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.019971</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>-0.002198</td>\n",
       "      <td>-0.000838</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>0.001812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.016495</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>-0.002053</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>-0.001083</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>-0.000643</td>\n",
       "      <td>0.001476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.016510</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>-0.002271</td>\n",
       "      <td>-0.000548</td>\n",
       "      <td>-0.000507</td>\n",
       "      <td>-0.001186</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>0.001526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.011314</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>-0.000782</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>0.002111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.011205</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>-0.001251</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>-0.002756</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>0.002159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.012792</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.002021</td>\n",
       "      <td>-0.005342</td>\n",
       "      <td>-0.001838</td>\n",
       "      <td>-0.000817</td>\n",
       "      <td>0.002713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.012685</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>-0.000402</td>\n",
       "      <td>-0.002049</td>\n",
       "      <td>-0.005410</td>\n",
       "      <td>-0.001859</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>0.002708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.017611</td>\n",
       "      <td>-0.002294</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>-0.004588</td>\n",
       "      <td>-0.011472</td>\n",
       "      <td>-0.003494</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>0.001990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.017611</td>\n",
       "      <td>-0.002294</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>-0.004588</td>\n",
       "      <td>-0.011472</td>\n",
       "      <td>-0.003494</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>0.001990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1          1          1          1          1          1  \\\n",
       "0   -0.012377  -0.008408  -0.002669   0.005495   0.008699   0.000310   \n",
       "1   -0.011908  -0.008797  -0.002492   0.005386   0.008722   0.000491   \n",
       "2   -0.010526  -0.004697  -0.000590   0.003816   0.008723   0.000157   \n",
       "3   -0.010701  -0.004678  -0.000045   0.004220   0.008681   0.000760   \n",
       "4   -0.011175  -0.002744  -0.001799   0.002283   0.006326  -0.002546   \n",
       "5   -0.010739  -0.002702  -0.001509   0.002668   0.006632  -0.002185   \n",
       "6   -0.011099  -0.003457  -0.001746   0.004306   0.005722  -0.002362   \n",
       "7   -0.009779  -0.004186  -0.001002   0.004732   0.006041  -0.002142   \n",
       "8   -0.009821  -0.003191  -0.001087   0.005724   0.005217  -0.002203   \n",
       "9   -0.011221  -0.003830  -0.001033   0.005764   0.005574  -0.000982   \n",
       "10  -0.012145  -0.002385  -0.003248   0.005264   0.005600  -0.000704   \n",
       "11  -0.012509  -0.002297  -0.003261   0.005224   0.006177   0.000126   \n",
       "12  -0.010746  -0.000038  -0.002549   0.007414   0.005884  -0.000534   \n",
       "13  -0.011147  -0.000112  -0.002534   0.007370   0.006413   0.000134   \n",
       "14  -0.007546   0.004796   0.001303   0.009567   0.006674   0.000333   \n",
       "15   0.639051   0.635898   0.843498   1.982780   2.256428   2.603778   \n",
       "16   1.814343   3.295753   2.746976   5.817867   6.423006   8.212381   \n",
       "17   3.588575   6.609897   5.656024   9.344378  10.357342  12.644737   \n",
       "18   7.595252  13.879253  13.842419  19.580328  21.464272  25.057659   \n",
       "19   8.647614  16.832405  17.848009  23.733877  25.369984  29.109535   \n",
       "20  12.565716  26.434645  26.116669  30.763998  32.340950  37.603115   \n",
       "21  13.355133  29.459236  29.044569  32.611523  35.292900  38.157532   \n",
       "22  16.214230  30.014963  30.131413  37.141304  40.878246  46.543797   \n",
       "23  15.461583  30.050043  29.642187  35.482990  38.248829  44.175770   \n",
       "24  15.829645  26.572556  27.212975  38.487041  39.427395  43.737904   \n",
       "25  15.413681  25.731159  27.238661  38.078571  40.145130  44.641834   \n",
       "26  13.965651  21.169024  23.215572  34.138832  37.506283  40.659565   \n",
       "27  14.931929  22.830767  23.891876  34.956406  35.964188  40.592705   \n",
       "28  11.454762  16.772087  19.008219  28.416666  31.030113  34.061485   \n",
       "29  11.479879  16.782578  19.353642  28.391935  31.379198  34.174431   \n",
       "30   8.403148  12.403229  15.980728  23.602724  26.707657  27.987970   \n",
       "31   8.652231  12.185276  14.748415  21.813589  22.833496  25.308094   \n",
       "32   5.323413   7.650569   8.590864  12.570680  12.143576  13.096897   \n",
       "33   3.997223   4.491141   3.348555   6.742145   5.155645   6.307911   \n",
       "34  -0.013756   0.001518   0.003093   0.002931  -0.001695   0.001241   \n",
       "35  -0.015333   0.000811   0.002329   0.002877  -0.001510   0.001300   \n",
       "36  -0.012758   0.005166  -0.001884  -0.001592  -0.001545   0.000149   \n",
       "37  -0.012845   0.005026  -0.002002  -0.001412  -0.001418   0.000188   \n",
       "38  -0.019962   0.004680  -0.002069  -0.001052  -0.001355  -0.000359   \n",
       "39  -0.019971   0.004645  -0.002198  -0.000838  -0.001207  -0.000318   \n",
       "40  -0.016495   0.004422  -0.002053  -0.000415  -0.000637  -0.001083   \n",
       "41  -0.016510   0.004582  -0.002271  -0.000548  -0.000507  -0.001186   \n",
       "42  -0.011314   0.002465  -0.000989  -0.000540  -0.000782  -0.002717   \n",
       "43  -0.011205   0.002807  -0.001251  -0.000676  -0.000988  -0.002756   \n",
       "44  -0.012792   0.000506  -0.000278  -0.000299  -0.002021  -0.005342   \n",
       "45  -0.012685   0.000528  -0.000365  -0.000402  -0.002049  -0.005410   \n",
       "46  -0.017611  -0.002294   0.000239  -0.000664  -0.004588  -0.011472   \n",
       "47  -0.017611  -0.002294   0.000239  -0.000664  -0.004588  -0.011472   \n",
       "\n",
       "            1          1          1  \n",
       "0   -0.005510  -0.004215   0.008374  \n",
       "1   -0.005051  -0.004180   0.007388  \n",
       "2   -0.002543   0.001161   0.003742  \n",
       "3   -0.002043   0.000600   0.004744  \n",
       "4   -0.003118   0.000097   0.002712  \n",
       "5   -0.003657  -0.001281   0.001608  \n",
       "6   -0.001750  -0.001075   0.002287  \n",
       "7   -0.001386  -0.000815   0.002213  \n",
       "8   -0.000805  -0.000230   0.002826  \n",
       "9   -0.001051  -0.000322   0.003127  \n",
       "10  -0.001115  -0.000063   0.004341  \n",
       "11  -0.000928   0.000067   0.004327  \n",
       "12   0.000180   0.000671   0.009221  \n",
       "13   0.000349   0.000426   0.009349  \n",
       "14   0.001010   0.004217   0.006869  \n",
       "15   5.363832   6.449433  15.617063  \n",
       "16  12.193195  14.353599  26.773714  \n",
       "17  16.442818  19.096739  31.644506  \n",
       "18  28.468496  32.681065  45.594498  \n",
       "19  31.853228  34.154743  45.717892  \n",
       "20  38.784103  40.021847  41.573956  \n",
       "21  38.711216  40.273335  41.185211  \n",
       "22  47.951195  46.542347  51.413349  \n",
       "23  45.887787  44.448658  50.929131  \n",
       "24  45.538689  43.334656  49.250668  \n",
       "25  46.135239  44.152813  47.106987  \n",
       "26  44.352531  41.669754  45.997246  \n",
       "27  46.225418  43.423851  52.211876  \n",
       "28  37.703865  36.401390  39.311161  \n",
       "29  37.964203  36.769184  37.942368  \n",
       "30  30.370726  29.519817  30.116423  \n",
       "31  28.803246  27.512985  29.924093  \n",
       "32  15.412798  15.768902  20.099825  \n",
       "33  10.580891   9.936776  15.877209  \n",
       "34   0.004423   0.001658   0.014364  \n",
       "35   0.004647   0.001726   0.013583  \n",
       "36   0.000766  -0.000349   0.002037  \n",
       "37   0.000854  -0.000276   0.001957  \n",
       "38   0.000625  -0.001453   0.001843  \n",
       "39   0.000644  -0.001456   0.001812  \n",
       "40   0.000680  -0.000643   0.001476  \n",
       "41   0.000639  -0.000701   0.001526  \n",
       "42  -0.000466  -0.000149   0.002111  \n",
       "43  -0.000474  -0.000221   0.002159  \n",
       "44  -0.001838  -0.000817   0.002713  \n",
       "45  -0.001859  -0.000855   0.002708  \n",
       "46  -0.003494  -0.003327   0.001990  \n",
       "47  -0.003494  -0.003327   0.001990  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1][:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34889\n",
      "Early stopping, best iteration is:\n",
      "[418]\tvalid_0's quantile: 1.34812\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.14466\n",
      "[1000]\tvalid_0's quantile: 2.13764\n",
      "[1500]\tvalid_0's quantile: 2.13582\n",
      "[2000]\tvalid_0's quantile: 2.1334\n",
      "Early stopping, best iteration is:\n",
      "[1749]\tvalid_0's quantile: 2.13312\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.53565\n",
      "[1000]\tvalid_0's quantile: 2.50726\n",
      "[1500]\tvalid_0's quantile: 2.49216\n",
      "Early stopping, best iteration is:\n",
      "[1604]\tvalid_0's quantile: 2.48959\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.66191\n",
      "[1000]\tvalid_0's quantile: 2.62846\n",
      "[1500]\tvalid_0's quantile: 2.61266\n",
      "[2000]\tvalid_0's quantile: 2.6059\n",
      "[2500]\tvalid_0's quantile: 2.59923\n",
      "[3000]\tvalid_0's quantile: 2.59644\n",
      "Early stopping, best iteration is:\n",
      "[2707]\tvalid_0's quantile: 2.59598\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.55537\n",
      "[1000]\tvalid_0's quantile: 2.54183\n",
      "[1500]\tvalid_0's quantile: 2.52395\n",
      "[2000]\tvalid_0's quantile: 2.5191\n",
      "[2500]\tvalid_0's quantile: 2.51605\n",
      "[3000]\tvalid_0's quantile: 2.51386\n",
      "[3500]\tvalid_0's quantile: 2.50859\n",
      "[4000]\tvalid_0's quantile: 2.50447\n",
      "[4500]\tvalid_0's quantile: 2.50256\n",
      "[5000]\tvalid_0's quantile: 2.50036\n",
      "[5500]\tvalid_0's quantile: 2.498\n",
      "[6000]\tvalid_0's quantile: 2.49693\n",
      "[6500]\tvalid_0's quantile: 2.49595\n",
      "Early stopping, best iteration is:\n",
      "[6563]\tvalid_0's quantile: 2.49582\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.28838\n",
      "[1000]\tvalid_0's quantile: 2.26798\n",
      "[1500]\tvalid_0's quantile: 2.25775\n",
      "[2000]\tvalid_0's quantile: 2.25273\n",
      "[2500]\tvalid_0's quantile: 2.24936\n",
      "[3000]\tvalid_0's quantile: 2.24642\n",
      "[3500]\tvalid_0's quantile: 2.24451\n",
      "[4000]\tvalid_0's quantile: 2.24427\n",
      "Early stopping, best iteration is:\n",
      "[3735]\tvalid_0's quantile: 2.24391\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.87856\n",
      "[1000]\tvalid_0's quantile: 1.86236\n",
      "[1500]\tvalid_0's quantile: 1.85737\n",
      "[2000]\tvalid_0's quantile: 1.85491\n",
      "Early stopping, best iteration is:\n",
      "[1840]\tvalid_0's quantile: 1.85404\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34949\n",
      "[1000]\tvalid_0's quantile: 1.34333\n",
      "Early stopping, best iteration is:\n",
      "[898]\tvalid_0's quantile: 1.34271\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.748569\n",
      "[1000]\tvalid_0's quantile: 0.746232\n",
      "[1500]\tvalid_0's quantile: 0.745479\n",
      "Early stopping, best iteration is:\n",
      "[1284]\tvalid_0's quantile: 0.745153\n",
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.39392\n",
      "Early stopping, best iteration is:\n",
      "[331]\tvalid_0's quantile: 1.3933\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.19349\n",
      "Early stopping, best iteration is:\n",
      "[616]\tvalid_0's quantile: 2.18579\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.61892\n",
      "[1000]\tvalid_0's quantile: 2.57944\n",
      "[1500]\tvalid_0's quantile: 2.578\n",
      "Early stopping, best iteration is:\n",
      "[1380]\tvalid_0's quantile: 2.57786\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.74331\n",
      "[1000]\tvalid_0's quantile: 2.70696\n",
      "[1500]\tvalid_0's quantile: 2.70093\n",
      "[2000]\tvalid_0's quantile: 2.69465\n",
      "[2500]\tvalid_0's quantile: 2.68928\n",
      "[3000]\tvalid_0's quantile: 2.68205\n",
      "[3500]\tvalid_0's quantile: 2.67219\n",
      "[4000]\tvalid_0's quantile: 2.66516\n",
      "[4500]\tvalid_0's quantile: 2.6608\n",
      "Early stopping, best iteration is:\n",
      "[4475]\tvalid_0's quantile: 2.6608\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.6458\n",
      "[1000]\tvalid_0's quantile: 2.62814\n",
      "[1500]\tvalid_0's quantile: 2.61626\n",
      "[2000]\tvalid_0's quantile: 2.60023\n",
      "[2500]\tvalid_0's quantile: 2.58706\n",
      "[3000]\tvalid_0's quantile: 2.58403\n",
      "Early stopping, best iteration is:\n",
      "[2807]\tvalid_0's quantile: 2.58403\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.36786\n",
      "[1000]\tvalid_0's quantile: 2.35343\n",
      "[1500]\tvalid_0's quantile: 2.33476\n",
      "[2000]\tvalid_0's quantile: 2.32426\n",
      "[2500]\tvalid_0's quantile: 2.31984\n",
      "[3000]\tvalid_0's quantile: 2.31736\n",
      "Early stopping, best iteration is:\n",
      "[2871]\tvalid_0's quantile: 2.31729\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.96038\n",
      "[1000]\tvalid_0's quantile: 1.93778\n",
      "[1500]\tvalid_0's quantile: 1.92682\n",
      "[2000]\tvalid_0's quantile: 1.91583\n",
      "[2500]\tvalid_0's quantile: 1.91245\n",
      "Early stopping, best iteration is:\n",
      "[2560]\tvalid_0's quantile: 1.91213\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.40785\n",
      "[1000]\tvalid_0's quantile: 1.39944\n",
      "[1500]\tvalid_0's quantile: 1.39438\n",
      "[2000]\tvalid_0's quantile: 1.39224\n",
      "[2500]\tvalid_0's quantile: 1.39124\n",
      "Early stopping, best iteration is:\n",
      "[2308]\tvalid_0's quantile: 1.39107\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.780449\n",
      "[1000]\tvalid_0's quantile: 0.777177\n",
      "[1500]\tvalid_0's quantile: 0.775629\n",
      "[2000]\tvalid_0's quantile: 0.77524\n",
      "Early stopping, best iteration is:\n",
      "[1831]\tvalid_0's quantile: 0.774899\n"
     ]
    }
   ],
   "source": [
    "def LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "    \n",
    "    # (a) Modeling  \n",
    "    model = LGBMRegressor(objective='quantile', alpha=q,\n",
    "                         n_estimators=10000, bagging_fraction=0.7, learning_rate=0.027, subsample=0.7)                   \n",
    "                         \n",
    "                         \n",
    "    model.fit(X_train, Y_train, eval_metric = ['quantile'], \n",
    "          eval_set=[(X_valid, Y_valid)], early_stopping_rounds=300, verbose=500)\n",
    "\n",
    "    # (b) Predictions\n",
    "    pred = pd.Series(model.predict(X_test).round(2))\n",
    "    return pred, model\n",
    "\n",
    "def train_data(X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "\n",
    "    LGBM_models=[]\n",
    "    LGBM_actual_pred = pd.DataFrame()\n",
    "\n",
    "    for q in q_lst:\n",
    "        print(q)\n",
    "        pred , model = LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test)\n",
    "        LGBM_models.append(model)\n",
    "        LGBM_actual_pred = pd.concat([LGBM_actual_pred,pred],axis=1)\n",
    "\n",
    "    LGBM_actual_pred.columns=q_lst\n",
    "    \n",
    "    return LGBM_models, LGBM_actual_pred\n",
    "\n",
    "models_1, results_1 = train_data(X_train_1, Y_train_1, X_valid_1, Y_valid_1, df_test)\n",
    "models_2, results_2 = train_data(X_train_2, Y_train_2, X_valid_2, Y_valid_2, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45\n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51\n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11\n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34\n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51\n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17\n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03\n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17\n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82\n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14\n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61\n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92\n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20\n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73\n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94\n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20\n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59\n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49\n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13\n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_1[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>18.24</td>\n",
       "      <td>25.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.74</td>\n",
       "      <td>14.78</td>\n",
       "      <td>19.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>22.60</td>\n",
       "      <td>20.57</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.66</td>\n",
       "      <td>32.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.47</td>\n",
       "      <td>19.06</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.62</td>\n",
       "      <td>20.20</td>\n",
       "      <td>17.15</td>\n",
       "      <td>22.85</td>\n",
       "      <td>33.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.33</td>\n",
       "      <td>24.78</td>\n",
       "      <td>24.52</td>\n",
       "      <td>23.02</td>\n",
       "      <td>28.58</td>\n",
       "      <td>28.55</td>\n",
       "      <td>25.06</td>\n",
       "      <td>26.39</td>\n",
       "      <td>27.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.26</td>\n",
       "      <td>26.20</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.70</td>\n",
       "      <td>29.22</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.05</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.96</td>\n",
       "      <td>29.89</td>\n",
       "      <td>33.58</td>\n",
       "      <td>33.28</td>\n",
       "      <td>38.13</td>\n",
       "      <td>35.95</td>\n",
       "      <td>34.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.24</td>\n",
       "      <td>24.45</td>\n",
       "      <td>29.77</td>\n",
       "      <td>29.36</td>\n",
       "      <td>34.56</td>\n",
       "      <td>30.81</td>\n",
       "      <td>26.23</td>\n",
       "      <td>27.65</td>\n",
       "      <td>34.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.30</td>\n",
       "      <td>25.22</td>\n",
       "      <td>31.45</td>\n",
       "      <td>29.95</td>\n",
       "      <td>34.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>29.10</td>\n",
       "      <td>27.16</td>\n",
       "      <td>31.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.03</td>\n",
       "      <td>30.02</td>\n",
       "      <td>34.72</td>\n",
       "      <td>32.49</td>\n",
       "      <td>35.12</td>\n",
       "      <td>35.82</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.21</td>\n",
       "      <td>27.84</td>\n",
       "      <td>33.51</td>\n",
       "      <td>32.70</td>\n",
       "      <td>37.10</td>\n",
       "      <td>34.75</td>\n",
       "      <td>29.17</td>\n",
       "      <td>27.61</td>\n",
       "      <td>29.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.50</td>\n",
       "      <td>22.99</td>\n",
       "      <td>30.11</td>\n",
       "      <td>31.82</td>\n",
       "      <td>32.80</td>\n",
       "      <td>33.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>36.02</td>\n",
       "      <td>44.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.97</td>\n",
       "      <td>21.03</td>\n",
       "      <td>25.88</td>\n",
       "      <td>24.37</td>\n",
       "      <td>28.26</td>\n",
       "      <td>26.10</td>\n",
       "      <td>21.49</td>\n",
       "      <td>20.61</td>\n",
       "      <td>26.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>22.61</td>\n",
       "      <td>27.12</td>\n",
       "      <td>27.53</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.98</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.92</td>\n",
       "      <td>25.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.57</td>\n",
       "      <td>17.69</td>\n",
       "      <td>21.43</td>\n",
       "      <td>22.13</td>\n",
       "      <td>22.79</td>\n",
       "      <td>24.23</td>\n",
       "      <td>23.32</td>\n",
       "      <td>21.47</td>\n",
       "      <td>23.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.19</td>\n",
       "      <td>15.84</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.86</td>\n",
       "      <td>15.93</td>\n",
       "      <td>16.22</td>\n",
       "      <td>15.55</td>\n",
       "      <td>15.47</td>\n",
       "      <td>24.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.95</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>15.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>13.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "15   0.48   0.70   1.80   2.18   0.98   3.56   2.99   6.80   9.87\n",
       "16   2.67   7.45   8.62  10.08  10.06  11.52  17.00  19.79  20.92\n",
       "17   3.80   8.55   8.15  10.49  10.84  12.03  15.05  18.24  25.56\n",
       "18   7.74  14.78  19.70  18.93  22.60  20.57  16.19  15.66  32.96\n",
       "19   9.47  19.06  21.52  21.87  23.62  20.20  17.15  22.85  33.97\n",
       "20  14.33  24.78  24.52  23.02  28.58  28.55  25.06  26.39  27.24\n",
       "21  15.26  26.20  24.57  24.70  29.22  31.11  31.05  31.59  32.45\n",
       "22  15.96  29.89  33.58  33.28  38.13  35.95  34.24  33.24  33.78\n",
       "23  14.24  24.45  29.77  29.36  34.56  30.81  26.23  27.65  34.53\n",
       "24  14.30  25.22  31.45  29.95  34.70  31.31  29.10  27.16  31.12\n",
       "25  16.03  30.02  34.72  32.49  35.12  35.82  33.60  32.73  34.11\n",
       "26  15.21  27.84  33.51  32.70  37.10  34.75  29.17  27.61  29.70\n",
       "27   9.50  22.99  30.11  31.82  32.80  33.88  19.43  36.02  44.45\n",
       "28  10.97  21.03  25.88  24.37  28.26  26.10  21.49  20.61  26.21\n",
       "29  12.63  22.61  27.12  27.53  31.02  29.98  27.37  22.92  25.82\n",
       "30   8.57  17.69  21.43  22.13  22.79  24.23  23.32  21.47  23.67\n",
       "31   7.19  15.84  18.83  17.86  15.93  16.22  15.55  15.47  24.20\n",
       "32   2.95   8.16   8.37   7.24   6.39   6.81   6.33   6.51  15.83\n",
       "33   1.71   4.94   4.62   3.24   3.42   1.19   3.41  -0.50  13.80\n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17\n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17\n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4349 - val_loss: 1.6000\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4216 - val_loss: 1.6015\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4128 - val_loss: 1.5976\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4163 - val_loss: 1.5969\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4228 - val_loss: 1.6028\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4143 - val_loss: 1.6123\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4169 - val_loss: 1.6046\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4316 - val_loss: 1.5982\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4212 - val_loss: 1.5966\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4343 - val_loss: 1.5929\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4033 - val_loss: 1.6009\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4040 - val_loss: 1.6074\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4119 - val_loss: 1.5880\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3893 - val_loss: 1.5880\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4229 - val_loss: 1.5839\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4164 - val_loss: 1.5834\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4140 - val_loss: 1.5938\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4094 - val_loss: 1.6027\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4083 - val_loss: 1.5813\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4056 - val_loss: 1.5741\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4144 - val_loss: 1.5946\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3785 - val_loss: 1.5751\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3786 - val_loss: 1.6030\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3973 - val_loss: 1.6020\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.3790 - val_loss: 1.5686\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3752 - val_loss: 1.5674\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3881 - val_loss: 1.5901\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3807 - val_loss: 1.5945\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3684 - val_loss: 1.5761\n",
      "Epoch 30/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3778 - val_loss: 1.5786\n",
      "Epoch 31/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3859 - val_loss: 1.5628\n",
      "Epoch 32/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3851 - val_loss: 1.5553\n",
      "Epoch 33/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3658 - val_loss: 1.5736\n",
      "Epoch 34/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3692 - val_loss: 1.6187\n",
      "Epoch 35/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3765 - val_loss: 1.5580\n",
      "Epoch 36/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3879 - val_loss: 1.5732\n",
      "Epoch 37/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3730 - val_loss: 1.5862\n",
      "Epoch 00037: early stopping\n",
      "0.2\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2011 - val_loss: 2.5241\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1951 - val_loss: 2.5595\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1874 - val_loss: 2.5272\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1688 - val_loss: 2.5178\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1933 - val_loss: 2.5434\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1930 - val_loss: 2.5365\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1931 - val_loss: 2.5364\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2098 - val_loss: 2.5371\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1850 - val_loss: 2.5228\n",
      "Epoch 00009: early stopping\n",
      "0.3\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6042 - val_loss: 2.9723\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5792 - val_loss: 2.9720\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5767 - val_loss: 2.9646\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5482 - val_loss: 2.9829\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5885 - val_loss: 2.9738\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5854 - val_loss: 2.9865\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5898 - val_loss: 2.9822\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6268 - val_loss: 3.0030\n",
      "Epoch 00008: early stopping\n",
      "0.4\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6973 - val_loss: 3.0866\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6676 - val_loss: 3.0611\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6705 - val_loss: 3.0919\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6516 - val_loss: 3.0759\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6792 - val_loss: 3.1116\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6882 - val_loss: 3.1264\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7076 - val_loss: 3.0528\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7385 - val_loss: 3.1257\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6657 - val_loss: 3.1111\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7322 - val_loss: 3.0415\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6125 - val_loss: 3.0573\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6218 - val_loss: 3.0709\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6812 - val_loss: 3.0667\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6758 - val_loss: 3.1107\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6675 - val_loss: 3.1121\n",
      "Epoch 00015: early stopping\n",
      "0.5\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5776 - val_loss: 2.9105\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5338 - val_loss: 2.9305\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5406 - val_loss: 2.9119\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5273 - val_loss: 2.9162\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5401 - val_loss: 2.9679\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5625 - val_loss: 2.9946\n",
      "Epoch 00006: early stopping\n",
      "0.6\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2789 - val_loss: 2.6113\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2551 - val_loss: 2.5980\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2597 - val_loss: 2.6300\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2478 - val_loss: 2.6640\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2597 - val_loss: 2.6387\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2867 - val_loss: 2.5970\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2923 - val_loss: 2.5866\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3248 - val_loss: 2.6754\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2687 - val_loss: 2.6127\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3143 - val_loss: 2.5848\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2189 - val_loss: 2.6107\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2302 - val_loss: 2.6353\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2739 - val_loss: 2.6150\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2479 - val_loss: 2.6012\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2545 - val_loss: 2.6111\n",
      "Epoch 00015: early stopping\n",
      "0.7\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8647 - val_loss: 2.1376\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8500 - val_loss: 2.1665\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8549 - val_loss: 2.1493\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8509 - val_loss: 2.1371\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8460 - val_loss: 2.1521\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8668 - val_loss: 2.1243\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8771 - val_loss: 2.1108\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9045 - val_loss: 2.1723\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8556 - val_loss: 2.1826\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.8918 - val_loss: 2.1263\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.8179 - val_loss: 2.1367\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.8268 - val_loss: 2.1726\n",
      "Epoch 00012: early stopping\n",
      "0.8\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 4s 3ms/step - loss: 1.3614 - val_loss: 1.5507\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.3474 - val_loss: 1.5690\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.3602 - val_loss: 1.5716\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.3559 - val_loss: 1.5982\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.3446 - val_loss: 1.5887\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.3606 - val_loss: 1.5699\n",
      "Epoch 00006: early stopping\n",
      "0.9\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7710 - val_loss: 0.8679\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.7536 - val_loss: 0.8644\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7619 - val_loss: 0.9046\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7618 - val_loss: 0.9195\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.7564 - val_loss: 0.9080\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7677 - val_loss: 0.8664\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.7715 - val_loss: 0.8695\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001560</td>\n",
       "      <td>-0.003694</td>\n",
       "      <td>-0.000657</td>\n",
       "      <td>-0.003570</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.002829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001560</td>\n",
       "      <td>-0.003706</td>\n",
       "      <td>-0.000660</td>\n",
       "      <td>-0.003573</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.002832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001595</td>\n",
       "      <td>-0.003681</td>\n",
       "      <td>-0.000674</td>\n",
       "      <td>-0.003568</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.002820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001587</td>\n",
       "      <td>-0.003675</td>\n",
       "      <td>-0.000672</td>\n",
       "      <td>-0.003569</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.002818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001628</td>\n",
       "      <td>-0.003635</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.003560</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.002803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.001627</td>\n",
       "      <td>-0.003655</td>\n",
       "      <td>-0.000685</td>\n",
       "      <td>-0.003564</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.002808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.001660</td>\n",
       "      <td>-0.003625</td>\n",
       "      <td>-0.000696</td>\n",
       "      <td>-0.003557</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.002796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.001682</td>\n",
       "      <td>-0.003673</td>\n",
       "      <td>-0.000707</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.002806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.001745</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>-0.003544</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.001749</td>\n",
       "      <td>-0.003713</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>-0.003547</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.002806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.001813</td>\n",
       "      <td>-0.003699</td>\n",
       "      <td>-0.000747</td>\n",
       "      <td>-0.003536</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.002798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.001789</td>\n",
       "      <td>-0.003713</td>\n",
       "      <td>-0.000750</td>\n",
       "      <td>-0.003542</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.001849</td>\n",
       "      <td>-0.003660</td>\n",
       "      <td>-0.000744</td>\n",
       "      <td>-0.003516</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.002787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.001827</td>\n",
       "      <td>-0.003672</td>\n",
       "      <td>-0.000747</td>\n",
       "      <td>-0.003521</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.002790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.001909</td>\n",
       "      <td>-0.003602</td>\n",
       "      <td>-0.000735</td>\n",
       "      <td>-0.003483</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.002776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.217487</td>\n",
       "      <td>0.370709</td>\n",
       "      <td>0.717597</td>\n",
       "      <td>0.968850</td>\n",
       "      <td>1.463821</td>\n",
       "      <td>1.931874</td>\n",
       "      <td>4.018268</td>\n",
       "      <td>5.281368</td>\n",
       "      <td>9.595371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.263505</td>\n",
       "      <td>1.565225</td>\n",
       "      <td>2.827553</td>\n",
       "      <td>3.502896</td>\n",
       "      <td>7.535168</td>\n",
       "      <td>7.910530</td>\n",
       "      <td>12.770449</td>\n",
       "      <td>19.696402</td>\n",
       "      <td>24.831575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.651116</td>\n",
       "      <td>3.909996</td>\n",
       "      <td>5.529317</td>\n",
       "      <td>6.515191</td>\n",
       "      <td>11.297999</td>\n",
       "      <td>13.643661</td>\n",
       "      <td>16.212440</td>\n",
       "      <td>23.172733</td>\n",
       "      <td>27.890776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.921968</td>\n",
       "      <td>10.916505</td>\n",
       "      <td>15.097214</td>\n",
       "      <td>15.442690</td>\n",
       "      <td>23.635403</td>\n",
       "      <td>24.251202</td>\n",
       "      <td>27.351864</td>\n",
       "      <td>37.333565</td>\n",
       "      <td>36.890610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.142309</td>\n",
       "      <td>13.103320</td>\n",
       "      <td>16.525848</td>\n",
       "      <td>18.510332</td>\n",
       "      <td>26.018497</td>\n",
       "      <td>26.462925</td>\n",
       "      <td>29.467567</td>\n",
       "      <td>36.660442</td>\n",
       "      <td>36.888676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.817045</td>\n",
       "      <td>18.484957</td>\n",
       "      <td>24.269707</td>\n",
       "      <td>27.689884</td>\n",
       "      <td>36.856335</td>\n",
       "      <td>35.505989</td>\n",
       "      <td>37.783329</td>\n",
       "      <td>42.642441</td>\n",
       "      <td>39.159004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.064722</td>\n",
       "      <td>20.033958</td>\n",
       "      <td>26.404892</td>\n",
       "      <td>29.065340</td>\n",
       "      <td>37.942184</td>\n",
       "      <td>37.176495</td>\n",
       "      <td>39.161713</td>\n",
       "      <td>42.119129</td>\n",
       "      <td>38.262211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12.802986</td>\n",
       "      <td>22.785088</td>\n",
       "      <td>31.579567</td>\n",
       "      <td>31.946939</td>\n",
       "      <td>43.989319</td>\n",
       "      <td>41.387554</td>\n",
       "      <td>43.941307</td>\n",
       "      <td>49.510254</td>\n",
       "      <td>49.860203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.316620</td>\n",
       "      <td>21.869175</td>\n",
       "      <td>29.606476</td>\n",
       "      <td>29.822426</td>\n",
       "      <td>41.574654</td>\n",
       "      <td>39.896324</td>\n",
       "      <td>40.341465</td>\n",
       "      <td>47.848412</td>\n",
       "      <td>50.408634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13.185373</td>\n",
       "      <td>23.259563</td>\n",
       "      <td>33.345909</td>\n",
       "      <td>30.728445</td>\n",
       "      <td>42.558655</td>\n",
       "      <td>38.491875</td>\n",
       "      <td>39.778130</td>\n",
       "      <td>44.432903</td>\n",
       "      <td>48.182072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.224789</td>\n",
       "      <td>23.614647</td>\n",
       "      <td>33.593243</td>\n",
       "      <td>30.984320</td>\n",
       "      <td>43.197384</td>\n",
       "      <td>39.055225</td>\n",
       "      <td>42.101261</td>\n",
       "      <td>45.086334</td>\n",
       "      <td>46.217648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.953168</td>\n",
       "      <td>23.965050</td>\n",
       "      <td>35.199287</td>\n",
       "      <td>28.765471</td>\n",
       "      <td>40.359009</td>\n",
       "      <td>36.940216</td>\n",
       "      <td>38.734943</td>\n",
       "      <td>41.545818</td>\n",
       "      <td>44.681213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.873869</td>\n",
       "      <td>20.579067</td>\n",
       "      <td>33.434940</td>\n",
       "      <td>25.716551</td>\n",
       "      <td>35.304718</td>\n",
       "      <td>34.529945</td>\n",
       "      <td>34.311298</td>\n",
       "      <td>40.939774</td>\n",
       "      <td>50.587456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.637529</td>\n",
       "      <td>20.093632</td>\n",
       "      <td>31.691814</td>\n",
       "      <td>22.878805</td>\n",
       "      <td>31.852257</td>\n",
       "      <td>32.335869</td>\n",
       "      <td>32.638359</td>\n",
       "      <td>35.295261</td>\n",
       "      <td>39.694370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.787725</td>\n",
       "      <td>20.815420</td>\n",
       "      <td>32.242233</td>\n",
       "      <td>23.599123</td>\n",
       "      <td>33.087364</td>\n",
       "      <td>32.788445</td>\n",
       "      <td>33.557171</td>\n",
       "      <td>35.211308</td>\n",
       "      <td>39.511925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>12.859728</td>\n",
       "      <td>17.817818</td>\n",
       "      <td>26.582247</td>\n",
       "      <td>18.835241</td>\n",
       "      <td>27.694365</td>\n",
       "      <td>28.761845</td>\n",
       "      <td>28.606117</td>\n",
       "      <td>30.283297</td>\n",
       "      <td>33.269928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10.659247</td>\n",
       "      <td>14.790532</td>\n",
       "      <td>22.095211</td>\n",
       "      <td>15.044044</td>\n",
       "      <td>22.153427</td>\n",
       "      <td>25.473621</td>\n",
       "      <td>24.275826</td>\n",
       "      <td>26.628242</td>\n",
       "      <td>32.417522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6.759304</td>\n",
       "      <td>8.849477</td>\n",
       "      <td>11.784149</td>\n",
       "      <td>6.213902</td>\n",
       "      <td>9.909915</td>\n",
       "      <td>15.241508</td>\n",
       "      <td>14.739812</td>\n",
       "      <td>15.326421</td>\n",
       "      <td>23.069670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.507624</td>\n",
       "      <td>3.584225</td>\n",
       "      <td>6.191196</td>\n",
       "      <td>2.504045</td>\n",
       "      <td>3.554738</td>\n",
       "      <td>7.326305</td>\n",
       "      <td>7.813602</td>\n",
       "      <td>12.271775</td>\n",
       "      <td>16.404871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.002004</td>\n",
       "      <td>-0.003172</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.003220</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.002828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.001990</td>\n",
       "      <td>-0.003168</td>\n",
       "      <td>-0.000602</td>\n",
       "      <td>-0.003219</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.025713</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>0.002829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.001981</td>\n",
       "      <td>-0.003121</td>\n",
       "      <td>-0.000594</td>\n",
       "      <td>-0.003220</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.001967</td>\n",
       "      <td>-0.003120</td>\n",
       "      <td>-0.000594</td>\n",
       "      <td>-0.003219</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.002854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.001947</td>\n",
       "      <td>-0.003081</td>\n",
       "      <td>-0.000590</td>\n",
       "      <td>-0.003221</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.002874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.001933</td>\n",
       "      <td>-0.003080</td>\n",
       "      <td>-0.000589</td>\n",
       "      <td>-0.003221</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.002873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.001904</td>\n",
       "      <td>-0.003053</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>-0.003237</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.001890</td>\n",
       "      <td>-0.003055</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>-0.003237</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.001860</td>\n",
       "      <td>-0.003039</td>\n",
       "      <td>-0.000596</td>\n",
       "      <td>-0.003259</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.002911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.001842</td>\n",
       "      <td>-0.003040</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>-0.003259</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.002911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.001825</td>\n",
       "      <td>-0.003026</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.003280</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.001820</td>\n",
       "      <td>-0.003028</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.003280</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.001810</td>\n",
       "      <td>-0.003017</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.004512</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>-0.000152</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.002945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.001810</td>\n",
       "      <td>-0.003017</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.004512</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>-0.000152</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.002945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.001560  -0.003694  -0.000657  -0.003570   0.001820   0.001609   \n",
       "1   -0.001560  -0.003706  -0.000660  -0.003573   0.001820   0.001609   \n",
       "2   -0.001595  -0.003681  -0.000674  -0.003568   0.001821   0.001612   \n",
       "3   -0.001587  -0.003675  -0.000672  -0.003569   0.001821   0.001612   \n",
       "4   -0.001628  -0.003635  -0.000681  -0.003560   0.001822   0.001616   \n",
       "5   -0.001627  -0.003655  -0.000685  -0.003564   0.001822   0.001615   \n",
       "6   -0.001660  -0.003625  -0.000696  -0.003557   0.001823   0.001619   \n",
       "7   -0.001682  -0.003673  -0.000707  -0.003555   0.001823   0.001618   \n",
       "8   -0.001745  -0.003674  -0.000726  -0.003544   0.001825   0.001620   \n",
       "9   -0.001749  -0.003713  -0.000736  -0.003547   0.001824   0.001619   \n",
       "10  -0.001813  -0.003699  -0.000747  -0.003536   0.001826   0.001622   \n",
       "11  -0.001789  -0.003713  -0.000750  -0.003542   0.001826   0.001622   \n",
       "12  -0.001849  -0.003660  -0.000744  -0.003516   0.001827   0.001626   \n",
       "13  -0.001827  -0.003672  -0.000747  -0.003521   0.001827   0.001625   \n",
       "14  -0.001909  -0.003602  -0.000735  -0.003483   0.001829   0.001629   \n",
       "15   0.217487   0.370709   0.717597   0.968850   1.463821   1.931874   \n",
       "16   1.263505   1.565225   2.827553   3.502896   7.535168   7.910530   \n",
       "17   2.651116   3.909996   5.529317   6.515191  11.297999  13.643661   \n",
       "18   4.921968  10.916505  15.097214  15.442690  23.635403  24.251202   \n",
       "19   7.142309  13.103320  16.525848  18.510332  26.018497  26.462925   \n",
       "20  10.817045  18.484957  24.269707  27.689884  36.856335  35.505989   \n",
       "21  11.064722  20.033958  26.404892  29.065340  37.942184  37.176495   \n",
       "22  12.802986  22.785088  31.579567  31.946939  43.989319  41.387554   \n",
       "23  12.316620  21.869175  29.606476  29.822426  41.574654  39.896324   \n",
       "24  13.185373  23.259563  33.345909  30.728445  42.558655  38.491875   \n",
       "25  13.224789  23.614647  33.593243  30.984320  43.197384  39.055225   \n",
       "26  13.953168  23.965050  35.199287  28.765471  40.359009  36.940216   \n",
       "27  12.873869  20.579067  33.434940  25.716551  35.304718  34.529945   \n",
       "28  13.637529  20.093632  31.691814  22.878805  31.852257  32.335869   \n",
       "29  13.787725  20.815420  32.242233  23.599123  33.087364  32.788445   \n",
       "30  12.859728  17.817818  26.582247  18.835241  27.694365  28.761845   \n",
       "31  10.659247  14.790532  22.095211  15.044044  22.153427  25.473621   \n",
       "32   6.759304   8.849477  11.784149   6.213902   9.909915  15.241508   \n",
       "33   2.507624   3.584225   6.191196   2.504045   3.554738   7.326305   \n",
       "34  -0.002004  -0.003172  -0.000603  -0.003220   0.001838   0.036200   \n",
       "35  -0.001990  -0.003168  -0.000602  -0.003219   0.001838   0.025713   \n",
       "36  -0.001981  -0.003121  -0.000594  -0.003220   0.001837   0.014078   \n",
       "37  -0.001967  -0.003120  -0.000594  -0.003219   0.001837   0.006271   \n",
       "38  -0.001947  -0.003081  -0.000590  -0.003221   0.001837   0.006752   \n",
       "39  -0.001933  -0.003080  -0.000589  -0.003221   0.001837   0.001624   \n",
       "40  -0.001904  -0.003053  -0.000591  -0.003237   0.001835   0.001620   \n",
       "41  -0.001890  -0.003055  -0.000591  -0.003237   0.001835   0.001620   \n",
       "42  -0.001860  -0.003039  -0.000596  -0.003259   0.001834   0.001615   \n",
       "43  -0.001842  -0.003040  -0.000595  -0.003259   0.001834   0.001615   \n",
       "44  -0.001825  -0.003026  -0.000600  -0.003280   0.001832   0.001612   \n",
       "45  -0.001820  -0.003028  -0.000600  -0.003280   0.001832   0.001612   \n",
       "46  -0.001810  -0.003017  -0.000603  -0.004512   0.001831   0.001609   \n",
       "47  -0.001810  -0.003017  -0.000603  -0.004512   0.001831   0.001609   \n",
       "\n",
       "            0          0          0  \n",
       "0   -0.000083   0.002688   0.002829  \n",
       "1   -0.000084   0.002691   0.002832  \n",
       "2   -0.000073   0.002662   0.002820  \n",
       "3   -0.000072   0.002660   0.002818  \n",
       "4   -0.000059   0.002629   0.002803  \n",
       "5   -0.000062   0.002634   0.002808  \n",
       "6   -0.000050   0.002606   0.002796  \n",
       "7   -0.000056   0.002616   0.002806  \n",
       "8   -0.000048   0.002593   0.002800  \n",
       "9   -0.000053   0.002599   0.002806  \n",
       "10  -0.000047   0.002574   0.002798  \n",
       "11  -0.000049   0.002576   0.002801  \n",
       "12  -0.000035   0.002546   0.002787  \n",
       "13  -0.000037   0.002548   0.002790  \n",
       "14  -0.000020   0.002518   0.002776  \n",
       "15   4.018268   5.281368   9.595371  \n",
       "16  12.770449  19.696402  24.831575  \n",
       "17  16.212440  23.172733  27.890776  \n",
       "18  27.351864  37.333565  36.890610  \n",
       "19  29.467567  36.660442  36.888676  \n",
       "20  37.783329  42.642441  39.159004  \n",
       "21  39.161713  42.119129  38.262211  \n",
       "22  43.941307  49.510254  49.860203  \n",
       "23  40.341465  47.848412  50.408634  \n",
       "24  39.778130  44.432903  48.182072  \n",
       "25  42.101261  45.086334  46.217648  \n",
       "26  38.734943  41.545818  44.681213  \n",
       "27  34.311298  40.939774  50.587456  \n",
       "28  32.638359  35.295261  39.694370  \n",
       "29  33.557171  35.211308  39.511925  \n",
       "30  28.606117  30.283297  33.269928  \n",
       "31  24.275826  26.628242  32.417522  \n",
       "32  14.739812  15.326421  23.069670  \n",
       "33   7.813602  12.271775  16.404871  \n",
       "34  -0.000039   0.002477   0.002828  \n",
       "35  -0.000041   0.002479   0.002829  \n",
       "36  -0.000057   0.002505   0.002853  \n",
       "37  -0.000059   0.002506   0.002854  \n",
       "38  -0.000078   0.002532   0.002874  \n",
       "39  -0.000079   0.002533   0.002873  \n",
       "40  -0.000100   0.002564   0.002892  \n",
       "41  -0.000101   0.002564   0.002892  \n",
       "42  -0.000121   0.002592   0.002911  \n",
       "43  -0.000123   0.002594   0.002911  \n",
       "44  -0.000141   0.002620   0.002933  \n",
       "45  -0.000141   0.002619   0.002933  \n",
       "46  -0.000152   0.002634   0.002945  \n",
       "47  -0.000152   0.002634   0.002945  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model7.fit(np.array(Day).reshape(52464, 1, 7), np.array(Day7).reshape(52464, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred7 = np.squeeze(model7.predict(np.array(df_test).reshape(3888, 1, 7)))\n",
    "    pred7 = pd.DataFrame(pred7)\n",
    "    result7 = pd.concat([result7, pred7], axis=1)\n",
    "    \n",
    "result7[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4555 - val_loss: 1.6458\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4449 - val_loss: 1.6420\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4339 - val_loss: 1.6405\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4252 - val_loss: 1.6398\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4423 - val_loss: 1.6398\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4344 - val_loss: 1.6460\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4438 - val_loss: 1.6485\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4514 - val_loss: 1.6387\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4509 - val_loss: 1.6418\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4603 - val_loss: 1.6346\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4269 - val_loss: 1.6485\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4376 - val_loss: 1.6359\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4420 - val_loss: 1.6319\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4215 - val_loss: 1.6501\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4475 - val_loss: 1.6504\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4359 - val_loss: 1.6279\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4266 - val_loss: 1.6324\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4402 - val_loss: 1.6356\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4335 - val_loss: 1.6247\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4358 - val_loss: 1.6221\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4288 - val_loss: 1.6202\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4181 - val_loss: 1.6245\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4138 - val_loss: 1.6268\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4174 - val_loss: 1.6246\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4294 - val_loss: 1.6154\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4170 - val_loss: 1.6123\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4224 - val_loss: 1.6320\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4132 - val_loss: 1.6343\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4021 - val_loss: 1.6166\n",
      "Epoch 30/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4273 - val_loss: 1.6204\n",
      "Epoch 31/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4281 - val_loss: 1.6046\n",
      "Epoch 32/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4137 - val_loss: 1.6378\n",
      "Epoch 33/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4157 - val_loss: 1.6069\n",
      "Epoch 34/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4057 - val_loss: 1.5999\n",
      "Epoch 35/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.3990 - val_loss: 1.6001\n",
      "Epoch 36/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4141 - val_loss: 1.6527\n",
      "Epoch 37/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4098 - val_loss: 1.5966\n",
      "Epoch 38/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4152 - val_loss: 1.6071\n",
      "Epoch 39/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4193 - val_loss: 1.5965\n",
      "Epoch 40/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4271 - val_loss: 1.6320\n",
      "Epoch 41/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3934 - val_loss: 1.5942\n",
      "Epoch 42/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4014 - val_loss: 1.5920\n",
      "Epoch 43/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4093 - val_loss: 1.5966\n",
      "Epoch 44/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3897 - val_loss: 1.6071\n",
      "Epoch 45/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3957 - val_loss: 1.6004\n",
      "Epoch 46/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4129 - val_loss: 1.5983\n",
      "Epoch 47/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4135 - val_loss: 1.5929\n",
      "Epoch 00047: early stopping\n",
      "0.2\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 2.3421 - val_loss: 2.6424\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3182 - val_loss: 2.6441\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3040 - val_loss: 2.6315\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2939 - val_loss: 2.6340\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3109 - val_loss: 2.6224\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3086 - val_loss: 2.6135\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3225 - val_loss: 2.6422\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3176 - val_loss: 2.6488\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3193 - val_loss: 2.6388\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3323 - val_loss: 2.6343\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2804 - val_loss: 2.6492\n",
      "Epoch 00011: early stopping\n",
      "0.3\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7918 - val_loss: 3.1281\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7809 - val_loss: 3.1632\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7567 - val_loss: 3.1517\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7499 - val_loss: 3.0989\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7596 - val_loss: 3.2055\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7631 - val_loss: 3.1344\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7806 - val_loss: 3.1702\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7800 - val_loss: 3.1208\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7719 - val_loss: 3.1428\n",
      "Epoch 00009: early stopping\n",
      "0.4\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.9085 - val_loss: 3.2114\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8850 - val_loss: 3.2769\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8730 - val_loss: 3.2382\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8692 - val_loss: 3.2156\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8781 - val_loss: 3.3161\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8509 - val_loss: 3.2196\n",
      "Epoch 00006: early stopping\n",
      "0.5\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7481 - val_loss: 3.0884\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7337 - val_loss: 3.0842\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7314 - val_loss: 3.0814\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7314 - val_loss: 3.2325\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7550 - val_loss: 3.1091\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7065 - val_loss: 3.0738\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7544 - val_loss: 3.1246\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7526 - val_loss: 3.1195\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7264 - val_loss: 3.0705\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7549 - val_loss: 3.0592\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6973 - val_loss: 3.0757\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7240 - val_loss: 3.0958\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7351 - val_loss: 3.0838\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7281 - val_loss: 3.1608\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7249 - val_loss: 3.0691\n",
      "Epoch 00015: early stopping\n",
      "0.6\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4266 - val_loss: 2.7432\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4165 - val_loss: 2.7410\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4018 - val_loss: 2.7729\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4168 - val_loss: 2.8463\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4203 - val_loss: 2.7274\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3924 - val_loss: 2.7267\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4340 - val_loss: 2.7215\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4227 - val_loss: 2.8117\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4063 - val_loss: 2.7450\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4293 - val_loss: 2.7392\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3718 - val_loss: 2.7416\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4050 - val_loss: 2.7564\n",
      "Epoch 00012: early stopping\n",
      "0.7\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 1.9790 - val_loss: 2.2342\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9771 - val_loss: 2.2376\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9730 - val_loss: 2.2753\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9754 - val_loss: 2.2251\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9718 - val_loss: 2.2426\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9520 - val_loss: 2.2283\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9845 - val_loss: 2.2528\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9849 - val_loss: 2.3595\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9593 - val_loss: 2.2196\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9777 - val_loss: 2.2228\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9284 - val_loss: 2.2386\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9657 - val_loss: 2.2839\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9620 - val_loss: 2.2954\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9715 - val_loss: 2.2458\n",
      "Epoch 00014: early stopping\n",
      "0.8\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 1.4420 - val_loss: 1.6461\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4377 - val_loss: 1.6340\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4345 - val_loss: 1.6521\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4350 - val_loss: 1.6329\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4405 - val_loss: 1.6556\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4171 - val_loss: 1.6322\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4417 - val_loss: 1.6752\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4423 - val_loss: 1.6717\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4150 - val_loss: 1.6117\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4364 - val_loss: 1.5967\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3989 - val_loss: 1.6214\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4214 - val_loss: 1.6417\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4209 - val_loss: 1.6491\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4331 - val_loss: 1.6333\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4165 - val_loss: 1.6034\n",
      "Epoch 00015: early stopping\n",
      "0.9\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.8045 - val_loss: 0.9010\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7936 - val_loss: 0.8980\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7923 - val_loss: 0.8904\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7991 - val_loss: 0.8916\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7987 - val_loss: 0.8992\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7851 - val_loss: 0.8832\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7962 - val_loss: 0.8773\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7966 - val_loss: 0.9016\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7902 - val_loss: 0.9001\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7959 - val_loss: 0.8971\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7787 - val_loss: 0.8817\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7850 - val_loss: 0.8957\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.455512</td>\n",
       "      <td>1.289627</td>\n",
       "      <td>1.804587</td>\n",
       "      <td>1.891885</td>\n",
       "      <td>2.222310</td>\n",
       "      <td>3.247532</td>\n",
       "      <td>4.916279</td>\n",
       "      <td>7.519242</td>\n",
       "      <td>14.791611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.657450</td>\n",
       "      <td>1.484820</td>\n",
       "      <td>2.239498</td>\n",
       "      <td>3.867980</td>\n",
       "      <td>5.455654</td>\n",
       "      <td>8.843503</td>\n",
       "      <td>11.721914</td>\n",
       "      <td>16.088552</td>\n",
       "      <td>22.455490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.732489</td>\n",
       "      <td>3.017443</td>\n",
       "      <td>6.314467</td>\n",
       "      <td>7.922718</td>\n",
       "      <td>11.270049</td>\n",
       "      <td>12.549812</td>\n",
       "      <td>15.320793</td>\n",
       "      <td>21.033401</td>\n",
       "      <td>25.756531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.044663</td>\n",
       "      <td>10.607622</td>\n",
       "      <td>14.568073</td>\n",
       "      <td>19.367455</td>\n",
       "      <td>23.146391</td>\n",
       "      <td>24.868616</td>\n",
       "      <td>27.310402</td>\n",
       "      <td>33.952354</td>\n",
       "      <td>37.765488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.933978</td>\n",
       "      <td>14.727671</td>\n",
       "      <td>17.189114</td>\n",
       "      <td>21.736059</td>\n",
       "      <td>24.844507</td>\n",
       "      <td>26.523149</td>\n",
       "      <td>28.077097</td>\n",
       "      <td>34.514900</td>\n",
       "      <td>37.749832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.251307</td>\n",
       "      <td>26.632404</td>\n",
       "      <td>26.209330</td>\n",
       "      <td>28.940351</td>\n",
       "      <td>34.297390</td>\n",
       "      <td>33.500011</td>\n",
       "      <td>33.389790</td>\n",
       "      <td>37.897808</td>\n",
       "      <td>38.192883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.216282</td>\n",
       "      <td>29.506489</td>\n",
       "      <td>28.786219</td>\n",
       "      <td>31.273186</td>\n",
       "      <td>36.421467</td>\n",
       "      <td>34.728336</td>\n",
       "      <td>35.158245</td>\n",
       "      <td>36.659336</td>\n",
       "      <td>38.926994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16.034107</td>\n",
       "      <td>32.141808</td>\n",
       "      <td>34.751511</td>\n",
       "      <td>35.152252</td>\n",
       "      <td>42.162590</td>\n",
       "      <td>40.770943</td>\n",
       "      <td>40.833782</td>\n",
       "      <td>47.415546</td>\n",
       "      <td>54.691406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.363009</td>\n",
       "      <td>28.218748</td>\n",
       "      <td>31.168127</td>\n",
       "      <td>33.481579</td>\n",
       "      <td>39.392181</td>\n",
       "      <td>38.961308</td>\n",
       "      <td>39.417595</td>\n",
       "      <td>45.403435</td>\n",
       "      <td>51.403461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.383842</td>\n",
       "      <td>29.130327</td>\n",
       "      <td>31.997326</td>\n",
       "      <td>35.512280</td>\n",
       "      <td>41.059311</td>\n",
       "      <td>38.258926</td>\n",
       "      <td>38.865524</td>\n",
       "      <td>46.038471</td>\n",
       "      <td>53.363018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15.253213</td>\n",
       "      <td>30.159016</td>\n",
       "      <td>33.413010</td>\n",
       "      <td>35.432720</td>\n",
       "      <td>42.015141</td>\n",
       "      <td>38.286358</td>\n",
       "      <td>39.619781</td>\n",
       "      <td>45.781116</td>\n",
       "      <td>50.680790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.856067</td>\n",
       "      <td>27.230789</td>\n",
       "      <td>30.477161</td>\n",
       "      <td>34.277344</td>\n",
       "      <td>39.954723</td>\n",
       "      <td>36.209011</td>\n",
       "      <td>36.913338</td>\n",
       "      <td>43.789936</td>\n",
       "      <td>51.023136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.615381</td>\n",
       "      <td>25.779484</td>\n",
       "      <td>30.170456</td>\n",
       "      <td>34.959579</td>\n",
       "      <td>37.084499</td>\n",
       "      <td>36.078312</td>\n",
       "      <td>37.943287</td>\n",
       "      <td>45.618927</td>\n",
       "      <td>56.449566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12.254308</td>\n",
       "      <td>24.005617</td>\n",
       "      <td>26.687132</td>\n",
       "      <td>28.984060</td>\n",
       "      <td>34.848316</td>\n",
       "      <td>31.599295</td>\n",
       "      <td>32.559647</td>\n",
       "      <td>37.675156</td>\n",
       "      <td>43.894585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.352975</td>\n",
       "      <td>24.185097</td>\n",
       "      <td>26.405628</td>\n",
       "      <td>28.979986</td>\n",
       "      <td>35.561253</td>\n",
       "      <td>31.669052</td>\n",
       "      <td>32.567074</td>\n",
       "      <td>37.798897</td>\n",
       "      <td>42.985523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10.753920</td>\n",
       "      <td>20.130112</td>\n",
       "      <td>19.914677</td>\n",
       "      <td>21.714460</td>\n",
       "      <td>28.539160</td>\n",
       "      <td>27.276321</td>\n",
       "      <td>27.829311</td>\n",
       "      <td>31.105719</td>\n",
       "      <td>31.695375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10.310396</td>\n",
       "      <td>19.151878</td>\n",
       "      <td>20.949842</td>\n",
       "      <td>21.168991</td>\n",
       "      <td>26.594475</td>\n",
       "      <td>25.041620</td>\n",
       "      <td>25.612881</td>\n",
       "      <td>28.769831</td>\n",
       "      <td>34.351337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7.064955</td>\n",
       "      <td>13.620182</td>\n",
       "      <td>13.221620</td>\n",
       "      <td>13.299637</td>\n",
       "      <td>15.546128</td>\n",
       "      <td>17.053446</td>\n",
       "      <td>17.084219</td>\n",
       "      <td>19.410355</td>\n",
       "      <td>24.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.237569</td>\n",
       "      <td>6.357405</td>\n",
       "      <td>8.290802</td>\n",
       "      <td>9.251082</td>\n",
       "      <td>7.904823</td>\n",
       "      <td>9.868147</td>\n",
       "      <td>10.396194</td>\n",
       "      <td>12.566361</td>\n",
       "      <td>23.967562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.002769</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "1    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "2    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "3    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "4    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "5    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "6    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "7    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "8    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "9    0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "10   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "11   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "12   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "13   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "14   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "15   0.455512   1.289627   1.804587   1.891885   2.222310   3.247532   \n",
       "16   0.657450   1.484820   2.239498   3.867980   5.455654   8.843503   \n",
       "17   1.732489   3.017443   6.314467   7.922718  11.270049  12.549812   \n",
       "18   6.044663  10.607622  14.568073  19.367455  23.146391  24.868616   \n",
       "19   7.933978  14.727671  17.189114  21.736059  24.844507  26.523149   \n",
       "20  14.251307  26.632404  26.209330  28.940351  34.297390  33.500011   \n",
       "21  15.216282  29.506489  28.786219  31.273186  36.421467  34.728336   \n",
       "22  16.034107  32.141808  34.751511  35.152252  42.162590  40.770943   \n",
       "23  14.363009  28.218748  31.168127  33.481579  39.392181  38.961308   \n",
       "24  14.383842  29.130327  31.997326  35.512280  41.059311  38.258926   \n",
       "25  15.253213  30.159016  33.413010  35.432720  42.015141  38.286358   \n",
       "26  13.856067  27.230789  30.477161  34.277344  39.954723  36.209011   \n",
       "27  12.615381  25.779484  30.170456  34.959579  37.084499  36.078312   \n",
       "28  12.254308  24.005617  26.687132  28.984060  34.848316  31.599295   \n",
       "29  12.352975  24.185097  26.405628  28.979986  35.561253  31.669052   \n",
       "30  10.753920  20.130112  19.914677  21.714460  28.539160  27.276321   \n",
       "31  10.310396  19.151878  20.949842  21.168991  26.594475  25.041620   \n",
       "32   7.064955  13.620182  13.221620  13.299637  15.546128  17.053446   \n",
       "33   2.237569   6.357405   8.290802   9.251082   7.904823   9.868147   \n",
       "34   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "35   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "36   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "37   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "38   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "39   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "40   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "41   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "42   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "43   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "44   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "45   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "46   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "47   0.000406  -0.002769   0.000489   0.000755  -0.000681  -0.000986   \n",
       "\n",
       "            0          0          0  \n",
       "0   -0.000322   0.000442   0.001392  \n",
       "1   -0.000322   0.000442   0.001392  \n",
       "2   -0.000322   0.000442   0.001392  \n",
       "3   -0.000322   0.000442   0.001392  \n",
       "4   -0.000322   0.000442   0.001392  \n",
       "5   -0.000322   0.000442   0.001392  \n",
       "6   -0.000322   0.000442   0.001392  \n",
       "7   -0.000322   0.000442   0.001392  \n",
       "8   -0.000322   0.000442   0.001392  \n",
       "9   -0.000322   0.000442   0.001392  \n",
       "10  -0.000322   0.000442   0.001392  \n",
       "11  -0.000322   0.000442   0.001392  \n",
       "12  -0.000322   0.000442   0.001392  \n",
       "13  -0.000322   0.000442   0.001392  \n",
       "14  -0.000322   0.000442   0.001392  \n",
       "15   4.916279   7.519242  14.791611  \n",
       "16  11.721914  16.088552  22.455490  \n",
       "17  15.320793  21.033401  25.756531  \n",
       "18  27.310402  33.952354  37.765488  \n",
       "19  28.077097  34.514900  37.749832  \n",
       "20  33.389790  37.897808  38.192883  \n",
       "21  35.158245  36.659336  38.926994  \n",
       "22  40.833782  47.415546  54.691406  \n",
       "23  39.417595  45.403435  51.403461  \n",
       "24  38.865524  46.038471  53.363018  \n",
       "25  39.619781  45.781116  50.680790  \n",
       "26  36.913338  43.789936  51.023136  \n",
       "27  37.943287  45.618927  56.449566  \n",
       "28  32.559647  37.675156  43.894585  \n",
       "29  32.567074  37.798897  42.985523  \n",
       "30  27.829311  31.105719  31.695375  \n",
       "31  25.612881  28.769831  34.351337  \n",
       "32  17.084219  19.410355  24.466400  \n",
       "33  10.396194  12.566361  23.967562  \n",
       "34  -0.000322   0.000442   0.001392  \n",
       "35  -0.000322   0.000442   0.001392  \n",
       "36  -0.000322   0.000442   0.001392  \n",
       "37  -0.000322   0.000442   0.001392  \n",
       "38  -0.000322   0.000442   0.001392  \n",
       "39  -0.000322   0.000442   0.001392  \n",
       "40  -0.000322   0.000442   0.001392  \n",
       "41  -0.000322   0.000442   0.001392  \n",
       "42  -0.000322   0.000442   0.001392  \n",
       "43  -0.000322   0.000442   0.001392  \n",
       "44  -0.000322   0.000442   0.001392  \n",
       "45  -0.000322   0.000442   0.001392  \n",
       "46  -0.000322   0.000442   0.001392  \n",
       "47  -0.000322   0.000442   0.001392  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8 = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model8.fit(np.array(Day).reshape(52464, 1, 7), np.array(Day8).reshape(52464, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred8 = np.squeeze(model8.predict(np.array(df_test).reshape(3888, 1, 7)))\n",
    "    pred8 = pd.DataFrame(pred8)\n",
    "    result8 = pd.concat([result8, pred8], axis=1)\n",
    "    \n",
    "result8[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 39348, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 39348, 7), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 39348, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 39348, 7), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "615/615 [==============================] - ETA: 0s - loss: 667.0030WARNING:tensorflow:Model was constructed with shape (None, 39348, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 39348, 7), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "615/615 [==============================] - 8s 5ms/step - loss: 666.7963 - val_loss: 380.6853\n",
      "Epoch 2/100\n",
      "615/615 [==============================] - 3s 4ms/step - loss: 319.0092 - val_loss: 238.7392\n",
      "Epoch 3/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 212.7998 - val_loss: 182.8196\n",
      "Epoch 4/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 176.4320 - val_loss: 158.7719\n",
      "Epoch 5/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 152.8708 - val_loss: 147.3703\n",
      "Epoch 6/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 147.1763 - val_loss: 147.1366\n",
      "Epoch 7/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 144.5910 - val_loss: 138.9911\n",
      "Epoch 8/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 136.6039 - val_loss: 138.7947\n",
      "Epoch 9/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 140.8756 - val_loss: 138.0776\n",
      "Epoch 10/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 138.4360 - val_loss: 143.1614\n",
      "Epoch 11/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 144.5264 - val_loss: 140.5794\n",
      "Epoch 12/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 139.0337 - val_loss: 136.3280\n",
      "Epoch 13/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 139.3175 - val_loss: 137.2588\n",
      "Epoch 14/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 138.7728 - val_loss: 135.7291\n",
      "Epoch 15/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 133.8104 - val_loss: 135.8686\n",
      "Epoch 16/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 138.7705 - val_loss: 133.9468\n",
      "Epoch 17/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 138.3800 - val_loss: 134.5036\n",
      "Epoch 18/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 135.1442 - val_loss: 135.4723\n",
      "Epoch 19/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 134.7349 - val_loss: 133.9675\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.GRU(units=64, return_sequences=True, input_shape=[39348, 7]),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(np.array(X_train_1).reshape(39348, 1, 7), np.array(Y_train_1).reshape(39348, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "pred = np.squeeze(model8.predict(np.array(X_valid_1).reshape(13116, 1, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "814/820 [============================>.] - ETA: 0s - loss: 1.5278WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 8s 5ms/step - loss: 1.5274 - val_loss: 1.6286\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4350 - val_loss: 1.6110\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4222 - val_loss: 1.6038\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4115 - val_loss: 1.6016\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4166 - val_loss: 1.5898\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4019 - val_loss: 1.6108\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4040 - val_loss: 1.5916\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4179 - val_loss: 1.5810\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3994 - val_loss: 1.5773\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4212 - val_loss: 1.5814\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3866 - val_loss: 1.5779\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3827 - val_loss: 1.5760\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3899 - val_loss: 1.5929\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3789 - val_loss: 1.5757\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4034 - val_loss: 1.5771\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3979 - val_loss: 1.5748\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3912 - val_loss: 1.5744\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3891 - val_loss: 1.5787\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3911 - val_loss: 1.5848\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 1.3956 - val_loss: 1.5711\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3997 - val_loss: 1.6197\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3748 - val_loss: 1.5685\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3751 - val_loss: 1.5943\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3869 - val_loss: 1.5775\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3783 - val_loss: 1.5849\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3662 - val_loss: 1.5717\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3817 - val_loss: 1.5711\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "803/820 [============================>.] - ETA: 0s - loss: 2.2600WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 8s 5ms/step - loss: 2.2599 - val_loss: 2.5647\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2248 - val_loss: 2.5507\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2175 - val_loss: 2.5538\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1957 - val_loss: 2.5482\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2215 - val_loss: 2.5403\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2016 - val_loss: 2.6022\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2165 - val_loss: 2.5405\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2305 - val_loss: 2.5648\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1998 - val_loss: 2.5402\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2386 - val_loss: 2.5372\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1657 - val_loss: 2.5409\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1668 - val_loss: 2.5327\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1998 - val_loss: 2.5435\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2014 - val_loss: 2.5365\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2035 - val_loss: 2.5433\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1993 - val_loss: 2.5390\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1992 - val_loss: 2.5414\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "807/820 [============================>.] - ETA: 0s - loss: 2.6442WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 8s 5ms/step - loss: 2.6441 - val_loss: 2.9995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6131 - val_loss: 2.9885\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6094 - val_loss: 2.9917\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5870 - val_loss: 2.9967\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6233 - val_loss: 2.9884\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6142 - val_loss: 3.0042\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6197 - val_loss: 2.9828\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6516 - val_loss: 2.9878\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5999 - val_loss: 2.9803\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6607 - val_loss: 2.9751\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5538 - val_loss: 2.9986\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5580 - val_loss: 2.9741\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6014 - val_loss: 2.9879\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6052 - val_loss: 2.9759\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5953 - val_loss: 3.0286\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5965 - val_loss: 2.9827\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6025 - val_loss: 3.0151\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "819/820 [============================>.] - ETA: 0s - loss: 2.7493WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 10s 5ms/step - loss: 2.7493 - val_loss: 3.1043\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7225 - val_loss: 3.0962\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7125 - val_loss: 3.1031\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6941 - val_loss: 3.0966\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7330 - val_loss: 3.1155\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7314 - val_loss: 3.0963\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7440 - val_loss: 3.0902\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7751 - val_loss: 3.0757\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7069 - val_loss: 3.0847\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7820 - val_loss: 3.0624\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6606 - val_loss: 3.0815\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6682 - val_loss: 3.0646\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7063 - val_loss: 3.0721\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7099 - val_loss: 3.0619\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7047 - val_loss: 3.1222\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6999 - val_loss: 3.0798\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7199 - val_loss: 3.1102\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7210 - val_loss: 3.0654\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7157 - val_loss: 3.0690\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "811/820 [============================>.] - ETA: 0s - loss: 2.6224WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 8s 5ms/step - loss: 2.6224 - val_loss: 2.9462\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5851 - val_loss: 2.9255\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5799 - val_loss: 2.9227\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5680 - val_loss: 2.9360\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5934 - val_loss: 2.9517\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6017 - val_loss: 2.9302\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6099 - val_loss: 2.8971\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6364 - val_loss: 2.9021\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5641 - val_loss: 2.9459\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6284 - val_loss: 2.8942\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5208 - val_loss: 2.8940\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.5218 - val_loss: 2.8876\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.5631 - val_loss: 2.9224\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5541 - val_loss: 2.9028\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5505 - val_loss: 2.8903\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 2.5393 - val_loss: 2.8943\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5694 - val_loss: 2.9039\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "812/820 [============================>.] - ETA: 0s - loss: 2.2992WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 9s 5ms/step - loss: 2.2992 - val_loss: 2.5828\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 2.2620 - val_loss: 2.5883\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 2.2705 - val_loss: 2.5933\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2582 - val_loss: 2.6166\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 2.2688 - val_loss: 2.6181\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 2.2792 - val_loss: 2.6371\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - ETA: 0s - loss: 1.8909WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.8909 - val_loss: 2.1346\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8525 - val_loss: 2.1136\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8684 - val_loss: 2.1850\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8594 - val_loss: 2.1656\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8594 - val_loss: 2.1437\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8708 - val_loss: 2.1406\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8827 - val_loss: 2.1077\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9014 - val_loss: 2.1227\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8529 - val_loss: 2.1501\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8904 - val_loss: 2.1077\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8180 - val_loss: 2.1207\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8295 - val_loss: 2.1658\n",
      "Epoch 00012: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "817/820 [============================>.] - ETA: 0s - loss: 1.3640WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.3640 - val_loss: 1.5409\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3351 - val_loss: 1.5567\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3575 - val_loss: 1.5581\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3473 - val_loss: 1.5729\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3496 - val_loss: 1.5716\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3572 - val_loss: 1.5614\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "815/820 [============================>.] - ETA: 0s - loss: 0.7589WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 0.7590 - val_loss: 0.8545\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7427 - val_loss: 0.8687\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7555 - val_loss: 0.8947\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7532 - val_loss: 0.8579\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7511 - val_loss: 0.8975\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7556 - val_loss: 0.8554\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023767</td>\n",
       "      <td>-0.002623</td>\n",
       "      <td>-0.005467</td>\n",
       "      <td>0.008433</td>\n",
       "      <td>0.005853</td>\n",
       "      <td>0.010430</td>\n",
       "      <td>-0.007161</td>\n",
       "      <td>-0.003821</td>\n",
       "      <td>0.007020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.023680</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>-0.005973</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>0.005832</td>\n",
       "      <td>0.010751</td>\n",
       "      <td>-0.007089</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>0.007331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.023393</td>\n",
       "      <td>-0.002849</td>\n",
       "      <td>-0.006229</td>\n",
       "      <td>0.007898</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.007504</td>\n",
       "      <td>-0.006562</td>\n",
       "      <td>-0.003360</td>\n",
       "      <td>0.007389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.023406</td>\n",
       "      <td>-0.002756</td>\n",
       "      <td>-0.005613</td>\n",
       "      <td>0.008238</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>-0.006382</td>\n",
       "      <td>-0.003389</td>\n",
       "      <td>0.007342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.023227</td>\n",
       "      <td>-0.002963</td>\n",
       "      <td>-0.005545</td>\n",
       "      <td>0.008920</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.008081</td>\n",
       "      <td>-0.003639</td>\n",
       "      <td>-0.002846</td>\n",
       "      <td>0.006686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.023235</td>\n",
       "      <td>-0.003056</td>\n",
       "      <td>-0.006488</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>0.008125</td>\n",
       "      <td>-0.004171</td>\n",
       "      <td>-0.002818</td>\n",
       "      <td>0.006740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.023190</td>\n",
       "      <td>-0.003287</td>\n",
       "      <td>-0.006752</td>\n",
       "      <td>0.009082</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.010395</td>\n",
       "      <td>-0.000611</td>\n",
       "      <td>-0.002114</td>\n",
       "      <td>0.005813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.023968</td>\n",
       "      <td>-0.003798</td>\n",
       "      <td>-0.009183</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>0.010440</td>\n",
       "      <td>-0.002044</td>\n",
       "      <td>-0.003035</td>\n",
       "      <td>0.005085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.025139</td>\n",
       "      <td>-0.004484</td>\n",
       "      <td>-0.010663</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>-0.004091</td>\n",
       "      <td>0.011560</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>-0.003804</td>\n",
       "      <td>0.003939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.025881</td>\n",
       "      <td>-0.004770</td>\n",
       "      <td>-0.011587</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>-0.004916</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>-0.004078</td>\n",
       "      <td>0.004025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.027172</td>\n",
       "      <td>-0.005348</td>\n",
       "      <td>-0.012226</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>-0.009961</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.003376</td>\n",
       "      <td>-0.005304</td>\n",
       "      <td>0.005648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.026915</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>-0.012120</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>-0.009467</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>-0.005750</td>\n",
       "      <td>0.005092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.028190</td>\n",
       "      <td>-0.005787</td>\n",
       "      <td>-0.012518</td>\n",
       "      <td>0.007388</td>\n",
       "      <td>-0.013862</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>0.010137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.027929</td>\n",
       "      <td>-0.005576</td>\n",
       "      <td>-0.012425</td>\n",
       "      <td>0.007271</td>\n",
       "      <td>-0.013308</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>-0.007404</td>\n",
       "      <td>0.009231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.030139</td>\n",
       "      <td>-0.006793</td>\n",
       "      <td>-0.013234</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>-0.015821</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.028768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.301327</td>\n",
       "      <td>0.434573</td>\n",
       "      <td>0.604994</td>\n",
       "      <td>1.578436</td>\n",
       "      <td>2.730470</td>\n",
       "      <td>3.824771</td>\n",
       "      <td>4.343204</td>\n",
       "      <td>6.021968</td>\n",
       "      <td>8.961095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.780095</td>\n",
       "      <td>2.728196</td>\n",
       "      <td>3.554694</td>\n",
       "      <td>3.529503</td>\n",
       "      <td>8.415113</td>\n",
       "      <td>9.745694</td>\n",
       "      <td>10.126165</td>\n",
       "      <td>15.724873</td>\n",
       "      <td>21.688187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.701501</td>\n",
       "      <td>5.805064</td>\n",
       "      <td>7.913376</td>\n",
       "      <td>8.203202</td>\n",
       "      <td>12.967743</td>\n",
       "      <td>14.637361</td>\n",
       "      <td>13.893641</td>\n",
       "      <td>21.231514</td>\n",
       "      <td>28.554996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.926049</td>\n",
       "      <td>9.661717</td>\n",
       "      <td>13.612925</td>\n",
       "      <td>16.185455</td>\n",
       "      <td>24.174677</td>\n",
       "      <td>27.917683</td>\n",
       "      <td>23.849176</td>\n",
       "      <td>35.980873</td>\n",
       "      <td>43.742489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.429761</td>\n",
       "      <td>13.309625</td>\n",
       "      <td>17.987631</td>\n",
       "      <td>19.105272</td>\n",
       "      <td>24.604090</td>\n",
       "      <td>28.482018</td>\n",
       "      <td>25.314758</td>\n",
       "      <td>35.063873</td>\n",
       "      <td>41.340168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12.553931</td>\n",
       "      <td>19.646955</td>\n",
       "      <td>24.934383</td>\n",
       "      <td>28.075687</td>\n",
       "      <td>31.176710</td>\n",
       "      <td>35.555386</td>\n",
       "      <td>34.878735</td>\n",
       "      <td>37.441326</td>\n",
       "      <td>39.558636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.868907</td>\n",
       "      <td>20.792971</td>\n",
       "      <td>25.975134</td>\n",
       "      <td>30.740225</td>\n",
       "      <td>32.785381</td>\n",
       "      <td>37.277153</td>\n",
       "      <td>37.875446</td>\n",
       "      <td>38.862137</td>\n",
       "      <td>39.862083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13.440016</td>\n",
       "      <td>22.746141</td>\n",
       "      <td>29.417183</td>\n",
       "      <td>35.266312</td>\n",
       "      <td>38.974670</td>\n",
       "      <td>42.391403</td>\n",
       "      <td>41.430561</td>\n",
       "      <td>46.661736</td>\n",
       "      <td>52.656006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11.953986</td>\n",
       "      <td>19.590410</td>\n",
       "      <td>25.665386</td>\n",
       "      <td>30.555098</td>\n",
       "      <td>37.200592</td>\n",
       "      <td>40.619610</td>\n",
       "      <td>37.413563</td>\n",
       "      <td>44.377457</td>\n",
       "      <td>51.620369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11.369281</td>\n",
       "      <td>18.913881</td>\n",
       "      <td>24.541300</td>\n",
       "      <td>30.647467</td>\n",
       "      <td>35.917065</td>\n",
       "      <td>39.060734</td>\n",
       "      <td>36.913254</td>\n",
       "      <td>41.352577</td>\n",
       "      <td>47.399540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12.280924</td>\n",
       "      <td>20.874861</td>\n",
       "      <td>26.514498</td>\n",
       "      <td>33.174286</td>\n",
       "      <td>35.763313</td>\n",
       "      <td>39.474499</td>\n",
       "      <td>39.869926</td>\n",
       "      <td>42.210480</td>\n",
       "      <td>45.579853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.910115</td>\n",
       "      <td>18.442799</td>\n",
       "      <td>23.576628</td>\n",
       "      <td>30.485058</td>\n",
       "      <td>33.083977</td>\n",
       "      <td>36.637924</td>\n",
       "      <td>36.364105</td>\n",
       "      <td>38.417824</td>\n",
       "      <td>42.602364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.139075</td>\n",
       "      <td>14.252810</td>\n",
       "      <td>19.456827</td>\n",
       "      <td>27.152670</td>\n",
       "      <td>32.948395</td>\n",
       "      <td>36.890347</td>\n",
       "      <td>33.864090</td>\n",
       "      <td>40.997898</td>\n",
       "      <td>52.365910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.733068</td>\n",
       "      <td>14.910880</td>\n",
       "      <td>19.276785</td>\n",
       "      <td>26.136000</td>\n",
       "      <td>27.077944</td>\n",
       "      <td>30.635063</td>\n",
       "      <td>30.327812</td>\n",
       "      <td>32.306355</td>\n",
       "      <td>36.880199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9.124859</td>\n",
       "      <td>15.387950</td>\n",
       "      <td>19.606283</td>\n",
       "      <td>26.508562</td>\n",
       "      <td>27.182501</td>\n",
       "      <td>31.004055</td>\n",
       "      <td>31.293306</td>\n",
       "      <td>32.446217</td>\n",
       "      <td>35.761631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.013469</td>\n",
       "      <td>13.489043</td>\n",
       "      <td>16.852001</td>\n",
       "      <td>23.256371</td>\n",
       "      <td>22.010374</td>\n",
       "      <td>25.745207</td>\n",
       "      <td>27.289167</td>\n",
       "      <td>27.582920</td>\n",
       "      <td>29.087336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.592389</td>\n",
       "      <td>10.142919</td>\n",
       "      <td>13.652148</td>\n",
       "      <td>19.958630</td>\n",
       "      <td>18.500669</td>\n",
       "      <td>21.169724</td>\n",
       "      <td>22.198530</td>\n",
       "      <td>24.084312</td>\n",
       "      <td>28.108219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.550341</td>\n",
       "      <td>5.264950</td>\n",
       "      <td>7.862484</td>\n",
       "      <td>13.299493</td>\n",
       "      <td>11.035273</td>\n",
       "      <td>11.922475</td>\n",
       "      <td>12.066522</td>\n",
       "      <td>13.937607</td>\n",
       "      <td>18.057730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.479093</td>\n",
       "      <td>1.027330</td>\n",
       "      <td>1.736728</td>\n",
       "      <td>4.071378</td>\n",
       "      <td>5.387755</td>\n",
       "      <td>6.767647</td>\n",
       "      <td>6.130318</td>\n",
       "      <td>7.853072</td>\n",
       "      <td>14.699933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.031939</td>\n",
       "      <td>-0.009799</td>\n",
       "      <td>-0.007243</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>-0.005685</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.018877</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.046391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.031694</td>\n",
       "      <td>-0.009614</td>\n",
       "      <td>-0.007797</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>-0.004864</td>\n",
       "      <td>0.009141</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>0.013056</td>\n",
       "      <td>0.044147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.032016</td>\n",
       "      <td>-0.010339</td>\n",
       "      <td>-0.007636</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>-0.005323</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.014014</td>\n",
       "      <td>0.010773</td>\n",
       "      <td>0.043402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.031728</td>\n",
       "      <td>-0.010124</td>\n",
       "      <td>-0.007945</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>-0.005005</td>\n",
       "      <td>0.004965</td>\n",
       "      <td>0.013448</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.041957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.032189</td>\n",
       "      <td>-0.011010</td>\n",
       "      <td>-0.007792</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>-0.004972</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.037853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.031824</td>\n",
       "      <td>-0.010744</td>\n",
       "      <td>-0.008052</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>-0.004872</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.008907</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>0.036539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.032350</td>\n",
       "      <td>-0.011701</td>\n",
       "      <td>-0.007992</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>-0.004667</td>\n",
       "      <td>-0.000606</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.030534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.031957</td>\n",
       "      <td>-0.011350</td>\n",
       "      <td>-0.008145</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>-0.004733</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>0.029611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.032613</td>\n",
       "      <td>-0.012440</td>\n",
       "      <td>-0.008200</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.003057</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>0.022941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.031987</td>\n",
       "      <td>-0.011930</td>\n",
       "      <td>-0.008403</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>-0.004859</td>\n",
       "      <td>-0.003556</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>0.021618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.032466</td>\n",
       "      <td>-0.012825</td>\n",
       "      <td>-0.008583</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>-0.005415</td>\n",
       "      <td>-0.005786</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.003586</td>\n",
       "      <td>0.014632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.032377</td>\n",
       "      <td>-0.012723</td>\n",
       "      <td>-0.008605</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>-0.005462</td>\n",
       "      <td>-0.005893</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>0.014444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.033108</td>\n",
       "      <td>-0.013751</td>\n",
       "      <td>-0.008617</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>-0.006684</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>-0.006407</td>\n",
       "      <td>0.008403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.033108</td>\n",
       "      <td>-0.013751</td>\n",
       "      <td>-0.008617</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>-0.006684</td>\n",
       "      <td>-0.008414</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>-0.006407</td>\n",
       "      <td>0.008403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.023767  -0.002623  -0.005467   0.008433   0.005853   0.010430   \n",
       "1   -0.023680  -0.002665  -0.005973   0.008066   0.005832   0.010751   \n",
       "2   -0.023393  -0.002849  -0.006229   0.007898   0.000213   0.007504   \n",
       "3   -0.023406  -0.002756  -0.005613   0.008238   0.000344   0.007283   \n",
       "4   -0.023227  -0.002963  -0.005545   0.008920   0.000035   0.008081   \n",
       "5   -0.023235  -0.003056  -0.006488   0.008471  -0.000313   0.008125   \n",
       "6   -0.023190  -0.003287  -0.006752   0.009082   0.000552   0.010395   \n",
       "7   -0.023968  -0.003798  -0.009183   0.008247  -0.001208   0.010440   \n",
       "8   -0.025139  -0.004484  -0.010663   0.008522  -0.004091   0.011560   \n",
       "9   -0.025881  -0.004770  -0.011587   0.008291  -0.004916   0.011784   \n",
       "10  -0.027172  -0.005348  -0.012226   0.008276  -0.009961   0.010700   \n",
       "11  -0.026915  -0.005146  -0.012120   0.008151  -0.009467   0.010501   \n",
       "12  -0.028190  -0.005787  -0.012518   0.007388  -0.013862   0.006325   \n",
       "13  -0.027929  -0.005576  -0.012425   0.007271  -0.013308   0.006148   \n",
       "14  -0.030139  -0.006793  -0.013234   0.005768  -0.015821   0.001159   \n",
       "15   0.301327   0.434573   0.604994   1.578436   2.730470   3.824771   \n",
       "16   1.780095   2.728196   3.554694   3.529503   8.415113   9.745694   \n",
       "17   3.701501   5.805064   7.913376   8.203202  12.967743  14.637361   \n",
       "18   5.926049   9.661717  13.612925  16.185455  24.174677  27.917683   \n",
       "19   8.429761  13.309625  17.987631  19.105272  24.604090  28.482018   \n",
       "20  12.553931  19.646955  24.934383  28.075687  31.176710  35.555386   \n",
       "21  12.868907  20.792971  25.975134  30.740225  32.785381  37.277153   \n",
       "22  13.440016  22.746141  29.417183  35.266312  38.974670  42.391403   \n",
       "23  11.953986  19.590410  25.665386  30.555098  37.200592  40.619610   \n",
       "24  11.369281  18.913881  24.541300  30.647467  35.917065  39.060734   \n",
       "25  12.280924  20.874861  26.514498  33.174286  35.763313  39.474499   \n",
       "26  10.910115  18.442799  23.576628  30.485058  33.083977  36.637924   \n",
       "27   8.139075  14.252810  19.456827  27.152670  32.948395  36.890347   \n",
       "28   8.733068  14.910880  19.276785  26.136000  27.077944  30.635063   \n",
       "29   9.124859  15.387950  19.606283  26.508562  27.182501  31.004055   \n",
       "30   8.013469  13.489043  16.852001  23.256371  22.010374  25.745207   \n",
       "31   5.592389  10.142919  13.652148  19.958630  18.500669  21.169724   \n",
       "32   2.550341   5.264950   7.862484  13.299493  11.035273  11.922475   \n",
       "33   0.479093   1.027330   1.736728   4.071378   5.387755   6.767647   \n",
       "34  -0.031939  -0.009799  -0.007243   0.002875  -0.005685   0.009317   \n",
       "35  -0.031694  -0.009614  -0.007797   0.002727  -0.004864   0.009141   \n",
       "36  -0.032016  -0.010339  -0.007636   0.004901  -0.005323   0.005261   \n",
       "37  -0.031728  -0.010124  -0.007945   0.004514  -0.005005   0.004965   \n",
       "38  -0.032189  -0.011010  -0.007792   0.005945  -0.004972   0.002073   \n",
       "39  -0.031824  -0.010744  -0.008052   0.005333  -0.004872   0.001661   \n",
       "40  -0.032350  -0.011701  -0.007992   0.005954  -0.004667  -0.000606   \n",
       "41  -0.031957  -0.011350  -0.008145   0.005376  -0.004733  -0.001025   \n",
       "42  -0.032613  -0.012440  -0.008200   0.005204  -0.004725  -0.003057   \n",
       "43  -0.031987  -0.011930  -0.008403   0.004197  -0.004859  -0.003556   \n",
       "44  -0.032466  -0.012825  -0.008583   0.002909  -0.005415  -0.005786   \n",
       "45  -0.032377  -0.012723  -0.008605   0.002788  -0.005462  -0.005893   \n",
       "46  -0.033108  -0.013751  -0.008617   0.001103  -0.006684  -0.008414   \n",
       "47  -0.033108  -0.013751  -0.008617   0.001103  -0.006684  -0.008414   \n",
       "\n",
       "            0          0          0  \n",
       "0   -0.007161  -0.003821   0.007020  \n",
       "1   -0.007089  -0.003456   0.007331  \n",
       "2   -0.006562  -0.003360   0.007389  \n",
       "3   -0.006382  -0.003389   0.007342  \n",
       "4   -0.003639  -0.002846   0.006686  \n",
       "5   -0.004171  -0.002818   0.006740  \n",
       "6   -0.000611  -0.002114   0.005813  \n",
       "7   -0.002044  -0.003035   0.005085  \n",
       "8    0.000861  -0.003804   0.003939  \n",
       "9    0.000634  -0.004078   0.004025  \n",
       "10   0.003376  -0.005304   0.005648  \n",
       "11   0.002891  -0.005750   0.005092  \n",
       "12   0.003448  -0.006824   0.010137  \n",
       "13   0.003065  -0.007404   0.009231  \n",
       "14   0.006235   0.001224   0.028768  \n",
       "15   4.343204   6.021968   8.961095  \n",
       "16  10.126165  15.724873  21.688187  \n",
       "17  13.893641  21.231514  28.554996  \n",
       "18  23.849176  35.980873  43.742489  \n",
       "19  25.314758  35.063873  41.340168  \n",
       "20  34.878735  37.441326  39.558636  \n",
       "21  37.875446  38.862137  39.862083  \n",
       "22  41.430561  46.661736  52.656006  \n",
       "23  37.413563  44.377457  51.620369  \n",
       "24  36.913254  41.352577  47.399540  \n",
       "25  39.869926  42.210480  45.579853  \n",
       "26  36.364105  38.417824  42.602364  \n",
       "27  33.864090  40.997898  52.365910  \n",
       "28  30.327812  32.306355  36.880199  \n",
       "29  31.293306  32.446217  35.761631  \n",
       "30  27.289167  27.582920  29.087336  \n",
       "31  22.198530  24.084312  28.108219  \n",
       "32  12.066522  13.937607  18.057730  \n",
       "33   6.130318   7.853072  14.699933  \n",
       "34   0.018877   0.013442   0.046391  \n",
       "35   0.017984   0.013056   0.044147  \n",
       "36   0.014014   0.010773   0.043402  \n",
       "37   0.013448   0.010574   0.041957  \n",
       "38   0.009430   0.006761   0.037853  \n",
       "39   0.008907   0.006582   0.036539  \n",
       "40   0.005620   0.002862   0.030534  \n",
       "41   0.005174   0.002677   0.029611  \n",
       "42   0.002628  -0.000531   0.022941  \n",
       "43   0.002138  -0.000729   0.021618  \n",
       "44  -0.000054  -0.003586   0.014632  \n",
       "45  -0.000173  -0.003674   0.014444  \n",
       "46  -0.002158  -0.006407   0.008403  \n",
       "47  -0.002158  -0.006407   0.008403  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_G7 = tf.keras.Sequential([\n",
    "    layers.GRU(units=64, return_sequences=True, input_shape=[52464, 7]),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_G7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_G7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_G7.fit(np.array(Day).reshape(52464, 1, 7), np.array(Day7).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_G7 = np.squeeze(model_G7.predict(np.array(df_test).reshape(3888, 1, 7)))\n",
    "    pred_G7 = pd.DataFrame(pred_G7)\n",
    "    result_G7 = pd.concat([result_G7, pred_G7], axis=1)\n",
    "    \n",
    "result_G7[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 1.5323WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.5320 - val_loss: 1.6294\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4362 - val_loss: 1.6110\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4226 - val_loss: 1.6059\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4122 - val_loss: 1.5978\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4171 - val_loss: 1.5912\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4031 - val_loss: 1.6162\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 1.4048 - val_loss: 1.5945\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4187 - val_loss: 1.5801\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4007 - val_loss: 1.5778\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4225 - val_loss: 1.5829\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3869 - val_loss: 1.5781\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3837 - val_loss: 1.5774\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3924 - val_loss: 1.6070\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3814 - val_loss: 1.5815\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4034 - val_loss: 1.5798\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3988 - val_loss: 1.5769\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.3928 - val_loss: 1.5725\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3896 - val_loss: 1.5781\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3916 - val_loss: 1.5874\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3951 - val_loss: 1.5745\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4002 - val_loss: 1.6047\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3735 - val_loss: 1.5740\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "807/820 [============================>.] - ETA: 0s - loss: 2.2617WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.2616 - val_loss: 2.5735\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2287 - val_loss: 2.5548\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2189 - val_loss: 2.5603\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1986 - val_loss: 2.5514\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2239 - val_loss: 2.5470\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2044 - val_loss: 2.5946\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2168 - val_loss: 2.5419\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2330 - val_loss: 2.5715\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2031 - val_loss: 2.5437\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2414 - val_loss: 2.5377\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1689 - val_loss: 2.5460\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1678 - val_loss: 2.5378\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2012 - val_loss: 2.5532\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2050 - val_loss: 2.5463\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2063 - val_loss: 2.5451\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "805/820 [============================>.] - ETA: 0s - loss: 2.6558WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6557 - val_loss: 3.0060\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6197 - val_loss: 2.9977\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6152 - val_loss: 3.0094\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5910 - val_loss: 3.0016\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6291 - val_loss: 2.9862\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6140 - val_loss: 3.0109\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6230 - val_loss: 2.9931\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6550 - val_loss: 3.0046\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6011 - val_loss: 2.9869\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6617 - val_loss: 2.9871\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "811/820 [============================>.] - ETA: 0s - loss: 2.7658WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.7659 - val_loss: 3.1133\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7318 - val_loss: 3.1047\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7237 - val_loss: 3.1166\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7078 - val_loss: 3.1216\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7479 - val_loss: 3.1230\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7420 - val_loss: 3.1209\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7587 - val_loss: 3.1191\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "801/820 [============================>.] - ETA: 0s - loss: 2.6766WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6766 - val_loss: 3.0194\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6368 - val_loss: 2.9678\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6263 - val_loss: 2.9853\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6269 - val_loss: 2.9968\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6515 - val_loss: 3.0206\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6584 - val_loss: 3.0196\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6651 - val_loss: 2.9678\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "805/820 [============================>.] - ETA: 0s - loss: 2.4087WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.4088 - val_loss: 2.7198\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3691 - val_loss: 2.6691\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3604 - val_loss: 2.6811\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3609 - val_loss: 2.7591\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3930 - val_loss: 2.6963\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3916 - val_loss: 2.6890\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3908 - val_loss: 2.6440\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4256 - val_loss: 2.6658\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3577 - val_loss: 2.6779\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4119 - val_loss: 2.6555\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3264 - val_loss: 2.6397\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 2.3352 - val_loss: 2.6511\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3638 - val_loss: 2.6992\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 2.3426 - val_loss: 2.6560\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3527 - val_loss: 2.6483\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3308 - val_loss: 2.6311\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3661 - val_loss: 2.6181\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3823 - val_loss: 2.6355\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3513 - val_loss: 2.6246\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3711 - val_loss: 2.6108\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3940 - val_loss: 2.6117\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3386 - val_loss: 2.6278\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3414 - val_loss: 2.6338\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3466 - val_loss: 2.5936\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3069 - val_loss: 2.6004\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2998 - val_loss: 2.5955\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3436 - val_loss: 2.5861\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3031 - val_loss: 2.5855\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2995 - val_loss: 2.6213\n",
      "Epoch 30/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2986 - val_loss: 2.5685\n",
      "Epoch 31/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2980 - val_loss: 2.6112\n",
      "Epoch 32/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3062 - val_loss: 2.6133\n",
      "Epoch 33/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2775 - val_loss: 2.5929\n",
      "Epoch 34/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2807 - val_loss: 2.5953\n",
      "Epoch 35/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2659 - val_loss: 2.6010\n",
      "Epoch 00035: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "807/820 [============================>.] - ETA: 0s - loss: 1.8832WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.8833 - val_loss: 2.1522\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8533 - val_loss: 2.1145\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8673 - val_loss: 2.1879\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8597 - val_loss: 2.1706\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8619 - val_loss: 2.1573\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8757 - val_loss: 2.1633\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8799 - val_loss: 2.1020\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9038 - val_loss: 2.1164\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8538 - val_loss: 2.1391\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8875 - val_loss: 2.1064\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8183 - val_loss: 2.1403\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8337 - val_loss: 2.1473\n",
      "Epoch 00012: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "810/820 [============================>.] - ETA: 0s - loss: 1.3590WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.3591 - val_loss: 1.5425\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3339 - val_loss: 1.5580\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3526 - val_loss: 1.5573\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3463 - val_loss: 1.5738\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3427 - val_loss: 1.5936\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3624 - val_loss: 1.5412\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3662 - val_loss: 1.5612\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3799 - val_loss: 1.5581\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3399 - val_loss: 1.5552\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3662 - val_loss: 1.5279\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3126 - val_loss: 1.5649\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3328 - val_loss: 1.5719\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3463 - val_loss: 1.5782\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3394 - val_loss: 1.5584\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3344 - val_loss: 1.5436\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "807/820 [============================>.] - ETA: 0s - loss: 0.7500WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 0.7501 - val_loss: 0.8465\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7371 - val_loss: 0.8526\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7454 - val_loss: 0.8433\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7433 - val_loss: 0.8615\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7430 - val_loss: 0.8764\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7473 - val_loss: 0.8654\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7595 - val_loss: 0.8572\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7619 - val_loss: 0.8640\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_4_input'), name='gru_4_input', description=\"created by layer 'gru_4_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.006080</td>\n",
       "      <td>-0.017239</td>\n",
       "      <td>-0.012890</td>\n",
       "      <td>0.012042</td>\n",
       "      <td>0.016549</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>-0.009659</td>\n",
       "      <td>0.013538</td>\n",
       "      <td>0.007649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.005739</td>\n",
       "      <td>-0.016900</td>\n",
       "      <td>-0.012668</td>\n",
       "      <td>0.012232</td>\n",
       "      <td>0.016546</td>\n",
       "      <td>0.025031</td>\n",
       "      <td>-0.010068</td>\n",
       "      <td>0.013283</td>\n",
       "      <td>0.007372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.004688</td>\n",
       "      <td>-0.016395</td>\n",
       "      <td>-0.014915</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.013092</td>\n",
       "      <td>0.022475</td>\n",
       "      <td>-0.007171</td>\n",
       "      <td>0.012271</td>\n",
       "      <td>0.005666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005095</td>\n",
       "      <td>-0.016614</td>\n",
       "      <td>-0.015043</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.022952</td>\n",
       "      <td>-0.006833</td>\n",
       "      <td>0.012449</td>\n",
       "      <td>0.005790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004355</td>\n",
       "      <td>-0.016381</td>\n",
       "      <td>-0.017238</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.010225</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>-0.005015</td>\n",
       "      <td>0.010809</td>\n",
       "      <td>0.004737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.003740</td>\n",
       "      <td>-0.015905</td>\n",
       "      <td>-0.016838</td>\n",
       "      <td>0.010371</td>\n",
       "      <td>0.010299</td>\n",
       "      <td>0.017918</td>\n",
       "      <td>-0.005138</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>0.004659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.002873</td>\n",
       "      <td>-0.015461</td>\n",
       "      <td>-0.018507</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>0.008127</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>-0.002963</td>\n",
       "      <td>0.008401</td>\n",
       "      <td>0.004653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.001166</td>\n",
       "      <td>-0.015176</td>\n",
       "      <td>-0.018372</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.008588</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>-0.002961</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>0.005117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000892</td>\n",
       "      <td>-0.015099</td>\n",
       "      <td>-0.020445</td>\n",
       "      <td>0.013104</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>0.002852</td>\n",
       "      <td>0.006293</td>\n",
       "      <td>0.007539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001886</td>\n",
       "      <td>-0.014946</td>\n",
       "      <td>-0.020554</td>\n",
       "      <td>0.013891</td>\n",
       "      <td>0.007540</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>0.008529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.004019</td>\n",
       "      <td>-0.014732</td>\n",
       "      <td>-0.022984</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.006690</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.017279</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0.014435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003860</td>\n",
       "      <td>-0.014311</td>\n",
       "      <td>-0.022140</td>\n",
       "      <td>0.014892</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.018168</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.014221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.005608</td>\n",
       "      <td>-0.014416</td>\n",
       "      <td>-0.025160</td>\n",
       "      <td>0.015331</td>\n",
       "      <td>0.006418</td>\n",
       "      <td>-0.000522</td>\n",
       "      <td>0.039270</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.032387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.005466</td>\n",
       "      <td>-0.014016</td>\n",
       "      <td>-0.024380</td>\n",
       "      <td>0.015468</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.040754</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>0.033276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008072</td>\n",
       "      <td>-0.014381</td>\n",
       "      <td>-0.028686</td>\n",
       "      <td>0.015456</td>\n",
       "      <td>0.005689</td>\n",
       "      <td>0.002852</td>\n",
       "      <td>0.068024</td>\n",
       "      <td>0.024294</td>\n",
       "      <td>0.099238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.371304</td>\n",
       "      <td>0.505326</td>\n",
       "      <td>0.604331</td>\n",
       "      <td>0.997621</td>\n",
       "      <td>1.473068</td>\n",
       "      <td>2.797810</td>\n",
       "      <td>3.511837</td>\n",
       "      <td>5.213232</td>\n",
       "      <td>11.893352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.883875</td>\n",
       "      <td>2.981305</td>\n",
       "      <td>3.250978</td>\n",
       "      <td>4.558801</td>\n",
       "      <td>5.326631</td>\n",
       "      <td>7.276814</td>\n",
       "      <td>9.359587</td>\n",
       "      <td>12.231107</td>\n",
       "      <td>25.218607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.816192</td>\n",
       "      <td>6.312464</td>\n",
       "      <td>6.703304</td>\n",
       "      <td>9.104792</td>\n",
       "      <td>10.860206</td>\n",
       "      <td>10.862053</td>\n",
       "      <td>13.071905</td>\n",
       "      <td>16.815727</td>\n",
       "      <td>29.591112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.060435</td>\n",
       "      <td>10.515658</td>\n",
       "      <td>11.323014</td>\n",
       "      <td>14.694141</td>\n",
       "      <td>17.303898</td>\n",
       "      <td>18.340742</td>\n",
       "      <td>22.430216</td>\n",
       "      <td>27.462133</td>\n",
       "      <td>41.306126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.712265</td>\n",
       "      <td>14.528638</td>\n",
       "      <td>15.737056</td>\n",
       "      <td>18.534460</td>\n",
       "      <td>20.627459</td>\n",
       "      <td>18.079632</td>\n",
       "      <td>23.077234</td>\n",
       "      <td>27.001503</td>\n",
       "      <td>36.786316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.041468</td>\n",
       "      <td>20.647619</td>\n",
       "      <td>23.010826</td>\n",
       "      <td>23.054489</td>\n",
       "      <td>26.202168</td>\n",
       "      <td>28.759092</td>\n",
       "      <td>33.559471</td>\n",
       "      <td>32.917210</td>\n",
       "      <td>35.568935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.106730</td>\n",
       "      <td>20.792036</td>\n",
       "      <td>24.372156</td>\n",
       "      <td>24.732061</td>\n",
       "      <td>29.482521</td>\n",
       "      <td>33.271160</td>\n",
       "      <td>37.473076</td>\n",
       "      <td>36.265404</td>\n",
       "      <td>37.465626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.054950</td>\n",
       "      <td>23.781816</td>\n",
       "      <td>27.847975</td>\n",
       "      <td>28.064074</td>\n",
       "      <td>31.773804</td>\n",
       "      <td>34.472336</td>\n",
       "      <td>41.038567</td>\n",
       "      <td>41.506248</td>\n",
       "      <td>47.101749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.969482</td>\n",
       "      <td>20.997927</td>\n",
       "      <td>23.602024</td>\n",
       "      <td>24.459507</td>\n",
       "      <td>27.190718</td>\n",
       "      <td>29.865513</td>\n",
       "      <td>36.606640</td>\n",
       "      <td>37.661205</td>\n",
       "      <td>45.864857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12.488134</td>\n",
       "      <td>20.199202</td>\n",
       "      <td>22.473011</td>\n",
       "      <td>23.069437</td>\n",
       "      <td>26.027369</td>\n",
       "      <td>31.443798</td>\n",
       "      <td>37.160194</td>\n",
       "      <td>37.539013</td>\n",
       "      <td>43.873386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>14.086313</td>\n",
       "      <td>21.654930</td>\n",
       "      <td>24.873434</td>\n",
       "      <td>25.032885</td>\n",
       "      <td>29.272343</td>\n",
       "      <td>34.704235</td>\n",
       "      <td>39.922306</td>\n",
       "      <td>39.551712</td>\n",
       "      <td>42.702572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12.205894</td>\n",
       "      <td>19.577000</td>\n",
       "      <td>21.639002</td>\n",
       "      <td>22.029419</td>\n",
       "      <td>25.507477</td>\n",
       "      <td>32.268421</td>\n",
       "      <td>36.955379</td>\n",
       "      <td>36.939949</td>\n",
       "      <td>41.722382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.500475</td>\n",
       "      <td>15.460502</td>\n",
       "      <td>16.699511</td>\n",
       "      <td>19.108202</td>\n",
       "      <td>21.643040</td>\n",
       "      <td>31.688126</td>\n",
       "      <td>35.169258</td>\n",
       "      <td>37.199581</td>\n",
       "      <td>48.921932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9.441790</td>\n",
       "      <td>16.069981</td>\n",
       "      <td>16.964970</td>\n",
       "      <td>17.977592</td>\n",
       "      <td>20.557564</td>\n",
       "      <td>28.203987</td>\n",
       "      <td>30.999447</td>\n",
       "      <td>31.153376</td>\n",
       "      <td>37.491634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.072161</td>\n",
       "      <td>16.522398</td>\n",
       "      <td>17.517841</td>\n",
       "      <td>18.225594</td>\n",
       "      <td>21.304077</td>\n",
       "      <td>28.530560</td>\n",
       "      <td>31.539753</td>\n",
       "      <td>31.376888</td>\n",
       "      <td>36.406391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.855873</td>\n",
       "      <td>14.396804</td>\n",
       "      <td>14.794154</td>\n",
       "      <td>15.596222</td>\n",
       "      <td>18.758314</td>\n",
       "      <td>24.925007</td>\n",
       "      <td>26.658340</td>\n",
       "      <td>25.715256</td>\n",
       "      <td>29.274141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.826164</td>\n",
       "      <td>10.824870</td>\n",
       "      <td>11.084470</td>\n",
       "      <td>12.949096</td>\n",
       "      <td>14.855688</td>\n",
       "      <td>21.604095</td>\n",
       "      <td>22.242559</td>\n",
       "      <td>22.263960</td>\n",
       "      <td>28.678186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.543988</td>\n",
       "      <td>5.298042</td>\n",
       "      <td>5.593607</td>\n",
       "      <td>7.727602</td>\n",
       "      <td>9.351751</td>\n",
       "      <td>13.261674</td>\n",
       "      <td>13.093681</td>\n",
       "      <td>14.261778</td>\n",
       "      <td>21.094549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.513111</td>\n",
       "      <td>1.039399</td>\n",
       "      <td>1.240228</td>\n",
       "      <td>2.191832</td>\n",
       "      <td>3.248688</td>\n",
       "      <td>8.261014</td>\n",
       "      <td>7.484256</td>\n",
       "      <td>9.650099</td>\n",
       "      <td>18.829851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.011560</td>\n",
       "      <td>-0.014120</td>\n",
       "      <td>-0.036626</td>\n",
       "      <td>0.009344</td>\n",
       "      <td>0.009717</td>\n",
       "      <td>0.104296</td>\n",
       "      <td>0.006978</td>\n",
       "      <td>0.151328</td>\n",
       "      <td>0.035817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.010033</td>\n",
       "      <td>-0.014299</td>\n",
       "      <td>-0.035797</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.011315</td>\n",
       "      <td>0.096342</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.145412</td>\n",
       "      <td>0.035208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.008830</td>\n",
       "      <td>-0.015825</td>\n",
       "      <td>-0.036569</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.082209</td>\n",
       "      <td>-0.002911</td>\n",
       "      <td>0.155028</td>\n",
       "      <td>0.008165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.007804</td>\n",
       "      <td>-0.015728</td>\n",
       "      <td>-0.035823</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.078317</td>\n",
       "      <td>-0.006883</td>\n",
       "      <td>0.149804</td>\n",
       "      <td>0.008151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.006786</td>\n",
       "      <td>-0.017576</td>\n",
       "      <td>-0.036914</td>\n",
       "      <td>0.007823</td>\n",
       "      <td>0.011223</td>\n",
       "      <td>0.068425</td>\n",
       "      <td>-0.018042</td>\n",
       "      <td>0.146671</td>\n",
       "      <td>-0.000652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.005809</td>\n",
       "      <td>-0.017324</td>\n",
       "      <td>-0.036078</td>\n",
       "      <td>0.009114</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>0.066149</td>\n",
       "      <td>-0.020851</td>\n",
       "      <td>0.141343</td>\n",
       "      <td>-0.001829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.004835</td>\n",
       "      <td>-0.019337</td>\n",
       "      <td>-0.037240</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.011041</td>\n",
       "      <td>0.059898</td>\n",
       "      <td>-0.034961</td>\n",
       "      <td>0.130993</td>\n",
       "      <td>-0.008499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.004159</td>\n",
       "      <td>-0.018710</td>\n",
       "      <td>-0.036322</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.012450</td>\n",
       "      <td>0.059342</td>\n",
       "      <td>-0.036129</td>\n",
       "      <td>0.126472</td>\n",
       "      <td>-0.010746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.003282</td>\n",
       "      <td>-0.021023</td>\n",
       "      <td>-0.037647</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.010737</td>\n",
       "      <td>0.056465</td>\n",
       "      <td>-0.047130</td>\n",
       "      <td>0.117813</td>\n",
       "      <td>-0.018126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.002284</td>\n",
       "      <td>-0.020141</td>\n",
       "      <td>-0.036335</td>\n",
       "      <td>0.007486</td>\n",
       "      <td>0.012430</td>\n",
       "      <td>0.056408</td>\n",
       "      <td>-0.046046</td>\n",
       "      <td>0.113967</td>\n",
       "      <td>-0.020939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.001239</td>\n",
       "      <td>-0.022093</td>\n",
       "      <td>-0.037187</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>0.056663</td>\n",
       "      <td>-0.049355</td>\n",
       "      <td>0.111827</td>\n",
       "      <td>-0.025254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.001151</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.036932</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.011865</td>\n",
       "      <td>0.056683</td>\n",
       "      <td>-0.049222</td>\n",
       "      <td>0.111121</td>\n",
       "      <td>-0.025642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.024058</td>\n",
       "      <td>-0.038304</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.057983</td>\n",
       "      <td>-0.049399</td>\n",
       "      <td>0.115991</td>\n",
       "      <td>-0.022553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.024058</td>\n",
       "      <td>-0.038304</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.057983</td>\n",
       "      <td>-0.049399</td>\n",
       "      <td>0.115991</td>\n",
       "      <td>-0.022553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.006080  -0.017239  -0.012890   0.012042   0.016549   0.025605   \n",
       "1   -0.005739  -0.016900  -0.012668   0.012232   0.016546   0.025031   \n",
       "2   -0.004688  -0.016395  -0.014915   0.011110   0.013092   0.022475   \n",
       "3   -0.005095  -0.016614  -0.015043   0.010654   0.013000   0.022952   \n",
       "4   -0.004355  -0.016381  -0.017238   0.009901   0.010225   0.018400   \n",
       "5   -0.003740  -0.015905  -0.016838   0.010371   0.010299   0.017918   \n",
       "6   -0.002873  -0.015461  -0.018507   0.010007   0.008127   0.012516   \n",
       "7   -0.001166  -0.015176  -0.018372   0.011900   0.008588   0.011227   \n",
       "8    0.000892  -0.015099  -0.020445   0.013104   0.007344   0.005237   \n",
       "9    0.001886  -0.014946  -0.020554   0.013891   0.007540   0.004938   \n",
       "10   0.004019  -0.014732  -0.022984   0.014851   0.006690   0.000166   \n",
       "11   0.003860  -0.014311  -0.022140   0.014892   0.007006   0.000938   \n",
       "12   0.005608  -0.014416  -0.025160   0.015331   0.006418  -0.000522   \n",
       "13   0.005466  -0.014016  -0.024380   0.015468   0.006840   0.001052   \n",
       "14   0.008072  -0.014381  -0.028686   0.015456   0.005689   0.002852   \n",
       "15   0.371304   0.505326   0.604331   0.997621   1.473068   2.797810   \n",
       "16   1.883875   2.981305   3.250978   4.558801   5.326631   7.276814   \n",
       "17   3.816192   6.312464   6.703304   9.104792  10.860206  10.862053   \n",
       "18   6.060435  10.515658  11.323014  14.694141  17.303898  18.340742   \n",
       "19   8.712265  14.528638  15.737056  18.534460  20.627459  18.079632   \n",
       "20  14.041468  20.647619  23.010826  23.054489  26.202168  28.759092   \n",
       "21  15.106730  20.792036  24.372156  24.732061  29.482521  33.271160   \n",
       "22  15.054950  23.781816  27.847975  28.064074  31.773804  34.472336   \n",
       "23  12.969482  20.997927  23.602024  24.459507  27.190718  29.865513   \n",
       "24  12.488134  20.199202  22.473011  23.069437  26.027369  31.443798   \n",
       "25  14.086313  21.654930  24.873434  25.032885  29.272343  34.704235   \n",
       "26  12.205894  19.577000  21.639002  22.029419  25.507477  32.268421   \n",
       "27   8.500475  15.460502  16.699511  19.108202  21.643040  31.688126   \n",
       "28   9.441790  16.069981  16.964970  17.977592  20.557564  28.203987   \n",
       "29  10.072161  16.522398  17.517841  18.225594  21.304077  28.530560   \n",
       "30   8.855873  14.396804  14.794154  15.596222  18.758314  24.925007   \n",
       "31   5.826164  10.824870  11.084470  12.949096  14.855688  21.604095   \n",
       "32   2.543988   5.298042   5.593607   7.727602   9.351751  13.261674   \n",
       "33   0.513111   1.039399   1.240228   2.191832   3.248688   8.261014   \n",
       "34   0.011560  -0.014120  -0.036626   0.009344   0.009717   0.104296   \n",
       "35   0.010033  -0.014299  -0.035797   0.010597   0.011315   0.096342   \n",
       "36   0.008830  -0.015825  -0.036569   0.008959   0.011113   0.082209   \n",
       "37   0.007804  -0.015728  -0.035823   0.010084   0.012275   0.078317   \n",
       "38   0.006786  -0.017576  -0.036914   0.007823   0.011223   0.068425   \n",
       "39   0.005809  -0.017324  -0.036078   0.009114   0.012365   0.066149   \n",
       "40   0.004835  -0.019337  -0.037240   0.006612   0.011041   0.059898   \n",
       "41   0.004159  -0.018710  -0.036322   0.008200   0.012450   0.059342   \n",
       "42   0.003282  -0.021023  -0.037647   0.005227   0.010737   0.056465   \n",
       "43   0.002284  -0.020141  -0.036335   0.007486   0.012430   0.056408   \n",
       "44   0.001239  -0.022093  -0.037187   0.005332   0.011423   0.056663   \n",
       "45   0.001151  -0.021828  -0.036932   0.005841   0.011865   0.056683   \n",
       "46   0.000604  -0.024058  -0.038304   0.002716   0.010258   0.057983   \n",
       "47   0.000604  -0.024058  -0.038304   0.002716   0.010258   0.057983   \n",
       "\n",
       "            0          0          0  \n",
       "0   -0.009659   0.013538   0.007649  \n",
       "1   -0.010068   0.013283   0.007372  \n",
       "2   -0.007171   0.012271   0.005666  \n",
       "3   -0.006833   0.012449   0.005790  \n",
       "4   -0.005015   0.010809   0.004737  \n",
       "5   -0.005138   0.010601   0.004659  \n",
       "6   -0.002963   0.008401   0.004653  \n",
       "7   -0.002961   0.008092   0.005117  \n",
       "8    0.002852   0.006293   0.007539  \n",
       "9    0.003819   0.006465   0.008529  \n",
       "10   0.017279   0.006455   0.014435  \n",
       "11   0.018168   0.006400   0.014221  \n",
       "12   0.039270   0.009497   0.032387  \n",
       "13   0.040754   0.010006   0.033276  \n",
       "14   0.068024   0.024294   0.099238  \n",
       "15   3.511837   5.213232  11.893352  \n",
       "16   9.359587  12.231107  25.218607  \n",
       "17  13.071905  16.815727  29.591112  \n",
       "18  22.430216  27.462133  41.306126  \n",
       "19  23.077234  27.001503  36.786316  \n",
       "20  33.559471  32.917210  35.568935  \n",
       "21  37.473076  36.265404  37.465626  \n",
       "22  41.038567  41.506248  47.101749  \n",
       "23  36.606640  37.661205  45.864857  \n",
       "24  37.160194  37.539013  43.873386  \n",
       "25  39.922306  39.551712  42.702572  \n",
       "26  36.955379  36.939949  41.722382  \n",
       "27  35.169258  37.199581  48.921932  \n",
       "28  30.999447  31.153376  37.491634  \n",
       "29  31.539753  31.376888  36.406391  \n",
       "30  26.658340  25.715256  29.274141  \n",
       "31  22.242559  22.263960  28.678186  \n",
       "32  13.093681  14.261778  21.094549  \n",
       "33   7.484256   9.650099  18.829851  \n",
       "34   0.006978   0.151328   0.035817  \n",
       "35   0.000960   0.145412   0.035208  \n",
       "36  -0.002911   0.155028   0.008165  \n",
       "37  -0.006883   0.149804   0.008151  \n",
       "38  -0.018042   0.146671  -0.000652  \n",
       "39  -0.020851   0.141343  -0.001829  \n",
       "40  -0.034961   0.130993  -0.008499  \n",
       "41  -0.036129   0.126472  -0.010746  \n",
       "42  -0.047130   0.117813  -0.018126  \n",
       "43  -0.046046   0.113967  -0.020939  \n",
       "44  -0.049355   0.111827  -0.025254  \n",
       "45  -0.049222   0.111121  -0.025642  \n",
       "46  -0.049399   0.115991  -0.022553  \n",
       "47  -0.049399   0.115991  -0.022553  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_G8 = tf.keras.Sequential([\n",
    "    layers.GRU(units=64, return_sequences=True, input_shape=[52464, 7]),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_G8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_G8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_G8.fit(np.array(Day).reshape(52464, 1, 7), np.array(Day7).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_G8 = np.squeeze(model_G8.predict(np.array(df_test).reshape(3888, 1, 7)))\n",
    "    pred_G8 = pd.DataFrame(pred_G8)\n",
    "    result_G8 = pd.concat([result_G8, pred_G8], axis=1)\n",
    "    \n",
    "result_G8[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "810/820 [============================>.] - ETA: 0s - loss: 1.5503WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.5495 - val_loss: 1.6348\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4419 - val_loss: 1.6142\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4231 - val_loss: 1.6086\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4086 - val_loss: 1.6205\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4126 - val_loss: 1.5875\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3956 - val_loss: 1.6490\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3958 - val_loss: 1.6245\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4103 - val_loss: 1.6177\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "806/820 [============================>.] - ETA: 0s - loss: 2.3263WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.3262 - val_loss: 2.6022\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2741 - val_loss: 2.5807\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2472 - val_loss: 2.5840\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2228 - val_loss: 2.5703\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2392 - val_loss: 2.5576\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2146 - val_loss: 2.6591\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2257 - val_loss: 2.5761\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2388 - val_loss: 2.6316\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "812/820 [============================>.] - ETA: 0s - loss: 2.6957WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6957 - val_loss: 3.0558\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6546 - val_loss: 3.0356\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6426 - val_loss: 3.0211\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6123 - val_loss: 3.0089\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6500 - val_loss: 3.0113\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6215 - val_loss: 3.1048\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6308 - val_loss: 3.0664\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "817/820 [============================>.] - ETA: 0s - loss: 2.7982WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.7982 - val_loss: 3.1833\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7536 - val_loss: 3.1290\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7386 - val_loss: 3.1365\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7242 - val_loss: 3.1033\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7579 - val_loss: 3.1148\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7492 - val_loss: 3.2007\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7632 - val_loss: 3.1094\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 2.6928WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6928 - val_loss: 2.9882\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6507 - val_loss: 2.9735\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6382 - val_loss: 2.9780\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6358 - val_loss: 2.9977\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 2.6608 - val_loss: 2.9978\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "806/820 [============================>.] - ETA: 0s - loss: 2.4344WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.4344 - val_loss: 2.7102\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3976 - val_loss: 2.6808\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3832 - val_loss: 2.6813\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3802 - val_loss: 2.7654\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4043 - val_loss: 2.6946\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "817/820 [============================>.] - ETA: 0s - loss: 2.0289WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.0289 - val_loss: 2.2519\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9943 - val_loss: 2.2711\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9928 - val_loss: 2.2509\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9859 - val_loss: 2.2854\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0089 - val_loss: 2.3207\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0174 - val_loss: 2.2348\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0057 - val_loss: 2.2461\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0284 - val_loss: 2.2277\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9814 - val_loss: 2.2211\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0179 - val_loss: 2.2110\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9463 - val_loss: 2.2269\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.9649 - val_loss: 2.2747\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9850 - val_loss: 2.2489\n",
      "Epoch 00013: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "818/820 [============================>.] - ETA: 0s - loss: 1.4828WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.4828 - val_loss: 1.6503\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4544 - val_loss: 1.6591\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4681 - val_loss: 1.6497\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4537 - val_loss: 1.7528\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4803 - val_loss: 1.7771\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4725 - val_loss: 1.6540\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "810/820 [============================>.] - ETA: 0s - loss: 0.8407WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 0.8407 - val_loss: 0.9433\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8193 - val_loss: 0.9857\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8342 - val_loss: 0.9345\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8244 - val_loss: 0.9773\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 0.8348 - val_loss: 0.9672\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8270 - val_loss: 0.9493\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.019125</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.007617</td>\n",
       "      <td>-0.005385</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>0.023319</td>\n",
       "      <td>0.018366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.019038</td>\n",
       "      <td>-0.018860</td>\n",
       "      <td>-0.007571</td>\n",
       "      <td>-0.005126</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.019592</td>\n",
       "      <td>0.022475</td>\n",
       "      <td>0.017256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.018445</td>\n",
       "      <td>-0.018930</td>\n",
       "      <td>-0.008777</td>\n",
       "      <td>-0.007531</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>0.015187</td>\n",
       "      <td>0.017779</td>\n",
       "      <td>0.009924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.018493</td>\n",
       "      <td>-0.019040</td>\n",
       "      <td>-0.008770</td>\n",
       "      <td>-0.007772</td>\n",
       "      <td>-0.000332</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.016030</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>0.011051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.018082</td>\n",
       "      <td>-0.019398</td>\n",
       "      <td>-0.009946</td>\n",
       "      <td>-0.010143</td>\n",
       "      <td>-0.001520</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>0.013279</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>0.007459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.017874</td>\n",
       "      <td>-0.018921</td>\n",
       "      <td>-0.009755</td>\n",
       "      <td>-0.009475</td>\n",
       "      <td>-0.001727</td>\n",
       "      <td>-0.002171</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.014926</td>\n",
       "      <td>0.005757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.017307</td>\n",
       "      <td>-0.018858</td>\n",
       "      <td>-0.010540</td>\n",
       "      <td>-0.011035</td>\n",
       "      <td>-0.002522</td>\n",
       "      <td>-0.002984</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.013476</td>\n",
       "      <td>0.003971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.017095</td>\n",
       "      <td>-0.018107</td>\n",
       "      <td>-0.009864</td>\n",
       "      <td>-0.008948</td>\n",
       "      <td>-0.001966</td>\n",
       "      <td>-0.001984</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>0.001485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.016474</td>\n",
       "      <td>-0.017599</td>\n",
       "      <td>-0.009730</td>\n",
       "      <td>-0.008396</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>0.015984</td>\n",
       "      <td>0.003385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.016529</td>\n",
       "      <td>-0.017173</td>\n",
       "      <td>-0.008936</td>\n",
       "      <td>-0.006776</td>\n",
       "      <td>-0.000407</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.012892</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>0.003213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.016024</td>\n",
       "      <td>-0.016882</td>\n",
       "      <td>-0.008807</td>\n",
       "      <td>-0.006280</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>0.018502</td>\n",
       "      <td>0.023334</td>\n",
       "      <td>0.010275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.015915</td>\n",
       "      <td>-0.016484</td>\n",
       "      <td>-0.008372</td>\n",
       "      <td>-0.005836</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>0.018033</td>\n",
       "      <td>0.023613</td>\n",
       "      <td>0.010353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.015587</td>\n",
       "      <td>-0.016601</td>\n",
       "      <td>-0.008875</td>\n",
       "      <td>-0.006145</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.024712</td>\n",
       "      <td>0.029036</td>\n",
       "      <td>0.017955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.015502</td>\n",
       "      <td>-0.016263</td>\n",
       "      <td>-0.008512</td>\n",
       "      <td>-0.005748</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.024236</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>0.018322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.015438</td>\n",
       "      <td>-0.016671</td>\n",
       "      <td>-0.009089</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>0.020066</td>\n",
       "      <td>0.033335</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.024706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.273712</td>\n",
       "      <td>0.579337</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>1.429537</td>\n",
       "      <td>2.459546</td>\n",
       "      <td>4.434146</td>\n",
       "      <td>5.853877</td>\n",
       "      <td>8.413433</td>\n",
       "      <td>13.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.359357</td>\n",
       "      <td>3.117102</td>\n",
       "      <td>4.482175</td>\n",
       "      <td>7.221376</td>\n",
       "      <td>10.993061</td>\n",
       "      <td>15.369520</td>\n",
       "      <td>17.760300</td>\n",
       "      <td>25.476889</td>\n",
       "      <td>36.946301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.629808</td>\n",
       "      <td>6.297938</td>\n",
       "      <td>8.574986</td>\n",
       "      <td>12.396621</td>\n",
       "      <td>16.806257</td>\n",
       "      <td>21.061373</td>\n",
       "      <td>24.383640</td>\n",
       "      <td>33.391483</td>\n",
       "      <td>45.629196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.027555</td>\n",
       "      <td>10.051889</td>\n",
       "      <td>12.806709</td>\n",
       "      <td>16.891878</td>\n",
       "      <td>21.427212</td>\n",
       "      <td>25.463512</td>\n",
       "      <td>29.728369</td>\n",
       "      <td>39.646595</td>\n",
       "      <td>52.515381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.634828</td>\n",
       "      <td>13.959082</td>\n",
       "      <td>16.527153</td>\n",
       "      <td>20.266357</td>\n",
       "      <td>24.226301</td>\n",
       "      <td>27.340965</td>\n",
       "      <td>30.688051</td>\n",
       "      <td>39.474838</td>\n",
       "      <td>51.019337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9.294259</td>\n",
       "      <td>21.544348</td>\n",
       "      <td>22.042635</td>\n",
       "      <td>23.890402</td>\n",
       "      <td>26.176407</td>\n",
       "      <td>27.946430</td>\n",
       "      <td>31.602259</td>\n",
       "      <td>34.815697</td>\n",
       "      <td>38.098289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.555115</td>\n",
       "      <td>23.344545</td>\n",
       "      <td>22.718353</td>\n",
       "      <td>23.974604</td>\n",
       "      <td>27.213102</td>\n",
       "      <td>29.650118</td>\n",
       "      <td>34.550789</td>\n",
       "      <td>36.576504</td>\n",
       "      <td>38.338005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.744592</td>\n",
       "      <td>23.326319</td>\n",
       "      <td>24.893036</td>\n",
       "      <td>28.041769</td>\n",
       "      <td>30.543203</td>\n",
       "      <td>33.171665</td>\n",
       "      <td>38.021835</td>\n",
       "      <td>45.816437</td>\n",
       "      <td>53.586349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.270817</td>\n",
       "      <td>20.399950</td>\n",
       "      <td>22.187107</td>\n",
       "      <td>25.267614</td>\n",
       "      <td>28.074375</td>\n",
       "      <td>30.464939</td>\n",
       "      <td>34.652565</td>\n",
       "      <td>42.893795</td>\n",
       "      <td>52.498535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.091259</td>\n",
       "      <td>20.220682</td>\n",
       "      <td>21.496767</td>\n",
       "      <td>24.042891</td>\n",
       "      <td>26.663202</td>\n",
       "      <td>28.833647</td>\n",
       "      <td>32.890366</td>\n",
       "      <td>39.819202</td>\n",
       "      <td>47.987965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.405066</td>\n",
       "      <td>22.633169</td>\n",
       "      <td>23.153381</td>\n",
       "      <td>25.253382</td>\n",
       "      <td>27.913637</td>\n",
       "      <td>30.344498</td>\n",
       "      <td>35.113995</td>\n",
       "      <td>40.123653</td>\n",
       "      <td>45.054546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.068606</td>\n",
       "      <td>20.320787</td>\n",
       "      <td>21.009787</td>\n",
       "      <td>23.008614</td>\n",
       "      <td>25.579903</td>\n",
       "      <td>27.675587</td>\n",
       "      <td>31.922415</td>\n",
       "      <td>37.531811</td>\n",
       "      <td>43.983574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.621737</td>\n",
       "      <td>14.933422</td>\n",
       "      <td>16.959221</td>\n",
       "      <td>20.051760</td>\n",
       "      <td>23.654943</td>\n",
       "      <td>26.390972</td>\n",
       "      <td>30.416721</td>\n",
       "      <td>39.653301</td>\n",
       "      <td>52.045959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.378386</td>\n",
       "      <td>16.757147</td>\n",
       "      <td>17.597227</td>\n",
       "      <td>19.462786</td>\n",
       "      <td>22.070230</td>\n",
       "      <td>23.784245</td>\n",
       "      <td>27.088703</td>\n",
       "      <td>32.239811</td>\n",
       "      <td>39.116005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.854388</td>\n",
       "      <td>17.689793</td>\n",
       "      <td>18.050587</td>\n",
       "      <td>19.585070</td>\n",
       "      <td>22.130251</td>\n",
       "      <td>23.855522</td>\n",
       "      <td>27.569181</td>\n",
       "      <td>31.903282</td>\n",
       "      <td>37.366753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6.258477</td>\n",
       "      <td>16.409973</td>\n",
       "      <td>16.251860</td>\n",
       "      <td>17.124327</td>\n",
       "      <td>19.470133</td>\n",
       "      <td>20.855913</td>\n",
       "      <td>24.356153</td>\n",
       "      <td>26.837904</td>\n",
       "      <td>29.940657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.302255</td>\n",
       "      <td>11.597221</td>\n",
       "      <td>12.626295</td>\n",
       "      <td>14.420085</td>\n",
       "      <td>17.028147</td>\n",
       "      <td>18.435785</td>\n",
       "      <td>20.569262</td>\n",
       "      <td>24.454823</td>\n",
       "      <td>30.254778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.116210</td>\n",
       "      <td>5.766467</td>\n",
       "      <td>7.318921</td>\n",
       "      <td>9.462689</td>\n",
       "      <td>12.242281</td>\n",
       "      <td>13.908647</td>\n",
       "      <td>14.590513</td>\n",
       "      <td>18.296593</td>\n",
       "      <td>24.047079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.430942</td>\n",
       "      <td>1.111121</td>\n",
       "      <td>1.766121</td>\n",
       "      <td>3.076553</td>\n",
       "      <td>5.438250</td>\n",
       "      <td>9.089475</td>\n",
       "      <td>10.973436</td>\n",
       "      <td>15.452991</td>\n",
       "      <td>23.168827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.008247</td>\n",
       "      <td>-0.008145</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>0.033578</td>\n",
       "      <td>0.042250</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.032953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.008333</td>\n",
       "      <td>-0.008705</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.000724</td>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.031599</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.016081</td>\n",
       "      <td>0.030591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.007985</td>\n",
       "      <td>-0.008566</td>\n",
       "      <td>-0.005564</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.029768</td>\n",
       "      <td>0.033509</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.019742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.008071</td>\n",
       "      <td>-0.009059</td>\n",
       "      <td>-0.005876</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>0.008577</td>\n",
       "      <td>0.028346</td>\n",
       "      <td>0.031892</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.018378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.007806</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.006104</td>\n",
       "      <td>-0.001103</td>\n",
       "      <td>0.007922</td>\n",
       "      <td>0.026834</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>-0.006554</td>\n",
       "      <td>0.008845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.007920</td>\n",
       "      <td>-0.009575</td>\n",
       "      <td>-0.006457</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>0.007509</td>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>-0.007024</td>\n",
       "      <td>0.007915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.007726</td>\n",
       "      <td>-0.009629</td>\n",
       "      <td>-0.006779</td>\n",
       "      <td>-0.001673</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.023953</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>-0.016293</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.007800</td>\n",
       "      <td>-0.010120</td>\n",
       "      <td>-0.007105</td>\n",
       "      <td>-0.001845</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.019252</td>\n",
       "      <td>-0.015660</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.007657</td>\n",
       "      <td>-0.010208</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.021487</td>\n",
       "      <td>0.014983</td>\n",
       "      <td>-0.023261</td>\n",
       "      <td>-0.002419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.007826</td>\n",
       "      <td>-0.011047</td>\n",
       "      <td>-0.007969</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>0.014441</td>\n",
       "      <td>-0.022174</td>\n",
       "      <td>-0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.007742</td>\n",
       "      <td>-0.011445</td>\n",
       "      <td>-0.008594</td>\n",
       "      <td>-0.003150</td>\n",
       "      <td>0.004102</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>0.011442</td>\n",
       "      <td>-0.026332</td>\n",
       "      <td>-0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.007746</td>\n",
       "      <td>-0.011532</td>\n",
       "      <td>-0.008651</td>\n",
       "      <td>-0.003109</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.018503</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>-0.025918</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.007516</td>\n",
       "      <td>-0.011504</td>\n",
       "      <td>-0.009133</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>-0.028337</td>\n",
       "      <td>0.003379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.007516</td>\n",
       "      <td>-0.011504</td>\n",
       "      <td>-0.009133</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>-0.028337</td>\n",
       "      <td>0.003379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.019125  -0.019102  -0.007617  -0.005385   0.001388   0.002855   \n",
       "1   -0.019038  -0.018860  -0.007571  -0.005126   0.001064   0.002280   \n",
       "2   -0.018445  -0.018930  -0.008777  -0.007531  -0.000556  -0.000474   \n",
       "3   -0.018493  -0.019040  -0.008770  -0.007772  -0.000332  -0.000109   \n",
       "4   -0.018082  -0.019398  -0.009946  -0.010143  -0.001520  -0.001800   \n",
       "5   -0.017874  -0.018921  -0.009755  -0.009475  -0.001727  -0.002171   \n",
       "6   -0.017307  -0.018858  -0.010540  -0.011035  -0.002522  -0.002984   \n",
       "7   -0.017095  -0.018107  -0.009864  -0.008948  -0.001966  -0.001984   \n",
       "8   -0.016474  -0.017599  -0.009730  -0.008396  -0.000995   0.000788   \n",
       "9   -0.016529  -0.017173  -0.008936  -0.006776  -0.000407   0.001950   \n",
       "10  -0.016024  -0.016882  -0.008807  -0.006280   0.001219   0.006616   \n",
       "11  -0.015915  -0.016484  -0.008372  -0.005836   0.000777   0.005643   \n",
       "12  -0.015587  -0.016601  -0.008875  -0.006145   0.003093   0.011568   \n",
       "13  -0.015502  -0.016263  -0.008512  -0.005748   0.002602   0.010554   \n",
       "14  -0.015438  -0.016671  -0.009089  -0.005811   0.006752   0.020066   \n",
       "15   0.273712   0.579337   0.847134   1.429537   2.459546   4.434146   \n",
       "16   1.359357   3.117102   4.482175   7.221376  10.993061  15.369520   \n",
       "17   2.629808   6.297938   8.574986  12.396621  16.806257  21.061373   \n",
       "18   4.027555  10.051889  12.806709  16.891878  21.427212  25.463512   \n",
       "19   5.634828  13.959082  16.527153  20.266357  24.226301  27.340965   \n",
       "20   9.294259  21.544348  22.042635  23.890402  26.176407  27.946430   \n",
       "21  10.555115  23.344545  22.718353  23.974604  27.213102  29.650118   \n",
       "22   9.744592  23.326319  24.893036  28.041769  30.543203  33.171665   \n",
       "23   8.270817  20.399950  22.187107  25.267614  28.074375  30.464939   \n",
       "24   8.091259  20.220682  21.496767  24.042891  26.663202  28.833647   \n",
       "25   9.405066  22.633169  23.153381  25.253382  27.913637  30.344498   \n",
       "26   8.068606  20.320787  21.009787  23.008614  25.579903  27.675587   \n",
       "27   5.621737  14.933422  16.959221  20.051760  23.654943  26.390972   \n",
       "28   6.378386  16.757147  17.597227  19.462786  22.070230  23.784245   \n",
       "29   6.854388  17.689793  18.050587  19.585070  22.130251  23.855522   \n",
       "30   6.258477  16.409973  16.251860  17.124327  19.470133  20.855913   \n",
       "31   4.302255  11.597221  12.626295  14.420085  17.028147  18.435785   \n",
       "32   2.116210   5.766467   7.318921   9.462689  12.242281  13.908647   \n",
       "33   0.430942   1.111121   1.766121   3.076553   5.438250   9.089475   \n",
       "34  -0.008247  -0.008145  -0.005010  -0.000351   0.010345   0.033578   \n",
       "35  -0.008333  -0.008705  -0.005363  -0.000724   0.009779   0.031599   \n",
       "36  -0.007985  -0.008566  -0.005564  -0.000713   0.009021   0.029768   \n",
       "37  -0.008071  -0.009059  -0.005876  -0.001015   0.008577   0.028346   \n",
       "38  -0.007806  -0.008996  -0.006104  -0.001103   0.007922   0.026834   \n",
       "39  -0.007920  -0.009575  -0.006457  -0.001442   0.007509   0.025504   \n",
       "40  -0.007726  -0.009629  -0.006779  -0.001673   0.006751   0.023953   \n",
       "41  -0.007800  -0.010120  -0.007105  -0.001845   0.006411   0.023038   \n",
       "42  -0.007657  -0.010208  -0.007479  -0.002228   0.005563   0.021487   \n",
       "43  -0.007826  -0.011047  -0.007969  -0.002591   0.005218   0.020327   \n",
       "44  -0.007742  -0.011445  -0.008594  -0.003150   0.004102   0.018510   \n",
       "45  -0.007746  -0.011532  -0.008651  -0.003109   0.004112   0.018503   \n",
       "46  -0.007516  -0.011504  -0.009133  -0.003429   0.002778   0.017084   \n",
       "47  -0.007516  -0.011504  -0.009133  -0.003429   0.002778   0.017084   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.020482   0.023319   0.018366  \n",
       "1    0.019592   0.022475   0.017256  \n",
       "2    0.015187   0.017779   0.009924  \n",
       "3    0.016030   0.018532   0.011051  \n",
       "4    0.013279   0.015594   0.007459  \n",
       "5    0.012406   0.014926   0.005757  \n",
       "6    0.010963   0.013476   0.003971  \n",
       "7    0.010204   0.013302   0.001485  \n",
       "8    0.012231   0.015984   0.003385  \n",
       "9    0.012892   0.017088   0.003213  \n",
       "10   0.018502   0.023334   0.010275  \n",
       "11   0.018033   0.023613   0.010353  \n",
       "12   0.024712   0.029036   0.017955  \n",
       "13   0.024236   0.029514   0.018322  \n",
       "14   0.033335   0.033908   0.024706  \n",
       "15   5.853877   8.413433  13.160100  \n",
       "16  17.760300  25.476889  36.946301  \n",
       "17  24.383640  33.391483  45.629196  \n",
       "18  29.728369  39.646595  52.515381  \n",
       "19  30.688051  39.474838  51.019337  \n",
       "20  31.602259  34.815697  38.098289  \n",
       "21  34.550789  36.576504  38.338005  \n",
       "22  38.021835  45.816437  53.586349  \n",
       "23  34.652565  42.893795  52.498535  \n",
       "24  32.890366  39.819202  47.987965  \n",
       "25  35.113995  40.123653  45.054546  \n",
       "26  31.922415  37.531811  43.983574  \n",
       "27  30.416721  39.653301  52.045959  \n",
       "28  27.088703  32.239811  39.116005  \n",
       "29  27.569181  31.903282  37.366753  \n",
       "30  24.356153  26.837904  29.940657  \n",
       "31  20.569262  24.454823  30.254778  \n",
       "32  14.590513  18.296593  24.047079  \n",
       "33  10.973436  15.452991  23.168827  \n",
       "34   0.042250   0.017648   0.032953  \n",
       "35   0.039710   0.016081   0.030591  \n",
       "36   0.033509   0.004937   0.019742  \n",
       "37   0.031892   0.004147   0.018378  \n",
       "38   0.026220  -0.006554   0.008845  \n",
       "39   0.024859  -0.007024   0.007915  \n",
       "40   0.019815  -0.016293   0.001054  \n",
       "41   0.019252  -0.015660   0.001234  \n",
       "42   0.014983  -0.023261  -0.002419  \n",
       "43   0.014441  -0.022174  -0.001507  \n",
       "44   0.011442  -0.026332  -0.000218  \n",
       "45   0.011556  -0.025918   0.000006  \n",
       "46   0.009671  -0.028337   0.003379  \n",
       "47   0.009671  -0.028337   0.003379  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "model_M7 = tf.keras.Sequential([\n",
    "    layers.LSTM(units=64, return_sequences=True, input_shape=[52464, 8]),\n",
    "    layers.LSTM(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_M7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_M7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_M7.fit(np.array(Day0).reshape(52464, 1, 8), np.array(Day7).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_M7 = np.squeeze(model_M7.predict(np.array(df_test0).reshape(3888, 1, 8)))\n",
    "    pred_M7 = pd.DataFrame(pred_M7)\n",
    "    result_M7 = pd.concat([result_M7, pred_M7], axis=1)\n",
    "    \n",
    "result_M7[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "808/820 [============================>.] - ETA: 0s - loss: 1.5587WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.5578 - val_loss: 1.6492\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4543 - val_loss: 1.6545\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4309 - val_loss: 1.6668\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.4195 - val_loss: 1.6315\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4284 - val_loss: 1.6383\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4158 - val_loss: 1.6592\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4243 - val_loss: 1.6651\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "807/820 [============================>.] - ETA: 0s - loss: 2.4317WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 7s 5ms/step - loss: 2.4313 - val_loss: 2.7407\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3818 - val_loss: 2.7052\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3539 - val_loss: 2.7130\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3336 - val_loss: 2.6808\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3360 - val_loss: 2.6905\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3246 - val_loss: 2.7408\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3411 - val_loss: 2.6620\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3474 - val_loss: 2.7191\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3452 - val_loss: 2.6649\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3579 - val_loss: 2.6952\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "812/820 [============================>.] - ETA: 0s - loss: 2.8802WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.8800 - val_loss: 3.2763\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8439 - val_loss: 3.1953\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.8112 - val_loss: 3.2082\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8074 - val_loss: 3.1592\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8084 - val_loss: 3.1620\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7894 - val_loss: 3.2462\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8150 - val_loss: 3.1448\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8120 - val_loss: 3.2170\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8227 - val_loss: 3.1462\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8287 - val_loss: 3.2967\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - ETA: 0s - loss: 2.9895WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.9895 - val_loss: 3.3939\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9537 - val_loss: 3.2990\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9363 - val_loss: 3.2999\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9414 - val_loss: 3.3614\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9342 - val_loss: 3.2852\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 2.9161 - val_loss: 3.3128\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9578 - val_loss: 3.2969\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9477 - val_loss: 3.5658\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "810/820 [============================>.] - ETA: 0s - loss: 2.8737WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.8735 - val_loss: 3.1938\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8401 - val_loss: 3.1993\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8312 - val_loss: 3.2003\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8277 - val_loss: 3.2158\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "808/820 [============================>.] - ETA: 0s - loss: 2.5953WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.5951 - val_loss: 2.8839\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5565 - val_loss: 2.8732\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5443 - val_loss: 2.9140\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 2.5415 - val_loss: 2.9269\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5506 - val_loss: 2.9040\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "808/820 [============================>.] - ETA: 0s - loss: 2.1520WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.1517 - val_loss: 2.4089\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1196 - val_loss: 2.4087\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1093 - val_loss: 2.3781\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0975 - val_loss: 2.3974\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1161 - val_loss: 2.4043\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0793 - val_loss: 2.3648\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1210 - val_loss: 2.3939\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1078 - val_loss: 2.3857\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.0812 - val_loss: 2.3498\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.0944 - val_loss: 2.3895\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.0591 - val_loss: 2.3601\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.0917 - val_loss: 2.3365\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.0834 - val_loss: 2.3886\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.0737 - val_loss: 2.3474\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.0811 - val_loss: 2.3405\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "810/820 [============================>.] - ETA: 0s - loss: 1.5378WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 6s 4ms/step - loss: 1.5377 - val_loss: 1.7597\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.5193 - val_loss: 1.7246\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.5186 - val_loss: 1.7508\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.5156 - val_loss: 1.7440\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.5250 - val_loss: 1.7621\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "814/820 [============================>.] - ETA: 0s - loss: 0.8696WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 0.8696 - val_loss: 0.9997\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8533 - val_loss: 1.0148\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.8520 - val_loss: 0.9771\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.8531 - val_loss: 0.9603\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8549 - val_loss: 0.9780\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8356 - val_loss: 0.9542\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8571 - val_loss: 0.9592\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8466 - val_loss: 0.9816\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8319 - val_loss: 0.9446\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8485 - val_loss: 0.9375\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8228 - val_loss: 0.9450\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8317 - val_loss: 0.9417\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8254 - val_loss: 0.9540\n",
      "Epoch 00013: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004459</td>\n",
       "      <td>-0.008705</td>\n",
       "      <td>-0.016331</td>\n",
       "      <td>-0.012873</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>0.014229</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.031540</td>\n",
       "      <td>0.039548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.004180</td>\n",
       "      <td>-0.008579</td>\n",
       "      <td>-0.016586</td>\n",
       "      <td>-0.012574</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.032769</td>\n",
       "      <td>0.040871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.004035</td>\n",
       "      <td>-0.008178</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>-0.010843</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.034962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.004286</td>\n",
       "      <td>-0.008284</td>\n",
       "      <td>-0.016949</td>\n",
       "      <td>-0.011166</td>\n",
       "      <td>0.011336</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.022585</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004504</td>\n",
       "      <td>-0.008215</td>\n",
       "      <td>-0.017544</td>\n",
       "      <td>-0.010111</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.019797</td>\n",
       "      <td>0.035764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.003969</td>\n",
       "      <td>-0.007754</td>\n",
       "      <td>-0.017774</td>\n",
       "      <td>-0.009390</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.020536</td>\n",
       "      <td>0.036747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.003969</td>\n",
       "      <td>-0.007321</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.008168</td>\n",
       "      <td>0.011212</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>0.022477</td>\n",
       "      <td>0.042794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.002918</td>\n",
       "      <td>-0.006726</td>\n",
       "      <td>-0.019038</td>\n",
       "      <td>-0.006386</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>-0.000580</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>0.043593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.002470</td>\n",
       "      <td>-0.005955</td>\n",
       "      <td>-0.020004</td>\n",
       "      <td>-0.004453</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>-0.003809</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.050150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.002085</td>\n",
       "      <td>-0.006459</td>\n",
       "      <td>-0.020982</td>\n",
       "      <td>-0.004370</td>\n",
       "      <td>0.010595</td>\n",
       "      <td>-0.005873</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>0.049952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.001978</td>\n",
       "      <td>-0.005754</td>\n",
       "      <td>-0.022295</td>\n",
       "      <td>-0.004056</td>\n",
       "      <td>0.008588</td>\n",
       "      <td>-0.008251</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>0.057399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.001947</td>\n",
       "      <td>-0.006076</td>\n",
       "      <td>-0.022328</td>\n",
       "      <td>-0.004110</td>\n",
       "      <td>0.008468</td>\n",
       "      <td>-0.008483</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.056975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.002020</td>\n",
       "      <td>-0.004498</td>\n",
       "      <td>-0.022562</td>\n",
       "      <td>-0.004282</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>-0.008522</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.075394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.001987</td>\n",
       "      <td>-0.004810</td>\n",
       "      <td>-0.022639</td>\n",
       "      <td>-0.004355</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>-0.008912</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.030508</td>\n",
       "      <td>0.073933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>-0.022807</td>\n",
       "      <td>-0.005288</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>-0.008221</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>0.040208</td>\n",
       "      <td>0.116890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.234644</td>\n",
       "      <td>0.463549</td>\n",
       "      <td>0.761069</td>\n",
       "      <td>1.563330</td>\n",
       "      <td>2.646295</td>\n",
       "      <td>4.231714</td>\n",
       "      <td>6.971114</td>\n",
       "      <td>9.692414</td>\n",
       "      <td>13.553650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.068583</td>\n",
       "      <td>2.409046</td>\n",
       "      <td>4.675882</td>\n",
       "      <td>9.359540</td>\n",
       "      <td>13.214917</td>\n",
       "      <td>14.007280</td>\n",
       "      <td>18.917479</td>\n",
       "      <td>24.398327</td>\n",
       "      <td>33.296726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.043161</td>\n",
       "      <td>5.012439</td>\n",
       "      <td>9.585613</td>\n",
       "      <td>16.715616</td>\n",
       "      <td>20.855846</td>\n",
       "      <td>20.123676</td>\n",
       "      <td>26.380701</td>\n",
       "      <td>31.559332</td>\n",
       "      <td>38.425480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.095799</td>\n",
       "      <td>8.164346</td>\n",
       "      <td>14.750845</td>\n",
       "      <td>23.068312</td>\n",
       "      <td>27.154726</td>\n",
       "      <td>25.184723</td>\n",
       "      <td>32.590385</td>\n",
       "      <td>37.731945</td>\n",
       "      <td>45.355114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.390857</td>\n",
       "      <td>11.824681</td>\n",
       "      <td>19.311783</td>\n",
       "      <td>27.177267</td>\n",
       "      <td>30.514595</td>\n",
       "      <td>26.478289</td>\n",
       "      <td>31.496145</td>\n",
       "      <td>35.057873</td>\n",
       "      <td>39.932507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.802339</td>\n",
       "      <td>19.814159</td>\n",
       "      <td>26.721149</td>\n",
       "      <td>29.886969</td>\n",
       "      <td>30.452824</td>\n",
       "      <td>26.656958</td>\n",
       "      <td>30.779909</td>\n",
       "      <td>32.709866</td>\n",
       "      <td>34.859814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.388758</td>\n",
       "      <td>21.730694</td>\n",
       "      <td>27.419022</td>\n",
       "      <td>29.096201</td>\n",
       "      <td>29.635658</td>\n",
       "      <td>29.236378</td>\n",
       "      <td>34.296619</td>\n",
       "      <td>36.827160</td>\n",
       "      <td>38.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.203132</td>\n",
       "      <td>21.373383</td>\n",
       "      <td>30.123791</td>\n",
       "      <td>35.451000</td>\n",
       "      <td>37.933434</td>\n",
       "      <td>32.827522</td>\n",
       "      <td>38.408718</td>\n",
       "      <td>41.311008</td>\n",
       "      <td>44.888351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.597206</td>\n",
       "      <td>18.024208</td>\n",
       "      <td>26.291977</td>\n",
       "      <td>32.616802</td>\n",
       "      <td>35.180676</td>\n",
       "      <td>29.430502</td>\n",
       "      <td>34.368755</td>\n",
       "      <td>37.460190</td>\n",
       "      <td>41.908260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.399664</td>\n",
       "      <td>17.532560</td>\n",
       "      <td>25.250664</td>\n",
       "      <td>30.665916</td>\n",
       "      <td>32.757206</td>\n",
       "      <td>27.466944</td>\n",
       "      <td>32.340839</td>\n",
       "      <td>35.126392</td>\n",
       "      <td>38.594013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.915727</td>\n",
       "      <td>20.271654</td>\n",
       "      <td>27.582012</td>\n",
       "      <td>31.171041</td>\n",
       "      <td>32.775154</td>\n",
       "      <td>29.562883</td>\n",
       "      <td>35.402729</td>\n",
       "      <td>38.024281</td>\n",
       "      <td>41.027096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.401133</td>\n",
       "      <td>17.403168</td>\n",
       "      <td>24.454617</td>\n",
       "      <td>28.796335</td>\n",
       "      <td>30.479309</td>\n",
       "      <td>26.260344</td>\n",
       "      <td>31.550613</td>\n",
       "      <td>34.133892</td>\n",
       "      <td>36.677464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.192406</td>\n",
       "      <td>12.092266</td>\n",
       "      <td>19.799160</td>\n",
       "      <td>27.405876</td>\n",
       "      <td>30.629284</td>\n",
       "      <td>25.642973</td>\n",
       "      <td>31.001484</td>\n",
       "      <td>34.760403</td>\n",
       "      <td>40.601711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.801584</td>\n",
       "      <td>13.604786</td>\n",
       "      <td>20.210182</td>\n",
       "      <td>25.149429</td>\n",
       "      <td>26.669170</td>\n",
       "      <td>21.907263</td>\n",
       "      <td>25.833042</td>\n",
       "      <td>27.993830</td>\n",
       "      <td>29.190224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.289308</td>\n",
       "      <td>14.693268</td>\n",
       "      <td>20.902817</td>\n",
       "      <td>24.895205</td>\n",
       "      <td>26.095184</td>\n",
       "      <td>22.234713</td>\n",
       "      <td>26.813074</td>\n",
       "      <td>29.008127</td>\n",
       "      <td>30.395195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.699314</td>\n",
       "      <td>12.997721</td>\n",
       "      <td>17.906013</td>\n",
       "      <td>20.675901</td>\n",
       "      <td>21.210585</td>\n",
       "      <td>19.101952</td>\n",
       "      <td>23.671947</td>\n",
       "      <td>25.667013</td>\n",
       "      <td>26.782272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.141488</td>\n",
       "      <td>8.855782</td>\n",
       "      <td>14.510162</td>\n",
       "      <td>19.605568</td>\n",
       "      <td>20.842255</td>\n",
       "      <td>16.485331</td>\n",
       "      <td>18.788458</td>\n",
       "      <td>20.285421</td>\n",
       "      <td>20.439587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.513478</td>\n",
       "      <td>3.967975</td>\n",
       "      <td>7.886890</td>\n",
       "      <td>13.226873</td>\n",
       "      <td>14.994874</td>\n",
       "      <td>11.546255</td>\n",
       "      <td>12.393466</td>\n",
       "      <td>13.691509</td>\n",
       "      <td>13.802272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.320279</td>\n",
       "      <td>0.721823</td>\n",
       "      <td>1.470101</td>\n",
       "      <td>3.497169</td>\n",
       "      <td>5.759925</td>\n",
       "      <td>7.264647</td>\n",
       "      <td>10.291542</td>\n",
       "      <td>14.022621</td>\n",
       "      <td>20.541195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.003728</td>\n",
       "      <td>0.006942</td>\n",
       "      <td>-0.019830</td>\n",
       "      <td>-0.027979</td>\n",
       "      <td>-0.021775</td>\n",
       "      <td>-0.012140</td>\n",
       "      <td>0.043297</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.212867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.003797</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>-0.018900</td>\n",
       "      <td>-0.025557</td>\n",
       "      <td>-0.018845</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>0.040333</td>\n",
       "      <td>0.057133</td>\n",
       "      <td>0.206152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.003850</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>-0.017559</td>\n",
       "      <td>-0.026263</td>\n",
       "      <td>-0.018172</td>\n",
       "      <td>-0.007014</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.038302</td>\n",
       "      <td>0.129194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.003911</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>-0.017063</td>\n",
       "      <td>-0.024696</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-0.005746</td>\n",
       "      <td>0.032163</td>\n",
       "      <td>0.035495</td>\n",
       "      <td>0.126422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.003920</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>-0.015840</td>\n",
       "      <td>-0.025478</td>\n",
       "      <td>-0.015570</td>\n",
       "      <td>-0.001750</td>\n",
       "      <td>0.026702</td>\n",
       "      <td>0.020025</td>\n",
       "      <td>0.077067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.003996</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>-0.015451</td>\n",
       "      <td>-0.024012</td>\n",
       "      <td>-0.013960</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.025119</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.075892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.003967</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>-0.014370</td>\n",
       "      <td>-0.024648</td>\n",
       "      <td>-0.012855</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.021158</td>\n",
       "      <td>0.008355</td>\n",
       "      <td>0.053916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.004027</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.014302</td>\n",
       "      <td>-0.023672</td>\n",
       "      <td>-0.011939</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>0.055050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.003955</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.013326</td>\n",
       "      <td>-0.024217</td>\n",
       "      <td>-0.010593</td>\n",
       "      <td>0.010730</td>\n",
       "      <td>0.019082</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.054457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.004072</td>\n",
       "      <td>-0.002385</td>\n",
       "      <td>-0.013189</td>\n",
       "      <td>-0.022772</td>\n",
       "      <td>-0.009213</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>0.019075</td>\n",
       "      <td>0.006591</td>\n",
       "      <td>0.056564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.003984</td>\n",
       "      <td>-0.004031</td>\n",
       "      <td>-0.012468</td>\n",
       "      <td>-0.022902</td>\n",
       "      <td>-0.007419</td>\n",
       "      <td>0.019041</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>0.014081</td>\n",
       "      <td>0.072170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.003995</td>\n",
       "      <td>-0.004104</td>\n",
       "      <td>-0.012511</td>\n",
       "      <td>-0.022759</td>\n",
       "      <td>-0.007331</td>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.021788</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.072598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.003762</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>-0.023460</td>\n",
       "      <td>-0.005852</td>\n",
       "      <td>0.025252</td>\n",
       "      <td>0.026335</td>\n",
       "      <td>0.026427</td>\n",
       "      <td>0.093083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.003762</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>-0.023460</td>\n",
       "      <td>-0.005852</td>\n",
       "      <td>0.025252</td>\n",
       "      <td>0.026335</td>\n",
       "      <td>0.026427</td>\n",
       "      <td>0.093083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          0          0          0          0          0  \\\n",
       "0  -0.004459  -0.008705  -0.016331  -0.012873   0.011366   0.014229   \n",
       "1  -0.004180  -0.008579  -0.016586  -0.012574   0.011626   0.013720   \n",
       "2  -0.004035  -0.008178  -0.017271  -0.010843   0.011422   0.008403   \n",
       "3  -0.004286  -0.008284  -0.016949  -0.011166   0.011336   0.009346   \n",
       "4  -0.004504  -0.008215  -0.017544  -0.010111   0.010687   0.005679   \n",
       "5  -0.003969  -0.007754  -0.017774  -0.009390   0.011333   0.004812   \n",
       "6  -0.003969  -0.007321  -0.018142  -0.008168   0.011212   0.002407   \n",
       "7  -0.002918  -0.006726  -0.019038  -0.006386   0.011956  -0.000580   \n",
       "8  -0.002470  -0.005955  -0.020004  -0.004453   0.011636  -0.003809   \n",
       "9  -0.002085  -0.006459  -0.020982  -0.004370   0.010595  -0.005873   \n",
       "10 -0.001978  -0.005754  -0.022295  -0.004056   0.008588  -0.008251   \n",
       "11 -0.001947  -0.006076  -0.022328  -0.004110   0.008468  -0.008483   \n",
       "12 -0.002020  -0.004498  -0.022562  -0.004282   0.007034  -0.008522   \n",
       "13 -0.001987  -0.004810  -0.022639  -0.004355   0.006829  -0.008912   \n",
       "14 -0.002032  -0.002895  -0.022807  -0.005288   0.004632  -0.008221   \n",
       "15  0.234644   0.463549   0.761069   1.563330   2.646295   4.231714   \n",
       "16  1.068583   2.409046   4.675882   9.359540  13.214917  14.007280   \n",
       "17  2.043161   5.012439   9.585613  16.715616  20.855846  20.123676   \n",
       "18  3.095799   8.164346  14.750845  23.068312  27.154726  25.184723   \n",
       "19  4.390857  11.824681  19.311783  27.177267  30.514595  26.478289   \n",
       "20  7.802339  19.814159  26.721149  29.886969  30.452824  26.656958   \n",
       "21  9.388758  21.730694  27.419022  29.096201  29.635658  29.236378   \n",
       "22  8.203132  21.373383  30.123791  35.451000  37.933434  32.827522   \n",
       "23  6.597206  18.024208  26.291977  32.616802  35.180676  29.430502   \n",
       "24  6.399664  17.532560  25.250664  30.665916  32.757206  27.466944   \n",
       "25  7.915727  20.271654  27.582012  31.171041  32.775154  29.562883   \n",
       "26  6.401133  17.403168  24.454617  28.796335  30.479309  26.260344   \n",
       "27  4.192406  12.092266  19.799160  27.405876  30.629284  25.642973   \n",
       "28  4.801584  13.604786  20.210182  25.149429  26.669170  21.907263   \n",
       "29  5.289308  14.693268  20.902817  24.895205  26.095184  22.234713   \n",
       "30  4.699314  12.997721  17.906013  20.675901  21.210585  19.101952   \n",
       "31  3.141488   8.855782  14.510162  19.605568  20.842255  16.485331   \n",
       "32  1.513478   3.967975   7.886890  13.226873  14.994874  11.546255   \n",
       "33  0.320279   0.721823   1.470101   3.497169   5.759925   7.264647   \n",
       "34 -0.003728   0.006942  -0.019830  -0.027979  -0.021775  -0.012140   \n",
       "35 -0.003797   0.005865  -0.018900  -0.025557  -0.018845  -0.010223   \n",
       "36 -0.003850   0.004960  -0.017559  -0.026263  -0.018172  -0.007014   \n",
       "37 -0.003911   0.004112  -0.017063  -0.024696  -0.016378  -0.005746   \n",
       "38 -0.003920   0.002992  -0.015840  -0.025478  -0.015570  -0.001750   \n",
       "39 -0.003996   0.002086  -0.015451  -0.024012  -0.013960  -0.000426   \n",
       "40 -0.003967   0.000721  -0.014370  -0.024648  -0.012855   0.004374   \n",
       "41 -0.004027   0.000092  -0.014302  -0.023672  -0.011939   0.005202   \n",
       "42 -0.003955  -0.001375  -0.013326  -0.024217  -0.010593   0.010730   \n",
       "43 -0.004072  -0.002385  -0.013189  -0.022772  -0.009213   0.012431   \n",
       "44 -0.003984  -0.004031  -0.012468  -0.022902  -0.007419   0.019041   \n",
       "45 -0.003995  -0.004104  -0.012511  -0.022759  -0.007331   0.019190   \n",
       "46 -0.003762  -0.005146  -0.012048  -0.023460  -0.005852   0.025252   \n",
       "47 -0.003762  -0.005146  -0.012048  -0.023460  -0.005852   0.025252   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.009576   0.031540   0.039548  \n",
       "1    0.010237   0.032769   0.040871  \n",
       "2    0.004604   0.022989   0.034962  \n",
       "3    0.004492   0.022585   0.034200  \n",
       "4    0.003218   0.019797   0.035764  \n",
       "5    0.003572   0.020536   0.036747  \n",
       "6    0.005245   0.022477   0.042794  \n",
       "7    0.004924   0.022463   0.043593  \n",
       "8    0.006939   0.024896   0.050150  \n",
       "9    0.005827   0.023954   0.049952  \n",
       "10   0.007813   0.026255   0.057399  \n",
       "11   0.007253   0.025890   0.056975  \n",
       "12   0.011312   0.031193   0.075394  \n",
       "13   0.010515   0.030508   0.073933  \n",
       "14   0.017289   0.040208   0.116890  \n",
       "15   6.971114   9.692414  13.553650  \n",
       "16  18.917479  24.398327  33.296726  \n",
       "17  26.380701  31.559332  38.425480  \n",
       "18  32.590385  37.731945  45.355114  \n",
       "19  31.496145  35.057873  39.932507  \n",
       "20  30.779909  32.709866  34.859814  \n",
       "21  34.296619  36.827160  38.316700  \n",
       "22  38.408718  41.311008  44.888351  \n",
       "23  34.368755  37.460190  41.908260  \n",
       "24  32.340839  35.126392  38.594013  \n",
       "25  35.402729  38.024281  41.027096  \n",
       "26  31.550613  34.133892  36.677464  \n",
       "27  31.001484  34.760403  40.601711  \n",
       "28  25.833042  27.993830  29.190224  \n",
       "29  26.813074  29.008127  30.395195  \n",
       "30  23.671947  25.667013  26.782272  \n",
       "31  18.788458  20.285421  20.439587  \n",
       "32  12.393466  13.691509  13.802272  \n",
       "33  10.291542  14.022621  20.541195  \n",
       "34   0.043297   0.061839   0.212867  \n",
       "35   0.040333   0.057133   0.206152  \n",
       "36   0.034137   0.038302   0.129194  \n",
       "37   0.032163   0.035495   0.126422  \n",
       "38   0.026702   0.020025   0.077067  \n",
       "39   0.025119   0.018000   0.075892  \n",
       "40   0.021158   0.008355   0.053916  \n",
       "41   0.020561   0.008047   0.055050  \n",
       "42   0.019082   0.005911   0.054457  \n",
       "43   0.019075   0.006591   0.056564  \n",
       "44   0.021564   0.014081   0.072170  \n",
       "45   0.021788   0.014407   0.072598  \n",
       "46   0.026335   0.026427   0.093083  \n",
       "47   0.026335   0.026427   0.093083  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_M8 = tf.keras.Sequential([\n",
    "    layers.LSTM(units=64, return_sequences=True, input_shape=[52464, 8]),\n",
    "    layers.LSTM(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_M8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_M8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_M8.fit(np.array(Day0).reshape(52464, 1, 8), np.array(Day8).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_M8 = np.squeeze(model_M8.predict(np.array(df_test0).reshape(3888, 1, 8)))\n",
    "    pred_M8 = pd.DataFrame(pred_M8)\n",
    "    result_M8 = pd.concat([result_M8, pred_M8], axis=1)\n",
    "    \n",
    "result_M8[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_L0 = pd.DataFrame(results_1.sort_index())\n",
    "res_L0.columns = ['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']\n",
    "res_L1 = pd.DataFrame(results_2.sort_index())\n",
    "res_L1.columns = ['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']\n",
    "\n",
    "res_D0 = pd.DataFrame(results[0].sort_index())\n",
    "res_D0.columns = ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9']\n",
    "res_D1 = pd.DataFrame(results[1].sort_index())\n",
    "res_D1.columns = ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9']\n",
    "\n",
    "res_C0 = pd.DataFrame(result7.sort_index())\n",
    "res_C0.columns = ['C00.1','C00.2','C00.3','C00.4','C00.5','C00.6','C00.7','C00.8','C00.9']\n",
    "res_C1 = pd.DataFrame(result8.sort_index())\n",
    "res_C1.columns = ['C10.1','C10.2','C10.3','C10.4','C10.5','C10.6','C10.7','C10.8','C10.9']\n",
    "\n",
    "res_G0 = pd.DataFrame(result_G7.sort_index())\n",
    "res_G0.columns = ['G00.1','G00.2','G00.3','G00.4','G00.5','G00.6','G00.7','G00.8','G00.9']\n",
    "res_G1 = pd.DataFrame(result_G8.sort_index())\n",
    "res_G1.columns = ['G10.1','G10.2','G10.3','G10.4','G10.5','G10.6','G10.7','G10.8','G10.9']\n",
    "\n",
    "res_M0 = pd.DataFrame(result_M7.sort_index())\n",
    "res_M0.columns = ['M00.1','M00.2','M00.3','M00.4','M00.5','M00.6','M00.7','M00.8','M00.9']\n",
    "res_M1 = pd.DataFrame(result_M8.sort_index())\n",
    "res_M1.columns = ['M10.1','M10.2','M10.3','M10.4','M10.5','M10.6','M10.7','M10.8','M10.9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0 = pd.DataFrame()\n",
    "res_1= pd.DataFrame()\n",
    "res_0 = pd.concat([res_L0, res_D0, res_C0, res_G0, res_M0], axis=1)\n",
    "res_1 = pd.concat([res_L1, res_D1, res_C1, res_G1, res_M1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L00.1</th>\n",
       "      <th>L00.2</th>\n",
       "      <th>L00.3</th>\n",
       "      <th>L00.4</th>\n",
       "      <th>L00.5</th>\n",
       "      <th>L00.6</th>\n",
       "      <th>L00.7</th>\n",
       "      <th>L00.8</th>\n",
       "      <th>L00.9</th>\n",
       "      <th>D00.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G00.9</th>\n",
       "      <th>M00.1</th>\n",
       "      <th>M00.2</th>\n",
       "      <th>M00.3</th>\n",
       "      <th>M00.4</th>\n",
       "      <th>M00.5</th>\n",
       "      <th>M00.6</th>\n",
       "      <th>M00.7</th>\n",
       "      <th>M00.8</th>\n",
       "      <th>M00.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.004501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>-0.019125</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.007617</td>\n",
       "      <td>-0.005385</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>0.023319</td>\n",
       "      <td>0.018366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.004412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>-0.019038</td>\n",
       "      <td>-0.018860</td>\n",
       "      <td>-0.007571</td>\n",
       "      <td>-0.005126</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.019592</td>\n",
       "      <td>0.022475</td>\n",
       "      <td>0.017256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.003122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>-0.018445</td>\n",
       "      <td>-0.018930</td>\n",
       "      <td>-0.008777</td>\n",
       "      <td>-0.007531</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>0.015187</td>\n",
       "      <td>0.017779</td>\n",
       "      <td>0.009924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.003097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>-0.018493</td>\n",
       "      <td>-0.019040</td>\n",
       "      <td>-0.008770</td>\n",
       "      <td>-0.007772</td>\n",
       "      <td>-0.000332</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.016030</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>0.011051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006686</td>\n",
       "      <td>-0.018082</td>\n",
       "      <td>-0.019398</td>\n",
       "      <td>-0.009946</td>\n",
       "      <td>-0.010143</td>\n",
       "      <td>-0.001520</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>0.013279</td>\n",
       "      <td>0.015594</td>\n",
       "      <td>0.007459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.002604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>-0.017874</td>\n",
       "      <td>-0.018921</td>\n",
       "      <td>-0.009755</td>\n",
       "      <td>-0.009475</td>\n",
       "      <td>-0.001727</td>\n",
       "      <td>-0.002171</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.014926</td>\n",
       "      <td>0.005757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.002187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005813</td>\n",
       "      <td>-0.017307</td>\n",
       "      <td>-0.018858</td>\n",
       "      <td>-0.010540</td>\n",
       "      <td>-0.011035</td>\n",
       "      <td>-0.002522</td>\n",
       "      <td>-0.002984</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.013476</td>\n",
       "      <td>0.003971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.001872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>-0.017095</td>\n",
       "      <td>-0.018107</td>\n",
       "      <td>-0.009864</td>\n",
       "      <td>-0.008948</td>\n",
       "      <td>-0.001966</td>\n",
       "      <td>-0.001984</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>0.001485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>-0.016474</td>\n",
       "      <td>-0.017599</td>\n",
       "      <td>-0.009730</td>\n",
       "      <td>-0.008396</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>0.015984</td>\n",
       "      <td>0.003385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.003119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>-0.016529</td>\n",
       "      <td>-0.017173</td>\n",
       "      <td>-0.008936</td>\n",
       "      <td>-0.006776</td>\n",
       "      <td>-0.000407</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.012892</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>0.003213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005648</td>\n",
       "      <td>-0.016024</td>\n",
       "      <td>-0.016882</td>\n",
       "      <td>-0.008807</td>\n",
       "      <td>-0.006280</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>0.018502</td>\n",
       "      <td>0.023334</td>\n",
       "      <td>0.010275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.003289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>-0.015915</td>\n",
       "      <td>-0.016484</td>\n",
       "      <td>-0.008372</td>\n",
       "      <td>-0.005836</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>0.018033</td>\n",
       "      <td>0.023613</td>\n",
       "      <td>0.010353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.000824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010137</td>\n",
       "      <td>-0.015587</td>\n",
       "      <td>-0.016601</td>\n",
       "      <td>-0.008875</td>\n",
       "      <td>-0.006145</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.024712</td>\n",
       "      <td>0.029036</td>\n",
       "      <td>0.017955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.001096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>-0.015502</td>\n",
       "      <td>-0.016263</td>\n",
       "      <td>-0.008512</td>\n",
       "      <td>-0.005748</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.024236</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>0.018322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028768</td>\n",
       "      <td>-0.015438</td>\n",
       "      <td>-0.016671</td>\n",
       "      <td>-0.009089</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>0.020066</td>\n",
       "      <td>0.033335</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.024706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "      <td>0.921596</td>\n",
       "      <td>...</td>\n",
       "      <td>8.961095</td>\n",
       "      <td>0.273712</td>\n",
       "      <td>0.579337</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>1.429537</td>\n",
       "      <td>2.459546</td>\n",
       "      <td>4.434146</td>\n",
       "      <td>5.853877</td>\n",
       "      <td>8.413433</td>\n",
       "      <td>13.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "      <td>1.866856</td>\n",
       "      <td>...</td>\n",
       "      <td>21.688187</td>\n",
       "      <td>1.359357</td>\n",
       "      <td>3.117102</td>\n",
       "      <td>4.482175</td>\n",
       "      <td>7.221376</td>\n",
       "      <td>10.993061</td>\n",
       "      <td>15.369520</td>\n",
       "      <td>17.760300</td>\n",
       "      <td>25.476889</td>\n",
       "      <td>36.946301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "      <td>2.941126</td>\n",
       "      <td>...</td>\n",
       "      <td>28.554996</td>\n",
       "      <td>2.629808</td>\n",
       "      <td>6.297938</td>\n",
       "      <td>8.574986</td>\n",
       "      <td>12.396621</td>\n",
       "      <td>16.806257</td>\n",
       "      <td>21.061373</td>\n",
       "      <td>24.383640</td>\n",
       "      <td>33.391483</td>\n",
       "      <td>45.629196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "      <td>6.280856</td>\n",
       "      <td>...</td>\n",
       "      <td>43.742489</td>\n",
       "      <td>4.027555</td>\n",
       "      <td>10.051889</td>\n",
       "      <td>12.806709</td>\n",
       "      <td>16.891878</td>\n",
       "      <td>21.427212</td>\n",
       "      <td>25.463512</td>\n",
       "      <td>29.728369</td>\n",
       "      <td>39.646595</td>\n",
       "      <td>52.515381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "      <td>7.377127</td>\n",
       "      <td>...</td>\n",
       "      <td>41.340168</td>\n",
       "      <td>5.634828</td>\n",
       "      <td>13.959082</td>\n",
       "      <td>16.527153</td>\n",
       "      <td>20.266357</td>\n",
       "      <td>24.226301</td>\n",
       "      <td>27.340965</td>\n",
       "      <td>30.688051</td>\n",
       "      <td>39.474838</td>\n",
       "      <td>51.019337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "      <td>11.317371</td>\n",
       "      <td>...</td>\n",
       "      <td>39.558636</td>\n",
       "      <td>9.294259</td>\n",
       "      <td>21.544348</td>\n",
       "      <td>22.042635</td>\n",
       "      <td>23.890402</td>\n",
       "      <td>26.176407</td>\n",
       "      <td>27.946430</td>\n",
       "      <td>31.602259</td>\n",
       "      <td>34.815697</td>\n",
       "      <td>38.098289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "      <td>12.153330</td>\n",
       "      <td>...</td>\n",
       "      <td>39.862083</td>\n",
       "      <td>10.555115</td>\n",
       "      <td>23.344545</td>\n",
       "      <td>22.718353</td>\n",
       "      <td>23.974604</td>\n",
       "      <td>27.213102</td>\n",
       "      <td>29.650118</td>\n",
       "      <td>34.550789</td>\n",
       "      <td>36.576504</td>\n",
       "      <td>38.338005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "      <td>14.725376</td>\n",
       "      <td>...</td>\n",
       "      <td>52.656006</td>\n",
       "      <td>9.744592</td>\n",
       "      <td>23.326319</td>\n",
       "      <td>24.893036</td>\n",
       "      <td>28.041769</td>\n",
       "      <td>30.543203</td>\n",
       "      <td>33.171665</td>\n",
       "      <td>38.021835</td>\n",
       "      <td>45.816437</td>\n",
       "      <td>53.586349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "      <td>14.160217</td>\n",
       "      <td>...</td>\n",
       "      <td>51.620369</td>\n",
       "      <td>8.270817</td>\n",
       "      <td>20.399950</td>\n",
       "      <td>22.187107</td>\n",
       "      <td>25.267614</td>\n",
       "      <td>28.074375</td>\n",
       "      <td>30.464939</td>\n",
       "      <td>34.652565</td>\n",
       "      <td>42.893795</td>\n",
       "      <td>52.498535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "      <td>14.352590</td>\n",
       "      <td>...</td>\n",
       "      <td>47.399540</td>\n",
       "      <td>8.091259</td>\n",
       "      <td>20.220682</td>\n",
       "      <td>21.496767</td>\n",
       "      <td>24.042891</td>\n",
       "      <td>26.663202</td>\n",
       "      <td>28.833647</td>\n",
       "      <td>32.890366</td>\n",
       "      <td>39.819202</td>\n",
       "      <td>47.987965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "      <td>13.911210</td>\n",
       "      <td>...</td>\n",
       "      <td>45.579853</td>\n",
       "      <td>9.405066</td>\n",
       "      <td>22.633169</td>\n",
       "      <td>23.153381</td>\n",
       "      <td>25.253382</td>\n",
       "      <td>27.913637</td>\n",
       "      <td>30.344498</td>\n",
       "      <td>35.113995</td>\n",
       "      <td>40.123653</td>\n",
       "      <td>45.054546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "      <td>12.595205</td>\n",
       "      <td>...</td>\n",
       "      <td>42.602364</td>\n",
       "      <td>8.068606</td>\n",
       "      <td>20.320787</td>\n",
       "      <td>21.009787</td>\n",
       "      <td>23.008614</td>\n",
       "      <td>25.579903</td>\n",
       "      <td>27.675587</td>\n",
       "      <td>31.922415</td>\n",
       "      <td>37.531811</td>\n",
       "      <td>43.983574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "      <td>13.617439</td>\n",
       "      <td>...</td>\n",
       "      <td>52.365910</td>\n",
       "      <td>5.621737</td>\n",
       "      <td>14.933422</td>\n",
       "      <td>16.959221</td>\n",
       "      <td>20.051760</td>\n",
       "      <td>23.654943</td>\n",
       "      <td>26.390972</td>\n",
       "      <td>30.416721</td>\n",
       "      <td>39.653301</td>\n",
       "      <td>52.045959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "      <td>10.133067</td>\n",
       "      <td>...</td>\n",
       "      <td>36.880199</td>\n",
       "      <td>6.378386</td>\n",
       "      <td>16.757147</td>\n",
       "      <td>17.597227</td>\n",
       "      <td>19.462786</td>\n",
       "      <td>22.070230</td>\n",
       "      <td>23.784245</td>\n",
       "      <td>27.088703</td>\n",
       "      <td>32.239811</td>\n",
       "      <td>39.116005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "      <td>10.151043</td>\n",
       "      <td>...</td>\n",
       "      <td>35.761631</td>\n",
       "      <td>6.854388</td>\n",
       "      <td>17.689793</td>\n",
       "      <td>18.050587</td>\n",
       "      <td>19.585070</td>\n",
       "      <td>22.130251</td>\n",
       "      <td>23.855522</td>\n",
       "      <td>27.569181</td>\n",
       "      <td>31.903282</td>\n",
       "      <td>37.366753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "      <td>6.971280</td>\n",
       "      <td>...</td>\n",
       "      <td>29.087336</td>\n",
       "      <td>6.258477</td>\n",
       "      <td>16.409973</td>\n",
       "      <td>16.251860</td>\n",
       "      <td>17.124327</td>\n",
       "      <td>19.470133</td>\n",
       "      <td>20.855913</td>\n",
       "      <td>24.356153</td>\n",
       "      <td>26.837904</td>\n",
       "      <td>29.940657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "      <td>7.210488</td>\n",
       "      <td>...</td>\n",
       "      <td>28.108219</td>\n",
       "      <td>4.302255</td>\n",
       "      <td>11.597221</td>\n",
       "      <td>12.626295</td>\n",
       "      <td>14.420085</td>\n",
       "      <td>17.028147</td>\n",
       "      <td>18.435785</td>\n",
       "      <td>20.569262</td>\n",
       "      <td>24.454823</td>\n",
       "      <td>30.254778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "      <td>3.828187</td>\n",
       "      <td>...</td>\n",
       "      <td>18.057730</td>\n",
       "      <td>2.116210</td>\n",
       "      <td>5.766467</td>\n",
       "      <td>7.318921</td>\n",
       "      <td>9.462689</td>\n",
       "      <td>12.242281</td>\n",
       "      <td>13.908647</td>\n",
       "      <td>14.590513</td>\n",
       "      <td>18.296593</td>\n",
       "      <td>24.047079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "      <td>2.870463</td>\n",
       "      <td>...</td>\n",
       "      <td>14.699933</td>\n",
       "      <td>0.430942</td>\n",
       "      <td>1.111121</td>\n",
       "      <td>1.766121</td>\n",
       "      <td>3.076553</td>\n",
       "      <td>5.438250</td>\n",
       "      <td>9.089475</td>\n",
       "      <td>10.973436</td>\n",
       "      <td>15.452991</td>\n",
       "      <td>23.168827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.007546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046391</td>\n",
       "      <td>-0.008247</td>\n",
       "      <td>-0.008145</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>0.033578</td>\n",
       "      <td>0.042250</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.032953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.008617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044147</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>-0.008705</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.000724</td>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.031599</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.016081</td>\n",
       "      <td>0.030591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.002510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043402</td>\n",
       "      <td>-0.007985</td>\n",
       "      <td>-0.008566</td>\n",
       "      <td>-0.005564</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.029768</td>\n",
       "      <td>0.033509</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.019742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.002523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041957</td>\n",
       "      <td>-0.008071</td>\n",
       "      <td>-0.009059</td>\n",
       "      <td>-0.005876</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>0.008577</td>\n",
       "      <td>0.028346</td>\n",
       "      <td>0.031892</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.018378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.009464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>-0.007806</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.006104</td>\n",
       "      <td>-0.001103</td>\n",
       "      <td>0.007922</td>\n",
       "      <td>0.026834</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>-0.006554</td>\n",
       "      <td>0.008845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.009677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>-0.007920</td>\n",
       "      <td>-0.009575</td>\n",
       "      <td>-0.006457</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>0.007509</td>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>-0.007024</td>\n",
       "      <td>0.007915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.008105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>-0.007726</td>\n",
       "      <td>-0.009629</td>\n",
       "      <td>-0.006779</td>\n",
       "      <td>-0.001673</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.023953</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>-0.016293</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.008131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029611</td>\n",
       "      <td>-0.007800</td>\n",
       "      <td>-0.010120</td>\n",
       "      <td>-0.007105</td>\n",
       "      <td>-0.001845</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.019252</td>\n",
       "      <td>-0.015660</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.004441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022941</td>\n",
       "      <td>-0.007657</td>\n",
       "      <td>-0.010208</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.021487</td>\n",
       "      <td>0.014983</td>\n",
       "      <td>-0.023261</td>\n",
       "      <td>-0.002419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.004512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021618</td>\n",
       "      <td>-0.007826</td>\n",
       "      <td>-0.011047</td>\n",
       "      <td>-0.007969</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>0.014441</td>\n",
       "      <td>-0.022174</td>\n",
       "      <td>-0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.005342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>-0.007742</td>\n",
       "      <td>-0.011445</td>\n",
       "      <td>-0.008594</td>\n",
       "      <td>-0.003150</td>\n",
       "      <td>0.004102</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>0.011442</td>\n",
       "      <td>-0.026332</td>\n",
       "      <td>-0.000218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.005297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>-0.007746</td>\n",
       "      <td>-0.011532</td>\n",
       "      <td>-0.008651</td>\n",
       "      <td>-0.003109</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.018503</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>-0.025918</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.009501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>-0.007516</td>\n",
       "      <td>-0.011504</td>\n",
       "      <td>-0.009133</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>-0.028337</td>\n",
       "      <td>0.003379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.009501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>-0.007516</td>\n",
       "      <td>-0.011504</td>\n",
       "      <td>-0.009133</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>-0.028337</td>\n",
       "      <td>0.003379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L00.1  L00.2  L00.3  L00.4  L00.5  L00.6  L00.7  L00.8  L00.9      D00.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.004501   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.004412   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.003122   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.003097   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.002667   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.002604   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.002187   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.001872   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.001634   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.003119   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.002916   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.003289   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.000824   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.001096   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.001742   \n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45   0.921596   \n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51   1.866856   \n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11   2.941126   \n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34   6.280856   \n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51   7.377127   \n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17  11.317371   \n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03  12.153330   \n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17  14.725376   \n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82  14.160217   \n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14  14.352590   \n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61  13.911210   \n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92  12.595205   \n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20  13.617439   \n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73  10.133067   \n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94  10.151043   \n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20   6.971280   \n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59   7.210488   \n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49   3.828187   \n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13   2.870463   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.007546   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.008617   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.002510   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.002523   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.009464   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.009677   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.008105   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.008131   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.004441   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.004512   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.005342   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.005297   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.009501   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.009501   \n",
       "\n",
       "    ...      G00.9      M00.1      M00.2      M00.3      M00.4      M00.5  \\\n",
       "0   ...   0.007020  -0.019125  -0.019102  -0.007617  -0.005385   0.001388   \n",
       "1   ...   0.007331  -0.019038  -0.018860  -0.007571  -0.005126   0.001064   \n",
       "2   ...   0.007389  -0.018445  -0.018930  -0.008777  -0.007531  -0.000556   \n",
       "3   ...   0.007342  -0.018493  -0.019040  -0.008770  -0.007772  -0.000332   \n",
       "4   ...   0.006686  -0.018082  -0.019398  -0.009946  -0.010143  -0.001520   \n",
       "5   ...   0.006740  -0.017874  -0.018921  -0.009755  -0.009475  -0.001727   \n",
       "6   ...   0.005813  -0.017307  -0.018858  -0.010540  -0.011035  -0.002522   \n",
       "7   ...   0.005085  -0.017095  -0.018107  -0.009864  -0.008948  -0.001966   \n",
       "8   ...   0.003939  -0.016474  -0.017599  -0.009730  -0.008396  -0.000995   \n",
       "9   ...   0.004025  -0.016529  -0.017173  -0.008936  -0.006776  -0.000407   \n",
       "10  ...   0.005648  -0.016024  -0.016882  -0.008807  -0.006280   0.001219   \n",
       "11  ...   0.005092  -0.015915  -0.016484  -0.008372  -0.005836   0.000777   \n",
       "12  ...   0.010137  -0.015587  -0.016601  -0.008875  -0.006145   0.003093   \n",
       "13  ...   0.009231  -0.015502  -0.016263  -0.008512  -0.005748   0.002602   \n",
       "14  ...   0.028768  -0.015438  -0.016671  -0.009089  -0.005811   0.006752   \n",
       "15  ...   8.961095   0.273712   0.579337   0.847134   1.429537   2.459546   \n",
       "16  ...  21.688187   1.359357   3.117102   4.482175   7.221376  10.993061   \n",
       "17  ...  28.554996   2.629808   6.297938   8.574986  12.396621  16.806257   \n",
       "18  ...  43.742489   4.027555  10.051889  12.806709  16.891878  21.427212   \n",
       "19  ...  41.340168   5.634828  13.959082  16.527153  20.266357  24.226301   \n",
       "20  ...  39.558636   9.294259  21.544348  22.042635  23.890402  26.176407   \n",
       "21  ...  39.862083  10.555115  23.344545  22.718353  23.974604  27.213102   \n",
       "22  ...  52.656006   9.744592  23.326319  24.893036  28.041769  30.543203   \n",
       "23  ...  51.620369   8.270817  20.399950  22.187107  25.267614  28.074375   \n",
       "24  ...  47.399540   8.091259  20.220682  21.496767  24.042891  26.663202   \n",
       "25  ...  45.579853   9.405066  22.633169  23.153381  25.253382  27.913637   \n",
       "26  ...  42.602364   8.068606  20.320787  21.009787  23.008614  25.579903   \n",
       "27  ...  52.365910   5.621737  14.933422  16.959221  20.051760  23.654943   \n",
       "28  ...  36.880199   6.378386  16.757147  17.597227  19.462786  22.070230   \n",
       "29  ...  35.761631   6.854388  17.689793  18.050587  19.585070  22.130251   \n",
       "30  ...  29.087336   6.258477  16.409973  16.251860  17.124327  19.470133   \n",
       "31  ...  28.108219   4.302255  11.597221  12.626295  14.420085  17.028147   \n",
       "32  ...  18.057730   2.116210   5.766467   7.318921   9.462689  12.242281   \n",
       "33  ...  14.699933   0.430942   1.111121   1.766121   3.076553   5.438250   \n",
       "34  ...   0.046391  -0.008247  -0.008145  -0.005010  -0.000351   0.010345   \n",
       "35  ...   0.044147  -0.008333  -0.008705  -0.005363  -0.000724   0.009779   \n",
       "36  ...   0.043402  -0.007985  -0.008566  -0.005564  -0.000713   0.009021   \n",
       "37  ...   0.041957  -0.008071  -0.009059  -0.005876  -0.001015   0.008577   \n",
       "38  ...   0.037853  -0.007806  -0.008996  -0.006104  -0.001103   0.007922   \n",
       "39  ...   0.036539  -0.007920  -0.009575  -0.006457  -0.001442   0.007509   \n",
       "40  ...   0.030534  -0.007726  -0.009629  -0.006779  -0.001673   0.006751   \n",
       "41  ...   0.029611  -0.007800  -0.010120  -0.007105  -0.001845   0.006411   \n",
       "42  ...   0.022941  -0.007657  -0.010208  -0.007479  -0.002228   0.005563   \n",
       "43  ...   0.021618  -0.007826  -0.011047  -0.007969  -0.002591   0.005218   \n",
       "44  ...   0.014632  -0.007742  -0.011445  -0.008594  -0.003150   0.004102   \n",
       "45  ...   0.014444  -0.007746  -0.011532  -0.008651  -0.003109   0.004112   \n",
       "46  ...   0.008403  -0.007516  -0.011504  -0.009133  -0.003429   0.002778   \n",
       "47  ...   0.008403  -0.007516  -0.011504  -0.009133  -0.003429   0.002778   \n",
       "\n",
       "        M00.6      M00.7      M00.8      M00.9  \n",
       "0    0.002855   0.020482   0.023319   0.018366  \n",
       "1    0.002280   0.019592   0.022475   0.017256  \n",
       "2   -0.000474   0.015187   0.017779   0.009924  \n",
       "3   -0.000109   0.016030   0.018532   0.011051  \n",
       "4   -0.001800   0.013279   0.015594   0.007459  \n",
       "5   -0.002171   0.012406   0.014926   0.005757  \n",
       "6   -0.002984   0.010963   0.013476   0.003971  \n",
       "7   -0.001984   0.010204   0.013302   0.001485  \n",
       "8    0.000788   0.012231   0.015984   0.003385  \n",
       "9    0.001950   0.012892   0.017088   0.003213  \n",
       "10   0.006616   0.018502   0.023334   0.010275  \n",
       "11   0.005643   0.018033   0.023613   0.010353  \n",
       "12   0.011568   0.024712   0.029036   0.017955  \n",
       "13   0.010554   0.024236   0.029514   0.018322  \n",
       "14   0.020066   0.033335   0.033908   0.024706  \n",
       "15   4.434146   5.853877   8.413433  13.160100  \n",
       "16  15.369520  17.760300  25.476889  36.946301  \n",
       "17  21.061373  24.383640  33.391483  45.629196  \n",
       "18  25.463512  29.728369  39.646595  52.515381  \n",
       "19  27.340965  30.688051  39.474838  51.019337  \n",
       "20  27.946430  31.602259  34.815697  38.098289  \n",
       "21  29.650118  34.550789  36.576504  38.338005  \n",
       "22  33.171665  38.021835  45.816437  53.586349  \n",
       "23  30.464939  34.652565  42.893795  52.498535  \n",
       "24  28.833647  32.890366  39.819202  47.987965  \n",
       "25  30.344498  35.113995  40.123653  45.054546  \n",
       "26  27.675587  31.922415  37.531811  43.983574  \n",
       "27  26.390972  30.416721  39.653301  52.045959  \n",
       "28  23.784245  27.088703  32.239811  39.116005  \n",
       "29  23.855522  27.569181  31.903282  37.366753  \n",
       "30  20.855913  24.356153  26.837904  29.940657  \n",
       "31  18.435785  20.569262  24.454823  30.254778  \n",
       "32  13.908647  14.590513  18.296593  24.047079  \n",
       "33   9.089475  10.973436  15.452991  23.168827  \n",
       "34   0.033578   0.042250   0.017648   0.032953  \n",
       "35   0.031599   0.039710   0.016081   0.030591  \n",
       "36   0.029768   0.033509   0.004937   0.019742  \n",
       "37   0.028346   0.031892   0.004147   0.018378  \n",
       "38   0.026834   0.026220  -0.006554   0.008845  \n",
       "39   0.025504   0.024859  -0.007024   0.007915  \n",
       "40   0.023953   0.019815  -0.016293   0.001054  \n",
       "41   0.023038   0.019252  -0.015660   0.001234  \n",
       "42   0.021487   0.014983  -0.023261  -0.002419  \n",
       "43   0.020327   0.014441  -0.022174  -0.001507  \n",
       "44   0.018510   0.011442  -0.026332  -0.000218  \n",
       "45   0.018503   0.011556  -0.025918   0.000006  \n",
       "46   0.017084   0.009671  -0.028337   0.003379  \n",
       "47   0.017084   0.009671  -0.028337   0.003379  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_0[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L10.1</th>\n",
       "      <th>L10.2</th>\n",
       "      <th>L10.3</th>\n",
       "      <th>L10.4</th>\n",
       "      <th>L10.5</th>\n",
       "      <th>L10.6</th>\n",
       "      <th>L10.7</th>\n",
       "      <th>L10.8</th>\n",
       "      <th>L10.9</th>\n",
       "      <th>D10.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G10.9</th>\n",
       "      <th>M10.1</th>\n",
       "      <th>M10.2</th>\n",
       "      <th>M10.3</th>\n",
       "      <th>M10.4</th>\n",
       "      <th>M10.5</th>\n",
       "      <th>M10.6</th>\n",
       "      <th>M10.7</th>\n",
       "      <th>M10.8</th>\n",
       "      <th>M10.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.012377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007649</td>\n",
       "      <td>-0.004459</td>\n",
       "      <td>-0.008705</td>\n",
       "      <td>-0.016331</td>\n",
       "      <td>-0.012873</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>0.014229</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.031540</td>\n",
       "      <td>0.039548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.011908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007372</td>\n",
       "      <td>-0.004180</td>\n",
       "      <td>-0.008579</td>\n",
       "      <td>-0.016586</td>\n",
       "      <td>-0.012574</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.032769</td>\n",
       "      <td>0.040871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>-0.004035</td>\n",
       "      <td>-0.008178</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>-0.010843</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.034962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.010701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005790</td>\n",
       "      <td>-0.004286</td>\n",
       "      <td>-0.008284</td>\n",
       "      <td>-0.016949</td>\n",
       "      <td>-0.011166</td>\n",
       "      <td>0.011336</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.022585</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.011175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>-0.004504</td>\n",
       "      <td>-0.008215</td>\n",
       "      <td>-0.017544</td>\n",
       "      <td>-0.010111</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.019797</td>\n",
       "      <td>0.035764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.010739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>-0.003969</td>\n",
       "      <td>-0.007754</td>\n",
       "      <td>-0.017774</td>\n",
       "      <td>-0.009390</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.020536</td>\n",
       "      <td>0.036747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.011099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>-0.003969</td>\n",
       "      <td>-0.007321</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.008168</td>\n",
       "      <td>0.011212</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>0.022477</td>\n",
       "      <td>0.042794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.009779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.002918</td>\n",
       "      <td>-0.006726</td>\n",
       "      <td>-0.019038</td>\n",
       "      <td>-0.006386</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>-0.000580</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>0.043593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.009821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>-0.002470</td>\n",
       "      <td>-0.005955</td>\n",
       "      <td>-0.020004</td>\n",
       "      <td>-0.004453</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>-0.003809</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.050150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.011221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008529</td>\n",
       "      <td>-0.002085</td>\n",
       "      <td>-0.006459</td>\n",
       "      <td>-0.020982</td>\n",
       "      <td>-0.004370</td>\n",
       "      <td>0.010595</td>\n",
       "      <td>-0.005873</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>0.049952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.012145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014435</td>\n",
       "      <td>-0.001978</td>\n",
       "      <td>-0.005754</td>\n",
       "      <td>-0.022295</td>\n",
       "      <td>-0.004056</td>\n",
       "      <td>0.008588</td>\n",
       "      <td>-0.008251</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>0.057399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.012509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014221</td>\n",
       "      <td>-0.001947</td>\n",
       "      <td>-0.006076</td>\n",
       "      <td>-0.022328</td>\n",
       "      <td>-0.004110</td>\n",
       "      <td>0.008468</td>\n",
       "      <td>-0.008483</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.056975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.010746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032387</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>-0.004498</td>\n",
       "      <td>-0.022562</td>\n",
       "      <td>-0.004282</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>-0.008522</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.075394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.011147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033276</td>\n",
       "      <td>-0.001987</td>\n",
       "      <td>-0.004810</td>\n",
       "      <td>-0.022639</td>\n",
       "      <td>-0.004355</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>-0.008912</td>\n",
       "      <td>0.010515</td>\n",
       "      <td>0.030508</td>\n",
       "      <td>0.073933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.007546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099238</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>-0.022807</td>\n",
       "      <td>-0.005288</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>-0.008221</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>0.040208</td>\n",
       "      <td>0.116890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.87</td>\n",
       "      <td>0.639051</td>\n",
       "      <td>...</td>\n",
       "      <td>11.893352</td>\n",
       "      <td>0.234644</td>\n",
       "      <td>0.463549</td>\n",
       "      <td>0.761069</td>\n",
       "      <td>1.563330</td>\n",
       "      <td>2.646295</td>\n",
       "      <td>4.231714</td>\n",
       "      <td>6.971114</td>\n",
       "      <td>9.692414</td>\n",
       "      <td>13.553650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.92</td>\n",
       "      <td>1.814343</td>\n",
       "      <td>...</td>\n",
       "      <td>25.218607</td>\n",
       "      <td>1.068583</td>\n",
       "      <td>2.409046</td>\n",
       "      <td>4.675882</td>\n",
       "      <td>9.359540</td>\n",
       "      <td>13.214917</td>\n",
       "      <td>14.007280</td>\n",
       "      <td>18.917479</td>\n",
       "      <td>24.398327</td>\n",
       "      <td>33.296726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>18.24</td>\n",
       "      <td>25.56</td>\n",
       "      <td>3.588575</td>\n",
       "      <td>...</td>\n",
       "      <td>29.591112</td>\n",
       "      <td>2.043161</td>\n",
       "      <td>5.012439</td>\n",
       "      <td>9.585613</td>\n",
       "      <td>16.715616</td>\n",
       "      <td>20.855846</td>\n",
       "      <td>20.123676</td>\n",
       "      <td>26.380701</td>\n",
       "      <td>31.559332</td>\n",
       "      <td>38.425480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.74</td>\n",
       "      <td>14.78</td>\n",
       "      <td>19.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>22.60</td>\n",
       "      <td>20.57</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.66</td>\n",
       "      <td>32.96</td>\n",
       "      <td>7.595252</td>\n",
       "      <td>...</td>\n",
       "      <td>41.306126</td>\n",
       "      <td>3.095799</td>\n",
       "      <td>8.164346</td>\n",
       "      <td>14.750845</td>\n",
       "      <td>23.068312</td>\n",
       "      <td>27.154726</td>\n",
       "      <td>25.184723</td>\n",
       "      <td>32.590385</td>\n",
       "      <td>37.731945</td>\n",
       "      <td>45.355114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.47</td>\n",
       "      <td>19.06</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.62</td>\n",
       "      <td>20.20</td>\n",
       "      <td>17.15</td>\n",
       "      <td>22.85</td>\n",
       "      <td>33.97</td>\n",
       "      <td>8.647614</td>\n",
       "      <td>...</td>\n",
       "      <td>36.786316</td>\n",
       "      <td>4.390857</td>\n",
       "      <td>11.824681</td>\n",
       "      <td>19.311783</td>\n",
       "      <td>27.177267</td>\n",
       "      <td>30.514595</td>\n",
       "      <td>26.478289</td>\n",
       "      <td>31.496145</td>\n",
       "      <td>35.057873</td>\n",
       "      <td>39.932507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.33</td>\n",
       "      <td>24.78</td>\n",
       "      <td>24.52</td>\n",
       "      <td>23.02</td>\n",
       "      <td>28.58</td>\n",
       "      <td>28.55</td>\n",
       "      <td>25.06</td>\n",
       "      <td>26.39</td>\n",
       "      <td>27.24</td>\n",
       "      <td>12.565716</td>\n",
       "      <td>...</td>\n",
       "      <td>35.568935</td>\n",
       "      <td>7.802339</td>\n",
       "      <td>19.814159</td>\n",
       "      <td>26.721149</td>\n",
       "      <td>29.886969</td>\n",
       "      <td>30.452824</td>\n",
       "      <td>26.656958</td>\n",
       "      <td>30.779909</td>\n",
       "      <td>32.709866</td>\n",
       "      <td>34.859814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.26</td>\n",
       "      <td>26.20</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.70</td>\n",
       "      <td>29.22</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.05</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.45</td>\n",
       "      <td>13.355133</td>\n",
       "      <td>...</td>\n",
       "      <td>37.465626</td>\n",
       "      <td>9.388758</td>\n",
       "      <td>21.730694</td>\n",
       "      <td>27.419022</td>\n",
       "      <td>29.096201</td>\n",
       "      <td>29.635658</td>\n",
       "      <td>29.236378</td>\n",
       "      <td>34.296619</td>\n",
       "      <td>36.827160</td>\n",
       "      <td>38.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.96</td>\n",
       "      <td>29.89</td>\n",
       "      <td>33.58</td>\n",
       "      <td>33.28</td>\n",
       "      <td>38.13</td>\n",
       "      <td>35.95</td>\n",
       "      <td>34.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.78</td>\n",
       "      <td>16.214230</td>\n",
       "      <td>...</td>\n",
       "      <td>47.101749</td>\n",
       "      <td>8.203132</td>\n",
       "      <td>21.373383</td>\n",
       "      <td>30.123791</td>\n",
       "      <td>35.451000</td>\n",
       "      <td>37.933434</td>\n",
       "      <td>32.827522</td>\n",
       "      <td>38.408718</td>\n",
       "      <td>41.311008</td>\n",
       "      <td>44.888351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.24</td>\n",
       "      <td>24.45</td>\n",
       "      <td>29.77</td>\n",
       "      <td>29.36</td>\n",
       "      <td>34.56</td>\n",
       "      <td>30.81</td>\n",
       "      <td>26.23</td>\n",
       "      <td>27.65</td>\n",
       "      <td>34.53</td>\n",
       "      <td>15.461583</td>\n",
       "      <td>...</td>\n",
       "      <td>45.864857</td>\n",
       "      <td>6.597206</td>\n",
       "      <td>18.024208</td>\n",
       "      <td>26.291977</td>\n",
       "      <td>32.616802</td>\n",
       "      <td>35.180676</td>\n",
       "      <td>29.430502</td>\n",
       "      <td>34.368755</td>\n",
       "      <td>37.460190</td>\n",
       "      <td>41.908260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.30</td>\n",
       "      <td>25.22</td>\n",
       "      <td>31.45</td>\n",
       "      <td>29.95</td>\n",
       "      <td>34.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>29.10</td>\n",
       "      <td>27.16</td>\n",
       "      <td>31.12</td>\n",
       "      <td>15.829645</td>\n",
       "      <td>...</td>\n",
       "      <td>43.873386</td>\n",
       "      <td>6.399664</td>\n",
       "      <td>17.532560</td>\n",
       "      <td>25.250664</td>\n",
       "      <td>30.665916</td>\n",
       "      <td>32.757206</td>\n",
       "      <td>27.466944</td>\n",
       "      <td>32.340839</td>\n",
       "      <td>35.126392</td>\n",
       "      <td>38.594013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.03</td>\n",
       "      <td>30.02</td>\n",
       "      <td>34.72</td>\n",
       "      <td>32.49</td>\n",
       "      <td>35.12</td>\n",
       "      <td>35.82</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "      <td>15.413681</td>\n",
       "      <td>...</td>\n",
       "      <td>42.702572</td>\n",
       "      <td>7.915727</td>\n",
       "      <td>20.271654</td>\n",
       "      <td>27.582012</td>\n",
       "      <td>31.171041</td>\n",
       "      <td>32.775154</td>\n",
       "      <td>29.562883</td>\n",
       "      <td>35.402729</td>\n",
       "      <td>38.024281</td>\n",
       "      <td>41.027096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.21</td>\n",
       "      <td>27.84</td>\n",
       "      <td>33.51</td>\n",
       "      <td>32.70</td>\n",
       "      <td>37.10</td>\n",
       "      <td>34.75</td>\n",
       "      <td>29.17</td>\n",
       "      <td>27.61</td>\n",
       "      <td>29.70</td>\n",
       "      <td>13.965651</td>\n",
       "      <td>...</td>\n",
       "      <td>41.722382</td>\n",
       "      <td>6.401133</td>\n",
       "      <td>17.403168</td>\n",
       "      <td>24.454617</td>\n",
       "      <td>28.796335</td>\n",
       "      <td>30.479309</td>\n",
       "      <td>26.260344</td>\n",
       "      <td>31.550613</td>\n",
       "      <td>34.133892</td>\n",
       "      <td>36.677464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.50</td>\n",
       "      <td>22.99</td>\n",
       "      <td>30.11</td>\n",
       "      <td>31.82</td>\n",
       "      <td>32.80</td>\n",
       "      <td>33.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>36.02</td>\n",
       "      <td>44.45</td>\n",
       "      <td>14.931929</td>\n",
       "      <td>...</td>\n",
       "      <td>48.921932</td>\n",
       "      <td>4.192406</td>\n",
       "      <td>12.092266</td>\n",
       "      <td>19.799160</td>\n",
       "      <td>27.405876</td>\n",
       "      <td>30.629284</td>\n",
       "      <td>25.642973</td>\n",
       "      <td>31.001484</td>\n",
       "      <td>34.760403</td>\n",
       "      <td>40.601711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.97</td>\n",
       "      <td>21.03</td>\n",
       "      <td>25.88</td>\n",
       "      <td>24.37</td>\n",
       "      <td>28.26</td>\n",
       "      <td>26.10</td>\n",
       "      <td>21.49</td>\n",
       "      <td>20.61</td>\n",
       "      <td>26.21</td>\n",
       "      <td>11.454762</td>\n",
       "      <td>...</td>\n",
       "      <td>37.491634</td>\n",
       "      <td>4.801584</td>\n",
       "      <td>13.604786</td>\n",
       "      <td>20.210182</td>\n",
       "      <td>25.149429</td>\n",
       "      <td>26.669170</td>\n",
       "      <td>21.907263</td>\n",
       "      <td>25.833042</td>\n",
       "      <td>27.993830</td>\n",
       "      <td>29.190224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>22.61</td>\n",
       "      <td>27.12</td>\n",
       "      <td>27.53</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.98</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.92</td>\n",
       "      <td>25.82</td>\n",
       "      <td>11.479879</td>\n",
       "      <td>...</td>\n",
       "      <td>36.406391</td>\n",
       "      <td>5.289308</td>\n",
       "      <td>14.693268</td>\n",
       "      <td>20.902817</td>\n",
       "      <td>24.895205</td>\n",
       "      <td>26.095184</td>\n",
       "      <td>22.234713</td>\n",
       "      <td>26.813074</td>\n",
       "      <td>29.008127</td>\n",
       "      <td>30.395195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.57</td>\n",
       "      <td>17.69</td>\n",
       "      <td>21.43</td>\n",
       "      <td>22.13</td>\n",
       "      <td>22.79</td>\n",
       "      <td>24.23</td>\n",
       "      <td>23.32</td>\n",
       "      <td>21.47</td>\n",
       "      <td>23.67</td>\n",
       "      <td>8.403148</td>\n",
       "      <td>...</td>\n",
       "      <td>29.274141</td>\n",
       "      <td>4.699314</td>\n",
       "      <td>12.997721</td>\n",
       "      <td>17.906013</td>\n",
       "      <td>20.675901</td>\n",
       "      <td>21.210585</td>\n",
       "      <td>19.101952</td>\n",
       "      <td>23.671947</td>\n",
       "      <td>25.667013</td>\n",
       "      <td>26.782272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.19</td>\n",
       "      <td>15.84</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.86</td>\n",
       "      <td>15.93</td>\n",
       "      <td>16.22</td>\n",
       "      <td>15.55</td>\n",
       "      <td>15.47</td>\n",
       "      <td>24.20</td>\n",
       "      <td>8.652231</td>\n",
       "      <td>...</td>\n",
       "      <td>28.678186</td>\n",
       "      <td>3.141488</td>\n",
       "      <td>8.855782</td>\n",
       "      <td>14.510162</td>\n",
       "      <td>19.605568</td>\n",
       "      <td>20.842255</td>\n",
       "      <td>16.485331</td>\n",
       "      <td>18.788458</td>\n",
       "      <td>20.285421</td>\n",
       "      <td>20.439587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.95</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>15.83</td>\n",
       "      <td>5.323413</td>\n",
       "      <td>...</td>\n",
       "      <td>21.094549</td>\n",
       "      <td>1.513478</td>\n",
       "      <td>3.967975</td>\n",
       "      <td>7.886890</td>\n",
       "      <td>13.226873</td>\n",
       "      <td>14.994874</td>\n",
       "      <td>11.546255</td>\n",
       "      <td>12.393466</td>\n",
       "      <td>13.691509</td>\n",
       "      <td>13.802272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>13.80</td>\n",
       "      <td>3.997223</td>\n",
       "      <td>...</td>\n",
       "      <td>18.829851</td>\n",
       "      <td>0.320279</td>\n",
       "      <td>0.721823</td>\n",
       "      <td>1.470101</td>\n",
       "      <td>3.497169</td>\n",
       "      <td>5.759925</td>\n",
       "      <td>7.264647</td>\n",
       "      <td>10.291542</td>\n",
       "      <td>14.022621</td>\n",
       "      <td>20.541195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.013756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035817</td>\n",
       "      <td>-0.003728</td>\n",
       "      <td>0.006942</td>\n",
       "      <td>-0.019830</td>\n",
       "      <td>-0.027979</td>\n",
       "      <td>-0.021775</td>\n",
       "      <td>-0.012140</td>\n",
       "      <td>0.043297</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.212867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.015333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035208</td>\n",
       "      <td>-0.003797</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>-0.018900</td>\n",
       "      <td>-0.025557</td>\n",
       "      <td>-0.018845</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>0.040333</td>\n",
       "      <td>0.057133</td>\n",
       "      <td>0.206152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.012758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>-0.003850</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>-0.017559</td>\n",
       "      <td>-0.026263</td>\n",
       "      <td>-0.018172</td>\n",
       "      <td>-0.007014</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.038302</td>\n",
       "      <td>0.129194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.012845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>-0.003911</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>-0.017063</td>\n",
       "      <td>-0.024696</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>-0.005746</td>\n",
       "      <td>0.032163</td>\n",
       "      <td>0.035495</td>\n",
       "      <td>0.126422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.019962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>-0.003920</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>-0.015840</td>\n",
       "      <td>-0.025478</td>\n",
       "      <td>-0.015570</td>\n",
       "      <td>-0.001750</td>\n",
       "      <td>0.026702</td>\n",
       "      <td>0.020025</td>\n",
       "      <td>0.077067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.019971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001829</td>\n",
       "      <td>-0.003996</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>-0.015451</td>\n",
       "      <td>-0.024012</td>\n",
       "      <td>-0.013960</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.025119</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.075892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.016495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008499</td>\n",
       "      <td>-0.003967</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>-0.014370</td>\n",
       "      <td>-0.024648</td>\n",
       "      <td>-0.012855</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.021158</td>\n",
       "      <td>0.008355</td>\n",
       "      <td>0.053916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.016510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010746</td>\n",
       "      <td>-0.004027</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.014302</td>\n",
       "      <td>-0.023672</td>\n",
       "      <td>-0.011939</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>0.055050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.011314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018126</td>\n",
       "      <td>-0.003955</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.013326</td>\n",
       "      <td>-0.024217</td>\n",
       "      <td>-0.010593</td>\n",
       "      <td>0.010730</td>\n",
       "      <td>0.019082</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.054457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.011205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020939</td>\n",
       "      <td>-0.004072</td>\n",
       "      <td>-0.002385</td>\n",
       "      <td>-0.013189</td>\n",
       "      <td>-0.022772</td>\n",
       "      <td>-0.009213</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>0.019075</td>\n",
       "      <td>0.006591</td>\n",
       "      <td>0.056564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.012792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025254</td>\n",
       "      <td>-0.003984</td>\n",
       "      <td>-0.004031</td>\n",
       "      <td>-0.012468</td>\n",
       "      <td>-0.022902</td>\n",
       "      <td>-0.007419</td>\n",
       "      <td>0.019041</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>0.014081</td>\n",
       "      <td>0.072170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.012685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025642</td>\n",
       "      <td>-0.003995</td>\n",
       "      <td>-0.004104</td>\n",
       "      <td>-0.012511</td>\n",
       "      <td>-0.022759</td>\n",
       "      <td>-0.007331</td>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.021788</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.072598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.017611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022553</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>-0.023460</td>\n",
       "      <td>-0.005852</td>\n",
       "      <td>0.025252</td>\n",
       "      <td>0.026335</td>\n",
       "      <td>0.026427</td>\n",
       "      <td>0.093083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.017611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022553</td>\n",
       "      <td>-0.003762</td>\n",
       "      <td>-0.005146</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>-0.023460</td>\n",
       "      <td>-0.005852</td>\n",
       "      <td>0.025252</td>\n",
       "      <td>0.026335</td>\n",
       "      <td>0.026427</td>\n",
       "      <td>0.093083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L10.1  L10.2  L10.3  L10.4  L10.5  L10.6  L10.7  L10.8  L10.9      D10.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.012377   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.011908   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.010526   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.010701   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.011175   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.010739   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.011099   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.009779   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.009821   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.011221   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.012145   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.012509   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.010746   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.011147   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.007546   \n",
       "15   0.48   0.70   1.80   2.18   0.98   3.56   2.99   6.80   9.87   0.639051   \n",
       "16   2.67   7.45   8.62  10.08  10.06  11.52  17.00  19.79  20.92   1.814343   \n",
       "17   3.80   8.55   8.15  10.49  10.84  12.03  15.05  18.24  25.56   3.588575   \n",
       "18   7.74  14.78  19.70  18.93  22.60  20.57  16.19  15.66  32.96   7.595252   \n",
       "19   9.47  19.06  21.52  21.87  23.62  20.20  17.15  22.85  33.97   8.647614   \n",
       "20  14.33  24.78  24.52  23.02  28.58  28.55  25.06  26.39  27.24  12.565716   \n",
       "21  15.26  26.20  24.57  24.70  29.22  31.11  31.05  31.59  32.45  13.355133   \n",
       "22  15.96  29.89  33.58  33.28  38.13  35.95  34.24  33.24  33.78  16.214230   \n",
       "23  14.24  24.45  29.77  29.36  34.56  30.81  26.23  27.65  34.53  15.461583   \n",
       "24  14.30  25.22  31.45  29.95  34.70  31.31  29.10  27.16  31.12  15.829645   \n",
       "25  16.03  30.02  34.72  32.49  35.12  35.82  33.60  32.73  34.11  15.413681   \n",
       "26  15.21  27.84  33.51  32.70  37.10  34.75  29.17  27.61  29.70  13.965651   \n",
       "27   9.50  22.99  30.11  31.82  32.80  33.88  19.43  36.02  44.45  14.931929   \n",
       "28  10.97  21.03  25.88  24.37  28.26  26.10  21.49  20.61  26.21  11.454762   \n",
       "29  12.63  22.61  27.12  27.53  31.02  29.98  27.37  22.92  25.82  11.479879   \n",
       "30   8.57  17.69  21.43  22.13  22.79  24.23  23.32  21.47  23.67   8.403148   \n",
       "31   7.19  15.84  18.83  17.86  15.93  16.22  15.55  15.47  24.20   8.652231   \n",
       "32   2.95   8.16   8.37   7.24   6.39   6.81   6.33   6.51  15.83   5.323413   \n",
       "33   1.71   4.94   4.62   3.24   3.42   1.19   3.41  -0.50  13.80   3.997223   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17  -0.013756   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17  -0.015333   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.012758   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.012845   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.019962   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.019971   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.016495   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.016510   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.011314   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.011205   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.012792   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.012685   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.017611   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.017611   \n",
       "\n",
       "    ...      G10.9     M10.1      M10.2      M10.3      M10.4      M10.5  \\\n",
       "0   ...   0.007649 -0.004459  -0.008705  -0.016331  -0.012873   0.011366   \n",
       "1   ...   0.007372 -0.004180  -0.008579  -0.016586  -0.012574   0.011626   \n",
       "2   ...   0.005666 -0.004035  -0.008178  -0.017271  -0.010843   0.011422   \n",
       "3   ...   0.005790 -0.004286  -0.008284  -0.016949  -0.011166   0.011336   \n",
       "4   ...   0.004737 -0.004504  -0.008215  -0.017544  -0.010111   0.010687   \n",
       "5   ...   0.004659 -0.003969  -0.007754  -0.017774  -0.009390   0.011333   \n",
       "6   ...   0.004653 -0.003969  -0.007321  -0.018142  -0.008168   0.011212   \n",
       "7   ...   0.005117 -0.002918  -0.006726  -0.019038  -0.006386   0.011956   \n",
       "8   ...   0.007539 -0.002470  -0.005955  -0.020004  -0.004453   0.011636   \n",
       "9   ...   0.008529 -0.002085  -0.006459  -0.020982  -0.004370   0.010595   \n",
       "10  ...   0.014435 -0.001978  -0.005754  -0.022295  -0.004056   0.008588   \n",
       "11  ...   0.014221 -0.001947  -0.006076  -0.022328  -0.004110   0.008468   \n",
       "12  ...   0.032387 -0.002020  -0.004498  -0.022562  -0.004282   0.007034   \n",
       "13  ...   0.033276 -0.001987  -0.004810  -0.022639  -0.004355   0.006829   \n",
       "14  ...   0.099238 -0.002032  -0.002895  -0.022807  -0.005288   0.004632   \n",
       "15  ...  11.893352  0.234644   0.463549   0.761069   1.563330   2.646295   \n",
       "16  ...  25.218607  1.068583   2.409046   4.675882   9.359540  13.214917   \n",
       "17  ...  29.591112  2.043161   5.012439   9.585613  16.715616  20.855846   \n",
       "18  ...  41.306126  3.095799   8.164346  14.750845  23.068312  27.154726   \n",
       "19  ...  36.786316  4.390857  11.824681  19.311783  27.177267  30.514595   \n",
       "20  ...  35.568935  7.802339  19.814159  26.721149  29.886969  30.452824   \n",
       "21  ...  37.465626  9.388758  21.730694  27.419022  29.096201  29.635658   \n",
       "22  ...  47.101749  8.203132  21.373383  30.123791  35.451000  37.933434   \n",
       "23  ...  45.864857  6.597206  18.024208  26.291977  32.616802  35.180676   \n",
       "24  ...  43.873386  6.399664  17.532560  25.250664  30.665916  32.757206   \n",
       "25  ...  42.702572  7.915727  20.271654  27.582012  31.171041  32.775154   \n",
       "26  ...  41.722382  6.401133  17.403168  24.454617  28.796335  30.479309   \n",
       "27  ...  48.921932  4.192406  12.092266  19.799160  27.405876  30.629284   \n",
       "28  ...  37.491634  4.801584  13.604786  20.210182  25.149429  26.669170   \n",
       "29  ...  36.406391  5.289308  14.693268  20.902817  24.895205  26.095184   \n",
       "30  ...  29.274141  4.699314  12.997721  17.906013  20.675901  21.210585   \n",
       "31  ...  28.678186  3.141488   8.855782  14.510162  19.605568  20.842255   \n",
       "32  ...  21.094549  1.513478   3.967975   7.886890  13.226873  14.994874   \n",
       "33  ...  18.829851  0.320279   0.721823   1.470101   3.497169   5.759925   \n",
       "34  ...   0.035817 -0.003728   0.006942  -0.019830  -0.027979  -0.021775   \n",
       "35  ...   0.035208 -0.003797   0.005865  -0.018900  -0.025557  -0.018845   \n",
       "36  ...   0.008165 -0.003850   0.004960  -0.017559  -0.026263  -0.018172   \n",
       "37  ...   0.008151 -0.003911   0.004112  -0.017063  -0.024696  -0.016378   \n",
       "38  ...  -0.000652 -0.003920   0.002992  -0.015840  -0.025478  -0.015570   \n",
       "39  ...  -0.001829 -0.003996   0.002086  -0.015451  -0.024012  -0.013960   \n",
       "40  ...  -0.008499 -0.003967   0.000721  -0.014370  -0.024648  -0.012855   \n",
       "41  ...  -0.010746 -0.004027   0.000092  -0.014302  -0.023672  -0.011939   \n",
       "42  ...  -0.018126 -0.003955  -0.001375  -0.013326  -0.024217  -0.010593   \n",
       "43  ...  -0.020939 -0.004072  -0.002385  -0.013189  -0.022772  -0.009213   \n",
       "44  ...  -0.025254 -0.003984  -0.004031  -0.012468  -0.022902  -0.007419   \n",
       "45  ...  -0.025642 -0.003995  -0.004104  -0.012511  -0.022759  -0.007331   \n",
       "46  ...  -0.022553 -0.003762  -0.005146  -0.012048  -0.023460  -0.005852   \n",
       "47  ...  -0.022553 -0.003762  -0.005146  -0.012048  -0.023460  -0.005852   \n",
       "\n",
       "        M10.6      M10.7      M10.8      M10.9  \n",
       "0    0.014229   0.009576   0.031540   0.039548  \n",
       "1    0.013720   0.010237   0.032769   0.040871  \n",
       "2    0.008403   0.004604   0.022989   0.034962  \n",
       "3    0.009346   0.004492   0.022585   0.034200  \n",
       "4    0.005679   0.003218   0.019797   0.035764  \n",
       "5    0.004812   0.003572   0.020536   0.036747  \n",
       "6    0.002407   0.005245   0.022477   0.042794  \n",
       "7   -0.000580   0.004924   0.022463   0.043593  \n",
       "8   -0.003809   0.006939   0.024896   0.050150  \n",
       "9   -0.005873   0.005827   0.023954   0.049952  \n",
       "10  -0.008251   0.007813   0.026255   0.057399  \n",
       "11  -0.008483   0.007253   0.025890   0.056975  \n",
       "12  -0.008522   0.011312   0.031193   0.075394  \n",
       "13  -0.008912   0.010515   0.030508   0.073933  \n",
       "14  -0.008221   0.017289   0.040208   0.116890  \n",
       "15   4.231714   6.971114   9.692414  13.553650  \n",
       "16  14.007280  18.917479  24.398327  33.296726  \n",
       "17  20.123676  26.380701  31.559332  38.425480  \n",
       "18  25.184723  32.590385  37.731945  45.355114  \n",
       "19  26.478289  31.496145  35.057873  39.932507  \n",
       "20  26.656958  30.779909  32.709866  34.859814  \n",
       "21  29.236378  34.296619  36.827160  38.316700  \n",
       "22  32.827522  38.408718  41.311008  44.888351  \n",
       "23  29.430502  34.368755  37.460190  41.908260  \n",
       "24  27.466944  32.340839  35.126392  38.594013  \n",
       "25  29.562883  35.402729  38.024281  41.027096  \n",
       "26  26.260344  31.550613  34.133892  36.677464  \n",
       "27  25.642973  31.001484  34.760403  40.601711  \n",
       "28  21.907263  25.833042  27.993830  29.190224  \n",
       "29  22.234713  26.813074  29.008127  30.395195  \n",
       "30  19.101952  23.671947  25.667013  26.782272  \n",
       "31  16.485331  18.788458  20.285421  20.439587  \n",
       "32  11.546255  12.393466  13.691509  13.802272  \n",
       "33   7.264647  10.291542  14.022621  20.541195  \n",
       "34  -0.012140   0.043297   0.061839   0.212867  \n",
       "35  -0.010223   0.040333   0.057133   0.206152  \n",
       "36  -0.007014   0.034137   0.038302   0.129194  \n",
       "37  -0.005746   0.032163   0.035495   0.126422  \n",
       "38  -0.001750   0.026702   0.020025   0.077067  \n",
       "39  -0.000426   0.025119   0.018000   0.075892  \n",
       "40   0.004374   0.021158   0.008355   0.053916  \n",
       "41   0.005202   0.020561   0.008047   0.055050  \n",
       "42   0.010730   0.019082   0.005911   0.054457  \n",
       "43   0.012431   0.019075   0.006591   0.056564  \n",
       "44   0.019041   0.021564   0.014081   0.072170  \n",
       "45   0.019190   0.021788   0.014407   0.072598  \n",
       "46   0.025252   0.026335   0.026427   0.093083  \n",
       "47   0.025252   0.026335   0.026427   0.093083  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0.loc[res_0[res_0['L00.1'] == 0].index, ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9'\n",
    "                                            ,'C00.1','C00.2','C00.3','C00.4','C00.5','C00.6','C00.7','C00.8','C00.9'\n",
    "                                            ,'G00.1','G00.2','G00.3','G00.4','G00.5','G00.6','G00.7','G00.8','G00.9'\n",
    "                                            ,'M00.1','M00.2','M00.3','M00.4','M00.5','M00.6','M00.7','M00.8','M00.9'\n",
    "                                            ]] = 0\n",
    "res_1.loc[res_1[res_1['L10.1'] == 0].index, ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9'\n",
    "                                            ,'C10.1','C10.2','C10.3','C10.4','C10.5','C10.6','C10.7','C10.8','C10.9'\n",
    "                                            ,'G10.1','G10.2','G10.3','G10.4','G10.5','G10.6','G10.7','G10.8','G10.9'\n",
    "                                            ,'M10.1','M10.2','M10.3','M10.4','M10.5','M10.6','M10.7','M10.8','M10.9'\n",
    "                                            ]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L00.1</th>\n",
       "      <th>L00.2</th>\n",
       "      <th>L00.3</th>\n",
       "      <th>L00.4</th>\n",
       "      <th>L00.5</th>\n",
       "      <th>L00.6</th>\n",
       "      <th>L00.7</th>\n",
       "      <th>L00.8</th>\n",
       "      <th>L00.9</th>\n",
       "      <th>D00.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G00.9</th>\n",
       "      <th>M00.1</th>\n",
       "      <th>M00.2</th>\n",
       "      <th>M00.3</th>\n",
       "      <th>M00.4</th>\n",
       "      <th>M00.5</th>\n",
       "      <th>M00.6</th>\n",
       "      <th>M00.7</th>\n",
       "      <th>M00.8</th>\n",
       "      <th>M00.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "      <td>0.921596</td>\n",
       "      <td>...</td>\n",
       "      <td>8.961095</td>\n",
       "      <td>0.273712</td>\n",
       "      <td>0.579337</td>\n",
       "      <td>0.847134</td>\n",
       "      <td>1.429537</td>\n",
       "      <td>2.459546</td>\n",
       "      <td>4.434146</td>\n",
       "      <td>5.853877</td>\n",
       "      <td>8.413433</td>\n",
       "      <td>13.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "      <td>1.866856</td>\n",
       "      <td>...</td>\n",
       "      <td>21.688187</td>\n",
       "      <td>1.359357</td>\n",
       "      <td>3.117102</td>\n",
       "      <td>4.482175</td>\n",
       "      <td>7.221376</td>\n",
       "      <td>10.993061</td>\n",
       "      <td>15.369520</td>\n",
       "      <td>17.760300</td>\n",
       "      <td>25.476889</td>\n",
       "      <td>36.946301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "      <td>2.941126</td>\n",
       "      <td>...</td>\n",
       "      <td>28.554996</td>\n",
       "      <td>2.629808</td>\n",
       "      <td>6.297938</td>\n",
       "      <td>8.574986</td>\n",
       "      <td>12.396621</td>\n",
       "      <td>16.806257</td>\n",
       "      <td>21.061373</td>\n",
       "      <td>24.383640</td>\n",
       "      <td>33.391483</td>\n",
       "      <td>45.629196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "      <td>6.280856</td>\n",
       "      <td>...</td>\n",
       "      <td>43.742489</td>\n",
       "      <td>4.027555</td>\n",
       "      <td>10.051889</td>\n",
       "      <td>12.806709</td>\n",
       "      <td>16.891878</td>\n",
       "      <td>21.427212</td>\n",
       "      <td>25.463512</td>\n",
       "      <td>29.728369</td>\n",
       "      <td>39.646595</td>\n",
       "      <td>52.515381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "      <td>7.377127</td>\n",
       "      <td>...</td>\n",
       "      <td>41.340168</td>\n",
       "      <td>5.634828</td>\n",
       "      <td>13.959082</td>\n",
       "      <td>16.527153</td>\n",
       "      <td>20.266357</td>\n",
       "      <td>24.226301</td>\n",
       "      <td>27.340965</td>\n",
       "      <td>30.688051</td>\n",
       "      <td>39.474838</td>\n",
       "      <td>51.019337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "      <td>11.317371</td>\n",
       "      <td>...</td>\n",
       "      <td>39.558636</td>\n",
       "      <td>9.294259</td>\n",
       "      <td>21.544348</td>\n",
       "      <td>22.042635</td>\n",
       "      <td>23.890402</td>\n",
       "      <td>26.176407</td>\n",
       "      <td>27.946430</td>\n",
       "      <td>31.602259</td>\n",
       "      <td>34.815697</td>\n",
       "      <td>38.098289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "      <td>12.153330</td>\n",
       "      <td>...</td>\n",
       "      <td>39.862083</td>\n",
       "      <td>10.555115</td>\n",
       "      <td>23.344545</td>\n",
       "      <td>22.718353</td>\n",
       "      <td>23.974604</td>\n",
       "      <td>27.213102</td>\n",
       "      <td>29.650118</td>\n",
       "      <td>34.550789</td>\n",
       "      <td>36.576504</td>\n",
       "      <td>38.338005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "      <td>14.725376</td>\n",
       "      <td>...</td>\n",
       "      <td>52.656006</td>\n",
       "      <td>9.744592</td>\n",
       "      <td>23.326319</td>\n",
       "      <td>24.893036</td>\n",
       "      <td>28.041769</td>\n",
       "      <td>30.543203</td>\n",
       "      <td>33.171665</td>\n",
       "      <td>38.021835</td>\n",
       "      <td>45.816437</td>\n",
       "      <td>53.586349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "      <td>14.160217</td>\n",
       "      <td>...</td>\n",
       "      <td>51.620369</td>\n",
       "      <td>8.270817</td>\n",
       "      <td>20.399950</td>\n",
       "      <td>22.187107</td>\n",
       "      <td>25.267614</td>\n",
       "      <td>28.074375</td>\n",
       "      <td>30.464939</td>\n",
       "      <td>34.652565</td>\n",
       "      <td>42.893795</td>\n",
       "      <td>52.498535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "      <td>14.352590</td>\n",
       "      <td>...</td>\n",
       "      <td>47.399540</td>\n",
       "      <td>8.091259</td>\n",
       "      <td>20.220682</td>\n",
       "      <td>21.496767</td>\n",
       "      <td>24.042891</td>\n",
       "      <td>26.663202</td>\n",
       "      <td>28.833647</td>\n",
       "      <td>32.890366</td>\n",
       "      <td>39.819202</td>\n",
       "      <td>47.987965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "      <td>13.911210</td>\n",
       "      <td>...</td>\n",
       "      <td>45.579853</td>\n",
       "      <td>9.405066</td>\n",
       "      <td>22.633169</td>\n",
       "      <td>23.153381</td>\n",
       "      <td>25.253382</td>\n",
       "      <td>27.913637</td>\n",
       "      <td>30.344498</td>\n",
       "      <td>35.113995</td>\n",
       "      <td>40.123653</td>\n",
       "      <td>45.054546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "      <td>12.595205</td>\n",
       "      <td>...</td>\n",
       "      <td>42.602364</td>\n",
       "      <td>8.068606</td>\n",
       "      <td>20.320787</td>\n",
       "      <td>21.009787</td>\n",
       "      <td>23.008614</td>\n",
       "      <td>25.579903</td>\n",
       "      <td>27.675587</td>\n",
       "      <td>31.922415</td>\n",
       "      <td>37.531811</td>\n",
       "      <td>43.983574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "      <td>13.617439</td>\n",
       "      <td>...</td>\n",
       "      <td>52.365910</td>\n",
       "      <td>5.621737</td>\n",
       "      <td>14.933422</td>\n",
       "      <td>16.959221</td>\n",
       "      <td>20.051760</td>\n",
       "      <td>23.654943</td>\n",
       "      <td>26.390972</td>\n",
       "      <td>30.416721</td>\n",
       "      <td>39.653301</td>\n",
       "      <td>52.045959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "      <td>10.133067</td>\n",
       "      <td>...</td>\n",
       "      <td>36.880199</td>\n",
       "      <td>6.378386</td>\n",
       "      <td>16.757147</td>\n",
       "      <td>17.597227</td>\n",
       "      <td>19.462786</td>\n",
       "      <td>22.070230</td>\n",
       "      <td>23.784245</td>\n",
       "      <td>27.088703</td>\n",
       "      <td>32.239811</td>\n",
       "      <td>39.116005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "      <td>10.151043</td>\n",
       "      <td>...</td>\n",
       "      <td>35.761631</td>\n",
       "      <td>6.854388</td>\n",
       "      <td>17.689793</td>\n",
       "      <td>18.050587</td>\n",
       "      <td>19.585070</td>\n",
       "      <td>22.130251</td>\n",
       "      <td>23.855522</td>\n",
       "      <td>27.569181</td>\n",
       "      <td>31.903282</td>\n",
       "      <td>37.366753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "      <td>6.971280</td>\n",
       "      <td>...</td>\n",
       "      <td>29.087336</td>\n",
       "      <td>6.258477</td>\n",
       "      <td>16.409973</td>\n",
       "      <td>16.251860</td>\n",
       "      <td>17.124327</td>\n",
       "      <td>19.470133</td>\n",
       "      <td>20.855913</td>\n",
       "      <td>24.356153</td>\n",
       "      <td>26.837904</td>\n",
       "      <td>29.940657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "      <td>7.210488</td>\n",
       "      <td>...</td>\n",
       "      <td>28.108219</td>\n",
       "      <td>4.302255</td>\n",
       "      <td>11.597221</td>\n",
       "      <td>12.626295</td>\n",
       "      <td>14.420085</td>\n",
       "      <td>17.028147</td>\n",
       "      <td>18.435785</td>\n",
       "      <td>20.569262</td>\n",
       "      <td>24.454823</td>\n",
       "      <td>30.254778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "      <td>3.828187</td>\n",
       "      <td>...</td>\n",
       "      <td>18.057730</td>\n",
       "      <td>2.116210</td>\n",
       "      <td>5.766467</td>\n",
       "      <td>7.318921</td>\n",
       "      <td>9.462689</td>\n",
       "      <td>12.242281</td>\n",
       "      <td>13.908647</td>\n",
       "      <td>14.590513</td>\n",
       "      <td>18.296593</td>\n",
       "      <td>24.047079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "      <td>2.870463</td>\n",
       "      <td>...</td>\n",
       "      <td>14.699933</td>\n",
       "      <td>0.430942</td>\n",
       "      <td>1.111121</td>\n",
       "      <td>1.766121</td>\n",
       "      <td>3.076553</td>\n",
       "      <td>5.438250</td>\n",
       "      <td>9.089475</td>\n",
       "      <td>10.973436</td>\n",
       "      <td>15.452991</td>\n",
       "      <td>23.168827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L00.1  L00.2  L00.3  L00.4  L00.5  L00.6  L00.7  L00.8  L00.9      D00.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45   0.921596   \n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51   1.866856   \n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11   2.941126   \n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34   6.280856   \n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51   7.377127   \n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17  11.317371   \n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03  12.153330   \n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17  14.725376   \n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82  14.160217   \n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14  14.352590   \n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61  13.911210   \n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92  12.595205   \n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20  13.617439   \n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73  10.133067   \n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94  10.151043   \n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20   6.971280   \n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59   7.210488   \n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49   3.828187   \n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13   2.870463   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "\n",
       "    ...      G00.9      M00.1      M00.2      M00.3      M00.4      M00.5  \\\n",
       "0   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15  ...   8.961095   0.273712   0.579337   0.847134   1.429537   2.459546   \n",
       "16  ...  21.688187   1.359357   3.117102   4.482175   7.221376  10.993061   \n",
       "17  ...  28.554996   2.629808   6.297938   8.574986  12.396621  16.806257   \n",
       "18  ...  43.742489   4.027555  10.051889  12.806709  16.891878  21.427212   \n",
       "19  ...  41.340168   5.634828  13.959082  16.527153  20.266357  24.226301   \n",
       "20  ...  39.558636   9.294259  21.544348  22.042635  23.890402  26.176407   \n",
       "21  ...  39.862083  10.555115  23.344545  22.718353  23.974604  27.213102   \n",
       "22  ...  52.656006   9.744592  23.326319  24.893036  28.041769  30.543203   \n",
       "23  ...  51.620369   8.270817  20.399950  22.187107  25.267614  28.074375   \n",
       "24  ...  47.399540   8.091259  20.220682  21.496767  24.042891  26.663202   \n",
       "25  ...  45.579853   9.405066  22.633169  23.153381  25.253382  27.913637   \n",
       "26  ...  42.602364   8.068606  20.320787  21.009787  23.008614  25.579903   \n",
       "27  ...  52.365910   5.621737  14.933422  16.959221  20.051760  23.654943   \n",
       "28  ...  36.880199   6.378386  16.757147  17.597227  19.462786  22.070230   \n",
       "29  ...  35.761631   6.854388  17.689793  18.050587  19.585070  22.130251   \n",
       "30  ...  29.087336   6.258477  16.409973  16.251860  17.124327  19.470133   \n",
       "31  ...  28.108219   4.302255  11.597221  12.626295  14.420085  17.028147   \n",
       "32  ...  18.057730   2.116210   5.766467   7.318921   9.462689  12.242281   \n",
       "33  ...  14.699933   0.430942   1.111121   1.766121   3.076553   5.438250   \n",
       "34  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        M00.6      M00.7      M00.8      M00.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   4.434146   5.853877   8.413433  13.160100  \n",
       "16  15.369520  17.760300  25.476889  36.946301  \n",
       "17  21.061373  24.383640  33.391483  45.629196  \n",
       "18  25.463512  29.728369  39.646595  52.515381  \n",
       "19  27.340965  30.688051  39.474838  51.019337  \n",
       "20  27.946430  31.602259  34.815697  38.098289  \n",
       "21  29.650118  34.550789  36.576504  38.338005  \n",
       "22  33.171665  38.021835  45.816437  53.586349  \n",
       "23  30.464939  34.652565  42.893795  52.498535  \n",
       "24  28.833647  32.890366  39.819202  47.987965  \n",
       "25  30.344498  35.113995  40.123653  45.054546  \n",
       "26  27.675587  31.922415  37.531811  43.983574  \n",
       "27  26.390972  30.416721  39.653301  52.045959  \n",
       "28  23.784245  27.088703  32.239811  39.116005  \n",
       "29  23.855522  27.569181  31.903282  37.366753  \n",
       "30  20.855913  24.356153  26.837904  29.940657  \n",
       "31  18.435785  20.569262  24.454823  30.254778  \n",
       "32  13.908647  14.590513  18.296593  24.047079  \n",
       "33   9.089475  10.973436  15.452991  23.168827  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_0[:48]#.to_csv('0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L10.1</th>\n",
       "      <th>L10.2</th>\n",
       "      <th>L10.3</th>\n",
       "      <th>L10.4</th>\n",
       "      <th>L10.5</th>\n",
       "      <th>L10.6</th>\n",
       "      <th>L10.7</th>\n",
       "      <th>L10.8</th>\n",
       "      <th>L10.9</th>\n",
       "      <th>D10.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G10.9</th>\n",
       "      <th>M10.1</th>\n",
       "      <th>M10.2</th>\n",
       "      <th>M10.3</th>\n",
       "      <th>M10.4</th>\n",
       "      <th>M10.5</th>\n",
       "      <th>M10.6</th>\n",
       "      <th>M10.7</th>\n",
       "      <th>M10.8</th>\n",
       "      <th>M10.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.87</td>\n",
       "      <td>0.639051</td>\n",
       "      <td>...</td>\n",
       "      <td>11.893352</td>\n",
       "      <td>0.234644</td>\n",
       "      <td>0.463549</td>\n",
       "      <td>0.761069</td>\n",
       "      <td>1.563330</td>\n",
       "      <td>2.646295</td>\n",
       "      <td>4.231714</td>\n",
       "      <td>6.971114</td>\n",
       "      <td>9.692414</td>\n",
       "      <td>13.553650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.92</td>\n",
       "      <td>1.814343</td>\n",
       "      <td>...</td>\n",
       "      <td>25.218607</td>\n",
       "      <td>1.068583</td>\n",
       "      <td>2.409046</td>\n",
       "      <td>4.675882</td>\n",
       "      <td>9.359540</td>\n",
       "      <td>13.214917</td>\n",
       "      <td>14.007280</td>\n",
       "      <td>18.917479</td>\n",
       "      <td>24.398327</td>\n",
       "      <td>33.296726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>18.24</td>\n",
       "      <td>25.56</td>\n",
       "      <td>3.588575</td>\n",
       "      <td>...</td>\n",
       "      <td>29.591112</td>\n",
       "      <td>2.043161</td>\n",
       "      <td>5.012439</td>\n",
       "      <td>9.585613</td>\n",
       "      <td>16.715616</td>\n",
       "      <td>20.855846</td>\n",
       "      <td>20.123676</td>\n",
       "      <td>26.380701</td>\n",
       "      <td>31.559332</td>\n",
       "      <td>38.425480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.74</td>\n",
       "      <td>14.78</td>\n",
       "      <td>19.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>22.60</td>\n",
       "      <td>20.57</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.66</td>\n",
       "      <td>32.96</td>\n",
       "      <td>7.595252</td>\n",
       "      <td>...</td>\n",
       "      <td>41.306126</td>\n",
       "      <td>3.095799</td>\n",
       "      <td>8.164346</td>\n",
       "      <td>14.750845</td>\n",
       "      <td>23.068312</td>\n",
       "      <td>27.154726</td>\n",
       "      <td>25.184723</td>\n",
       "      <td>32.590385</td>\n",
       "      <td>37.731945</td>\n",
       "      <td>45.355114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.47</td>\n",
       "      <td>19.06</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.62</td>\n",
       "      <td>20.20</td>\n",
       "      <td>17.15</td>\n",
       "      <td>22.85</td>\n",
       "      <td>33.97</td>\n",
       "      <td>8.647614</td>\n",
       "      <td>...</td>\n",
       "      <td>36.786316</td>\n",
       "      <td>4.390857</td>\n",
       "      <td>11.824681</td>\n",
       "      <td>19.311783</td>\n",
       "      <td>27.177267</td>\n",
       "      <td>30.514595</td>\n",
       "      <td>26.478289</td>\n",
       "      <td>31.496145</td>\n",
       "      <td>35.057873</td>\n",
       "      <td>39.932507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.33</td>\n",
       "      <td>24.78</td>\n",
       "      <td>24.52</td>\n",
       "      <td>23.02</td>\n",
       "      <td>28.58</td>\n",
       "      <td>28.55</td>\n",
       "      <td>25.06</td>\n",
       "      <td>26.39</td>\n",
       "      <td>27.24</td>\n",
       "      <td>12.565716</td>\n",
       "      <td>...</td>\n",
       "      <td>35.568935</td>\n",
       "      <td>7.802339</td>\n",
       "      <td>19.814159</td>\n",
       "      <td>26.721149</td>\n",
       "      <td>29.886969</td>\n",
       "      <td>30.452824</td>\n",
       "      <td>26.656958</td>\n",
       "      <td>30.779909</td>\n",
       "      <td>32.709866</td>\n",
       "      <td>34.859814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.26</td>\n",
       "      <td>26.20</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.70</td>\n",
       "      <td>29.22</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.05</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.45</td>\n",
       "      <td>13.355133</td>\n",
       "      <td>...</td>\n",
       "      <td>37.465626</td>\n",
       "      <td>9.388758</td>\n",
       "      <td>21.730694</td>\n",
       "      <td>27.419022</td>\n",
       "      <td>29.096201</td>\n",
       "      <td>29.635658</td>\n",
       "      <td>29.236378</td>\n",
       "      <td>34.296619</td>\n",
       "      <td>36.827160</td>\n",
       "      <td>38.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.96</td>\n",
       "      <td>29.89</td>\n",
       "      <td>33.58</td>\n",
       "      <td>33.28</td>\n",
       "      <td>38.13</td>\n",
       "      <td>35.95</td>\n",
       "      <td>34.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.78</td>\n",
       "      <td>16.214230</td>\n",
       "      <td>...</td>\n",
       "      <td>47.101749</td>\n",
       "      <td>8.203132</td>\n",
       "      <td>21.373383</td>\n",
       "      <td>30.123791</td>\n",
       "      <td>35.451000</td>\n",
       "      <td>37.933434</td>\n",
       "      <td>32.827522</td>\n",
       "      <td>38.408718</td>\n",
       "      <td>41.311008</td>\n",
       "      <td>44.888351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.24</td>\n",
       "      <td>24.45</td>\n",
       "      <td>29.77</td>\n",
       "      <td>29.36</td>\n",
       "      <td>34.56</td>\n",
       "      <td>30.81</td>\n",
       "      <td>26.23</td>\n",
       "      <td>27.65</td>\n",
       "      <td>34.53</td>\n",
       "      <td>15.461583</td>\n",
       "      <td>...</td>\n",
       "      <td>45.864857</td>\n",
       "      <td>6.597206</td>\n",
       "      <td>18.024208</td>\n",
       "      <td>26.291977</td>\n",
       "      <td>32.616802</td>\n",
       "      <td>35.180676</td>\n",
       "      <td>29.430502</td>\n",
       "      <td>34.368755</td>\n",
       "      <td>37.460190</td>\n",
       "      <td>41.908260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.30</td>\n",
       "      <td>25.22</td>\n",
       "      <td>31.45</td>\n",
       "      <td>29.95</td>\n",
       "      <td>34.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>29.10</td>\n",
       "      <td>27.16</td>\n",
       "      <td>31.12</td>\n",
       "      <td>15.829645</td>\n",
       "      <td>...</td>\n",
       "      <td>43.873386</td>\n",
       "      <td>6.399664</td>\n",
       "      <td>17.532560</td>\n",
       "      <td>25.250664</td>\n",
       "      <td>30.665916</td>\n",
       "      <td>32.757206</td>\n",
       "      <td>27.466944</td>\n",
       "      <td>32.340839</td>\n",
       "      <td>35.126392</td>\n",
       "      <td>38.594013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.03</td>\n",
       "      <td>30.02</td>\n",
       "      <td>34.72</td>\n",
       "      <td>32.49</td>\n",
       "      <td>35.12</td>\n",
       "      <td>35.82</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "      <td>15.413681</td>\n",
       "      <td>...</td>\n",
       "      <td>42.702572</td>\n",
       "      <td>7.915727</td>\n",
       "      <td>20.271654</td>\n",
       "      <td>27.582012</td>\n",
       "      <td>31.171041</td>\n",
       "      <td>32.775154</td>\n",
       "      <td>29.562883</td>\n",
       "      <td>35.402729</td>\n",
       "      <td>38.024281</td>\n",
       "      <td>41.027096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.21</td>\n",
       "      <td>27.84</td>\n",
       "      <td>33.51</td>\n",
       "      <td>32.70</td>\n",
       "      <td>37.10</td>\n",
       "      <td>34.75</td>\n",
       "      <td>29.17</td>\n",
       "      <td>27.61</td>\n",
       "      <td>29.70</td>\n",
       "      <td>13.965651</td>\n",
       "      <td>...</td>\n",
       "      <td>41.722382</td>\n",
       "      <td>6.401133</td>\n",
       "      <td>17.403168</td>\n",
       "      <td>24.454617</td>\n",
       "      <td>28.796335</td>\n",
       "      <td>30.479309</td>\n",
       "      <td>26.260344</td>\n",
       "      <td>31.550613</td>\n",
       "      <td>34.133892</td>\n",
       "      <td>36.677464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.50</td>\n",
       "      <td>22.99</td>\n",
       "      <td>30.11</td>\n",
       "      <td>31.82</td>\n",
       "      <td>32.80</td>\n",
       "      <td>33.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>36.02</td>\n",
       "      <td>44.45</td>\n",
       "      <td>14.931929</td>\n",
       "      <td>...</td>\n",
       "      <td>48.921932</td>\n",
       "      <td>4.192406</td>\n",
       "      <td>12.092266</td>\n",
       "      <td>19.799160</td>\n",
       "      <td>27.405876</td>\n",
       "      <td>30.629284</td>\n",
       "      <td>25.642973</td>\n",
       "      <td>31.001484</td>\n",
       "      <td>34.760403</td>\n",
       "      <td>40.601711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.97</td>\n",
       "      <td>21.03</td>\n",
       "      <td>25.88</td>\n",
       "      <td>24.37</td>\n",
       "      <td>28.26</td>\n",
       "      <td>26.10</td>\n",
       "      <td>21.49</td>\n",
       "      <td>20.61</td>\n",
       "      <td>26.21</td>\n",
       "      <td>11.454762</td>\n",
       "      <td>...</td>\n",
       "      <td>37.491634</td>\n",
       "      <td>4.801584</td>\n",
       "      <td>13.604786</td>\n",
       "      <td>20.210182</td>\n",
       "      <td>25.149429</td>\n",
       "      <td>26.669170</td>\n",
       "      <td>21.907263</td>\n",
       "      <td>25.833042</td>\n",
       "      <td>27.993830</td>\n",
       "      <td>29.190224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>22.61</td>\n",
       "      <td>27.12</td>\n",
       "      <td>27.53</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.98</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.92</td>\n",
       "      <td>25.82</td>\n",
       "      <td>11.479879</td>\n",
       "      <td>...</td>\n",
       "      <td>36.406391</td>\n",
       "      <td>5.289308</td>\n",
       "      <td>14.693268</td>\n",
       "      <td>20.902817</td>\n",
       "      <td>24.895205</td>\n",
       "      <td>26.095184</td>\n",
       "      <td>22.234713</td>\n",
       "      <td>26.813074</td>\n",
       "      <td>29.008127</td>\n",
       "      <td>30.395195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.57</td>\n",
       "      <td>17.69</td>\n",
       "      <td>21.43</td>\n",
       "      <td>22.13</td>\n",
       "      <td>22.79</td>\n",
       "      <td>24.23</td>\n",
       "      <td>23.32</td>\n",
       "      <td>21.47</td>\n",
       "      <td>23.67</td>\n",
       "      <td>8.403148</td>\n",
       "      <td>...</td>\n",
       "      <td>29.274141</td>\n",
       "      <td>4.699314</td>\n",
       "      <td>12.997721</td>\n",
       "      <td>17.906013</td>\n",
       "      <td>20.675901</td>\n",
       "      <td>21.210585</td>\n",
       "      <td>19.101952</td>\n",
       "      <td>23.671947</td>\n",
       "      <td>25.667013</td>\n",
       "      <td>26.782272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.19</td>\n",
       "      <td>15.84</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.86</td>\n",
       "      <td>15.93</td>\n",
       "      <td>16.22</td>\n",
       "      <td>15.55</td>\n",
       "      <td>15.47</td>\n",
       "      <td>24.20</td>\n",
       "      <td>8.652231</td>\n",
       "      <td>...</td>\n",
       "      <td>28.678186</td>\n",
       "      <td>3.141488</td>\n",
       "      <td>8.855782</td>\n",
       "      <td>14.510162</td>\n",
       "      <td>19.605568</td>\n",
       "      <td>20.842255</td>\n",
       "      <td>16.485331</td>\n",
       "      <td>18.788458</td>\n",
       "      <td>20.285421</td>\n",
       "      <td>20.439587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.95</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>15.83</td>\n",
       "      <td>5.323413</td>\n",
       "      <td>...</td>\n",
       "      <td>21.094549</td>\n",
       "      <td>1.513478</td>\n",
       "      <td>3.967975</td>\n",
       "      <td>7.886890</td>\n",
       "      <td>13.226873</td>\n",
       "      <td>14.994874</td>\n",
       "      <td>11.546255</td>\n",
       "      <td>12.393466</td>\n",
       "      <td>13.691509</td>\n",
       "      <td>13.802272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>13.80</td>\n",
       "      <td>3.997223</td>\n",
       "      <td>...</td>\n",
       "      <td>18.829851</td>\n",
       "      <td>0.320279</td>\n",
       "      <td>0.721823</td>\n",
       "      <td>1.470101</td>\n",
       "      <td>3.497169</td>\n",
       "      <td>5.759925</td>\n",
       "      <td>7.264647</td>\n",
       "      <td>10.291542</td>\n",
       "      <td>14.022621</td>\n",
       "      <td>20.541195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L10.1  L10.2  L10.3  L10.4  L10.5  L10.6  L10.7  L10.8  L10.9      D10.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "15   0.48   0.70   1.80   2.18   0.98   3.56   2.99   6.80   9.87   0.639051   \n",
       "16   2.67   7.45   8.62  10.08  10.06  11.52  17.00  19.79  20.92   1.814343   \n",
       "17   3.80   8.55   8.15  10.49  10.84  12.03  15.05  18.24  25.56   3.588575   \n",
       "18   7.74  14.78  19.70  18.93  22.60  20.57  16.19  15.66  32.96   7.595252   \n",
       "19   9.47  19.06  21.52  21.87  23.62  20.20  17.15  22.85  33.97   8.647614   \n",
       "20  14.33  24.78  24.52  23.02  28.58  28.55  25.06  26.39  27.24  12.565716   \n",
       "21  15.26  26.20  24.57  24.70  29.22  31.11  31.05  31.59  32.45  13.355133   \n",
       "22  15.96  29.89  33.58  33.28  38.13  35.95  34.24  33.24  33.78  16.214230   \n",
       "23  14.24  24.45  29.77  29.36  34.56  30.81  26.23  27.65  34.53  15.461583   \n",
       "24  14.30  25.22  31.45  29.95  34.70  31.31  29.10  27.16  31.12  15.829645   \n",
       "25  16.03  30.02  34.72  32.49  35.12  35.82  33.60  32.73  34.11  15.413681   \n",
       "26  15.21  27.84  33.51  32.70  37.10  34.75  29.17  27.61  29.70  13.965651   \n",
       "27   9.50  22.99  30.11  31.82  32.80  33.88  19.43  36.02  44.45  14.931929   \n",
       "28  10.97  21.03  25.88  24.37  28.26  26.10  21.49  20.61  26.21  11.454762   \n",
       "29  12.63  22.61  27.12  27.53  31.02  29.98  27.37  22.92  25.82  11.479879   \n",
       "30   8.57  17.69  21.43  22.13  22.79  24.23  23.32  21.47  23.67   8.403148   \n",
       "31   7.19  15.84  18.83  17.86  15.93  16.22  15.55  15.47  24.20   8.652231   \n",
       "32   2.95   8.16   8.37   7.24   6.39   6.81   6.33   6.51  15.83   5.323413   \n",
       "33   1.71   4.94   4.62   3.24   3.42   1.19   3.41  -0.50  13.80   3.997223   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17   0.000000   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17   0.000000   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "\n",
       "    ...      G10.9     M10.1      M10.2      M10.3      M10.4      M10.5  \\\n",
       "0   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15  ...  11.893352  0.234644   0.463549   0.761069   1.563330   2.646295   \n",
       "16  ...  25.218607  1.068583   2.409046   4.675882   9.359540  13.214917   \n",
       "17  ...  29.591112  2.043161   5.012439   9.585613  16.715616  20.855846   \n",
       "18  ...  41.306126  3.095799   8.164346  14.750845  23.068312  27.154726   \n",
       "19  ...  36.786316  4.390857  11.824681  19.311783  27.177267  30.514595   \n",
       "20  ...  35.568935  7.802339  19.814159  26.721149  29.886969  30.452824   \n",
       "21  ...  37.465626  9.388758  21.730694  27.419022  29.096201  29.635658   \n",
       "22  ...  47.101749  8.203132  21.373383  30.123791  35.451000  37.933434   \n",
       "23  ...  45.864857  6.597206  18.024208  26.291977  32.616802  35.180676   \n",
       "24  ...  43.873386  6.399664  17.532560  25.250664  30.665916  32.757206   \n",
       "25  ...  42.702572  7.915727  20.271654  27.582012  31.171041  32.775154   \n",
       "26  ...  41.722382  6.401133  17.403168  24.454617  28.796335  30.479309   \n",
       "27  ...  48.921932  4.192406  12.092266  19.799160  27.405876  30.629284   \n",
       "28  ...  37.491634  4.801584  13.604786  20.210182  25.149429  26.669170   \n",
       "29  ...  36.406391  5.289308  14.693268  20.902817  24.895205  26.095184   \n",
       "30  ...  29.274141  4.699314  12.997721  17.906013  20.675901  21.210585   \n",
       "31  ...  28.678186  3.141488   8.855782  14.510162  19.605568  20.842255   \n",
       "32  ...  21.094549  1.513478   3.967975   7.886890  13.226873  14.994874   \n",
       "33  ...  18.829851  0.320279   0.721823   1.470101   3.497169   5.759925   \n",
       "34  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        M10.6      M10.7      M10.8      M10.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   4.231714   6.971114   9.692414  13.553650  \n",
       "16  14.007280  18.917479  24.398327  33.296726  \n",
       "17  20.123676  26.380701  31.559332  38.425480  \n",
       "18  25.184723  32.590385  37.731945  45.355114  \n",
       "19  26.478289  31.496145  35.057873  39.932507  \n",
       "20  26.656958  30.779909  32.709866  34.859814  \n",
       "21  29.236378  34.296619  36.827160  38.316700  \n",
       "22  32.827522  38.408718  41.311008  44.888351  \n",
       "23  29.430502  34.368755  37.460190  41.908260  \n",
       "24  27.466944  32.340839  35.126392  38.594013  \n",
       "25  29.562883  35.402729  38.024281  41.027096  \n",
       "26  26.260344  31.550613  34.133892  36.677464  \n",
       "27  25.642973  31.001484  34.760403  40.601711  \n",
       "28  21.907263  25.833042  27.993830  29.190224  \n",
       "29  22.234713  26.813074  29.008127  30.395195  \n",
       "30  19.101952  23.671947  25.667013  26.782272  \n",
       "31  16.485331  18.788458  20.285421  20.439587  \n",
       "32  11.546255  12.393466  13.691509  13.802272  \n",
       "33   7.264647  10.291542  14.022621  20.541195  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1[:48]#.to_csv('0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    res_0[\"L00.\"+str(i)] = (res_0[\"L00.\"+str(i)] + res_0[\"D00.\"+str(i)] + res_0[\"C00.\"+str(i)] + res_0[\"G00.\"+str(i)] + res_0[\"M00.\"+str(i)])/5\n",
    "    res_1[\"L10.\"+str(i)] = (res_1[\"L10.\"+str(i)] + res_1[\"D10.\"+str(i)] + res_1[\"C10.\"+str(i)] + res_1[\"G10.\"+str(i)] + res_1[\"M10.\"+str(i)])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.csv_Day7_2h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.csv_Day7_3h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.csv_Day7_3h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.csv_Day7_4h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.csv_Day7_4h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.csv_Day7_5h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.csv_Day7_5h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.csv_Day7_6h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.csv_Day7_6h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.csv_Day7_7h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.csv_Day7_7h30m</td>\n",
       "      <td>0.530824</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.881860</td>\n",
       "      <td>1.475716</td>\n",
       "      <td>2.540986</td>\n",
       "      <td>3.158886</td>\n",
       "      <td>5.115217</td>\n",
       "      <td>6.572472</td>\n",
       "      <td>10.980899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.csv_Day7_8h00m</td>\n",
       "      <td>1.859963</td>\n",
       "      <td>3.141735</td>\n",
       "      <td>4.034061</td>\n",
       "      <td>5.509964</td>\n",
       "      <td>8.557673</td>\n",
       "      <td>9.943336</td>\n",
       "      <td>13.038112</td>\n",
       "      <td>18.144376</td>\n",
       "      <td>26.172417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.csv_Day7_8h30m</td>\n",
       "      <td>3.056710</td>\n",
       "      <td>5.426099</td>\n",
       "      <td>6.882192</td>\n",
       "      <td>8.830279</td>\n",
       "      <td>12.162919</td>\n",
       "      <td>13.875269</td>\n",
       "      <td>16.789697</td>\n",
       "      <td>22.926963</td>\n",
       "      <td>31.562655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.csv_Day7_9h00m</td>\n",
       "      <td>5.965285</td>\n",
       "      <td>11.374463</td>\n",
       "      <td>14.010075</td>\n",
       "      <td>16.789352</td>\n",
       "      <td>21.581146</td>\n",
       "      <td>23.298368</td>\n",
       "      <td>23.807216</td>\n",
       "      <td>32.414890</td>\n",
       "      <td>41.789908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.csv_Day7_9h30m</td>\n",
       "      <td>8.014805</td>\n",
       "      <td>14.643485</td>\n",
       "      <td>16.596972</td>\n",
       "      <td>20.074063</td>\n",
       "      <td>24.077036</td>\n",
       "      <td>25.143840</td>\n",
       "      <td>25.909510</td>\n",
       "      <td>32.468594</td>\n",
       "      <td>41.070027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.csv_Day7_10h00m</td>\n",
       "      <td>12.634521</td>\n",
       "      <td>22.227851</td>\n",
       "      <td>24.315059</td>\n",
       "      <td>27.665234</td>\n",
       "      <td>31.061884</td>\n",
       "      <td>33.164349</td>\n",
       "      <td>33.882372</td>\n",
       "      <td>35.693852</td>\n",
       "      <td>37.657335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.csv_Day7_10h30m</td>\n",
       "      <td>13.310415</td>\n",
       "      <td>23.758229</td>\n",
       "      <td>26.079941</td>\n",
       "      <td>28.873160</td>\n",
       "      <td>32.677293</td>\n",
       "      <td>34.422590</td>\n",
       "      <td>36.480429</td>\n",
       "      <td>37.755708</td>\n",
       "      <td>37.793859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.csv_Day7_11h00m</td>\n",
       "      <td>14.660594</td>\n",
       "      <td>25.480044</td>\n",
       "      <td>29.745892</td>\n",
       "      <td>32.747565</td>\n",
       "      <td>37.786615</td>\n",
       "      <td>39.351014</td>\n",
       "      <td>40.740127</td>\n",
       "      <td>43.877964</td>\n",
       "      <td>48.359497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.csv_Day7_11h30m</td>\n",
       "      <td>13.088328</td>\n",
       "      <td>22.909275</td>\n",
       "      <td>26.320636</td>\n",
       "      <td>29.322043</td>\n",
       "      <td>34.260089</td>\n",
       "      <td>36.515522</td>\n",
       "      <td>38.432775</td>\n",
       "      <td>40.982679</td>\n",
       "      <td>48.463852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.csv_Day7_12h00m</td>\n",
       "      <td>13.365701</td>\n",
       "      <td>22.580890</td>\n",
       "      <td>26.652118</td>\n",
       "      <td>29.768343</td>\n",
       "      <td>34.866314</td>\n",
       "      <td>35.237037</td>\n",
       "      <td>35.872544</td>\n",
       "      <td>38.627179</td>\n",
       "      <td>44.785846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.csv_Day7_12h30m</td>\n",
       "      <td>14.024398</td>\n",
       "      <td>24.121882</td>\n",
       "      <td>28.422587</td>\n",
       "      <td>31.166275</td>\n",
       "      <td>35.202526</td>\n",
       "      <td>36.779875</td>\n",
       "      <td>38.632146</td>\n",
       "      <td>40.037993</td>\n",
       "      <td>43.497485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.csv_Day7_13h00m</td>\n",
       "      <td>12.895419</td>\n",
       "      <td>21.167749</td>\n",
       "      <td>25.801675</td>\n",
       "      <td>28.293102</td>\n",
       "      <td>33.268205</td>\n",
       "      <td>33.976121</td>\n",
       "      <td>35.440433</td>\n",
       "      <td>36.925231</td>\n",
       "      <td>41.206730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.csv_Day7_13h30m</td>\n",
       "      <td>10.736424</td>\n",
       "      <td>18.385916</td>\n",
       "      <td>23.334738</td>\n",
       "      <td>26.290205</td>\n",
       "      <td>30.517486</td>\n",
       "      <td>32.582209</td>\n",
       "      <td>33.490324</td>\n",
       "      <td>37.744597</td>\n",
       "      <td>48.874164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.csv_Day7_14h00m</td>\n",
       "      <td>10.144410</td>\n",
       "      <td>17.140973</td>\n",
       "      <td>21.434347</td>\n",
       "      <td>23.174978</td>\n",
       "      <td>26.421846</td>\n",
       "      <td>28.339741</td>\n",
       "      <td>28.781827</td>\n",
       "      <td>30.897937</td>\n",
       "      <td>36.077922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.csv_Day7_14h30m</td>\n",
       "      <td>10.509603</td>\n",
       "      <td>18.019649</td>\n",
       "      <td>22.019413</td>\n",
       "      <td>23.302929</td>\n",
       "      <td>26.390365</td>\n",
       "      <td>27.919289</td>\n",
       "      <td>29.813252</td>\n",
       "      <td>31.138058</td>\n",
       "      <td>34.821961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.csv_Day7_15h00m</td>\n",
       "      <td>8.444591</td>\n",
       "      <td>14.831996</td>\n",
       "      <td>18.491349</td>\n",
       "      <td>19.213952</td>\n",
       "      <td>22.158029</td>\n",
       "      <td>23.503476</td>\n",
       "      <td>25.663597</td>\n",
       "      <td>26.831746</td>\n",
       "      <td>29.023319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.csv_Day7_15h30m</td>\n",
       "      <td>6.934876</td>\n",
       "      <td>12.443200</td>\n",
       "      <td>14.636267</td>\n",
       "      <td>16.039930</td>\n",
       "      <td>17.865486</td>\n",
       "      <td>20.112928</td>\n",
       "      <td>21.463802</td>\n",
       "      <td>23.094188</td>\n",
       "      <td>28.014010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.csv_Day7_16h00m</td>\n",
       "      <td>3.952808</td>\n",
       "      <td>6.909486</td>\n",
       "      <td>8.372840</td>\n",
       "      <td>9.439932</td>\n",
       "      <td>10.611151</td>\n",
       "      <td>12.222500</td>\n",
       "      <td>12.411553</td>\n",
       "      <td>14.459550</td>\n",
       "      <td>19.700559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.csv_Day7_16h30m</td>\n",
       "      <td>1.547624</td>\n",
       "      <td>2.839903</td>\n",
       "      <td>3.146367</td>\n",
       "      <td>3.554163</td>\n",
       "      <td>4.240459</td>\n",
       "      <td>6.435917</td>\n",
       "      <td>7.533984</td>\n",
       "      <td>9.599387</td>\n",
       "      <td>16.997025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.csv_Day7_17h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.csv_Day7_17h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.csv_Day7_18h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.csv_Day7_18h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.csv_Day7_19h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.csv_Day7_19h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.csv_Day7_20h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.csv_Day7_20h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.csv_Day7_21h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.csv_Day7_21h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.csv_Day7_22h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.csv_Day7_22h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.csv_Day7_23h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.csv_Day7_23h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id      q_0.1      q_0.2      q_0.3      q_0.4      q_0.5  \\\n",
       "0    0.csv_Day7_0h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1    0.csv_Day7_0h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2    0.csv_Day7_1h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3    0.csv_Day7_1h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4    0.csv_Day7_2h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5    0.csv_Day7_2h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6    0.csv_Day7_3h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7    0.csv_Day7_3h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8    0.csv_Day7_4h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9    0.csv_Day7_4h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10   0.csv_Day7_5h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11   0.csv_Day7_5h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12   0.csv_Day7_6h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13   0.csv_Day7_6h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14   0.csv_Day7_7h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15   0.csv_Day7_7h30m   0.530824   0.827556   0.881860   1.475716   2.540986   \n",
       "16   0.csv_Day7_8h00m   1.859963   3.141735   4.034061   5.509964   8.557673   \n",
       "17   0.csv_Day7_8h30m   3.056710   5.426099   6.882192   8.830279  12.162919   \n",
       "18   0.csv_Day7_9h00m   5.965285  11.374463  14.010075  16.789352  21.581146   \n",
       "19   0.csv_Day7_9h30m   8.014805  14.643485  16.596972  20.074063  24.077036   \n",
       "20  0.csv_Day7_10h00m  12.634521  22.227851  24.315059  27.665234  31.061884   \n",
       "21  0.csv_Day7_10h30m  13.310415  23.758229  26.079941  28.873160  32.677293   \n",
       "22  0.csv_Day7_11h00m  14.660594  25.480044  29.745892  32.747565  37.786615   \n",
       "23  0.csv_Day7_11h30m  13.088328  22.909275  26.320636  29.322043  34.260089   \n",
       "24  0.csv_Day7_12h00m  13.365701  22.580890  26.652118  29.768343  34.866314   \n",
       "25  0.csv_Day7_12h30m  14.024398  24.121882  28.422587  31.166275  35.202526   \n",
       "26  0.csv_Day7_13h00m  12.895419  21.167749  25.801675  28.293102  33.268205   \n",
       "27  0.csv_Day7_13h30m  10.736424  18.385916  23.334738  26.290205  30.517486   \n",
       "28  0.csv_Day7_14h00m  10.144410  17.140973  21.434347  23.174978  26.421846   \n",
       "29  0.csv_Day7_14h30m  10.509603  18.019649  22.019413  23.302929  26.390365   \n",
       "30  0.csv_Day7_15h00m   8.444591  14.831996  18.491349  19.213952  22.158029   \n",
       "31  0.csv_Day7_15h30m   6.934876  12.443200  14.636267  16.039930  17.865486   \n",
       "32  0.csv_Day7_16h00m   3.952808   6.909486   8.372840   9.439932  10.611151   \n",
       "33  0.csv_Day7_16h30m   1.547624   2.839903   3.146367   3.554163   4.240459   \n",
       "34  0.csv_Day7_17h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  0.csv_Day7_17h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  0.csv_Day7_18h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  0.csv_Day7_18h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  0.csv_Day7_19h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  0.csv_Day7_19h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  0.csv_Day7_20h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  0.csv_Day7_20h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  0.csv_Day7_21h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  0.csv_Day7_21h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  0.csv_Day7_22h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  0.csv_Day7_22h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  0.csv_Day7_23h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  0.csv_Day7_23h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        q_0.6      q_0.7      q_0.8      q_0.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   3.158886   5.115217   6.572472  10.980899  \n",
       "16   9.943336  13.038112  18.144376  26.172417  \n",
       "17  13.875269  16.789697  22.926963  31.562655  \n",
       "18  23.298368  23.807216  32.414890  41.789908  \n",
       "19  25.143840  25.909510  32.468594  41.070027  \n",
       "20  33.164349  33.882372  35.693852  37.657335  \n",
       "21  34.422590  36.480429  37.755708  37.793859  \n",
       "22  39.351014  40.740127  43.877964  48.359497  \n",
       "23  36.515522  38.432775  40.982679  48.463852  \n",
       "24  35.237037  35.872544  38.627179  44.785846  \n",
       "25  36.779875  38.632146  40.037993  43.497485  \n",
       "26  33.976121  35.440433  36.925231  41.206730  \n",
       "27  32.582209  33.490324  37.744597  48.874164  \n",
       "28  28.339741  28.781827  30.897937  36.077922  \n",
       "29  27.919289  29.813252  31.138058  34.821961  \n",
       "30  23.503476  25.663597  26.831746  29.023319  \n",
       "31  20.112928  21.463802  23.094188  28.014010  \n",
       "32  12.222500  12.411553  14.459550  19.700559  \n",
       "33   6.435917   7.533984   9.599387  16.997025  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = res_0[['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']].values\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = res_1[['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']].values\n",
    "submission[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission/submission_20210125-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
