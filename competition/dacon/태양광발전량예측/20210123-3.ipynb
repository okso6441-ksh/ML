{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, is_train=True):\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['NET'] = 37-(37-temp['T'])/(0.68-0.0014*temp.RH+1/(1.76+1.4*temp.WS**0.75))-0.29*temp['T']*(1-0.001*temp.RH)\n",
    "    \n",
    "    temp = temp[['Day','Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T', 'NET']]\n",
    "\n",
    "    if is_train==True:          \n",
    "    \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill')\n",
    "        temp = temp.dropna()\n",
    "        \n",
    "        return temp.iloc[:-96]\n",
    "\n",
    "    elif is_train==False:\n",
    "        \n",
    "        temp = temp[['Day','Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T', 'NET']]\n",
    "                              \n",
    "        return temp.iloc[-48:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, is_train=True):\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    #temp['NET'] = 37-(37-temp['T'])/(0.68-0.0014*temp.RH+1/(1.76+1.4*temp.WS**0.75))-0.29*temp['T']*(1-0.001*temp.RH)\n",
    "    \n",
    "    temp = temp[['Day','Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "\n",
    "    if is_train==True:          \n",
    "    \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill')\n",
    "        temp = temp.dropna()\n",
    "        \n",
    "        return temp.iloc[:-96]\n",
    "\n",
    "    elif is_train==False:\n",
    "        \n",
    "        temp = temp[['Day','Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "                              \n",
    "        return temp.iloc[-48:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train/train.csv')\n",
    "\n",
    "test = []\n",
    "\n",
    "for i in range(81):\n",
    "    file_path = './data/test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp['Day'] = i\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    test.append(temp)\n",
    "\n",
    "df_test = pd.concat(test)\n",
    "\n",
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52464, 10), (3888, 8))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = preprocess_data(train)\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.WS = np.log1p(df_train.WS)\n",
    "df_test.WS = np.log1p(df_test.WS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = df_train[['Day','Hour','DHI','DNI','WS','RH','T','NET']].min()\n",
    "max  = df_train[['Day','Hour','DHI','DNI','WS','RH','T','NET']].max()\n",
    "\n",
    "for i, col in enumerate(['Day','Hour','DHI','DNI','WS','RH','T','NET']):\n",
    "    df_train[col] = (df_train[col] - min[i]) / (max[i] - min[i])\n",
    "    df_test[col] = (df_test[col] - min[i]) / (max[i] - min[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = df_train[['Day','Hour','DHI','DNI','WS','RH','T']].min()\n",
    "max  = df_train[['Day','Hour','DHI','DNI','WS','RH','T']].max()\n",
    "\n",
    "for i, col in enumerate(['Day','Hour','DHI','DNI','WS','RH','T']):\n",
    "    df_train[col] = (df_train[col] - min[i]) / (max[i] - min[i])\n",
    "    df_test[col] = (df_test[col] - min[i]) / (max[i] - min[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Day0 = df_train.iloc[:, :-2]\n",
    "Day  = df_train.iloc[:, 1:-2]\n",
    "Day7 = df_train.iloc[:, -2]\n",
    "Day8 = df_train.iloc[:, -1]\n",
    "Day78 = df_train.iloc[:, -2:]\n",
    "\n",
    "df_test0 = df_test.copy()\n",
    "df_test = df_test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(q, y, pred):\n",
    "    err = (y-pred)\n",
    "    return mean(maximum(q*err, (q-1)*err), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_lst = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39348, 7), (13116, 7), (39348, 2), (13116, 2))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(Day, Day78, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train_1, X_valid_1, Y_train_1, Y_valid_1 = train_test_split(Day, Day7, test_size=0.25, random_state=42)\n",
    "X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(Day, Day8, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import mean, maximum\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "578/615 [===========================>..] - ETA: 0s - loss: 394.4135WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 385.0964 - val_loss: 176.8785\n",
      "Epoch 2/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 169.5682 - val_loss: 158.1537\n",
      "Epoch 3/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 153.5013 - val_loss: 151.6759\n",
      "Epoch 4/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 154.4273 - val_loss: 149.5001\n",
      "Epoch 5/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 148.2966 - val_loss: 150.2883\n",
      "Epoch 6/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 149.8032 - val_loss: 151.9733\n",
      "Epoch 7/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 149.0454 - val_loss: 144.9851\n",
      "Epoch 8/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 141.6659 - val_loss: 149.7142\n",
      "Epoch 9/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 147.3332 - val_loss: 145.1392\n",
      "Epoch 10/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 147.9937 - val_loss: 142.5788\n",
      "Epoch 11/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 151.8745 - val_loss: 149.8084\n",
      "Epoch 12/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 144.3558 - val_loss: 141.8043\n",
      "Epoch 13/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 143.0712 - val_loss: 140.5261\n",
      "Epoch 14/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 144.6060 - val_loss: 139.1617\n",
      "Epoch 15/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 137.4192 - val_loss: 139.4843\n",
      "Epoch 16/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 143.7198 - val_loss: 139.7647\n",
      "Epoch 17/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 145.0125 - val_loss: 140.1033\n",
      "Epoch 00017: early stopping\n",
      "410/410 [==============================] - 0s 646us/step - loss: 137.2055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "137.20550537109375"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(Day.shape)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(2)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "hist = model.fit(X_train, Y_train, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "model.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'], 'b-', label='loss')\n",
    "plt.plot(hist.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "810/820 [============================>.] - ETA: 0s - loss: 1.4857WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 1.4851 - val_loss: 1.6052\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4185 - val_loss: 1.6069\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4072 - val_loss: 1.6011\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4009 - val_loss: 1.5986\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4115 - val_loss: 1.6163\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4009 - val_loss: 1.6200\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4136 - val_loss: 1.6082\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "812/820 [============================>.] - ETA: 0s - loss: 2.2933WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.2932 - val_loss: 2.6119\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2651 - val_loss: 2.6073\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2655 - val_loss: 2.6086\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2550 - val_loss: 2.6064\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2694 - val_loss: 2.6003\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2650 - val_loss: 2.6104\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2778 - val_loss: 2.6056\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2887 - val_loss: 2.6096\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "786/820 [===========================>..] - ETA: 0s - loss: 2.7201WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.7195 - val_loss: 3.0879\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6880 - val_loss: 3.1071\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6893 - val_loss: 3.1129\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6736 - val_loss: 3.0770\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6928 - val_loss: 3.1102\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6852 - val_loss: 3.1002\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7072 - val_loss: 3.0777\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "786/820 [===========================>..] - ETA: 0s - loss: 2.8189WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8183 - val_loss: 3.1552\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7805 - val_loss: 3.1758\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7828 - val_loss: 3.2284\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7777 - val_loss: 3.1911\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "779/820 [===========================>..] - ETA: 0s - loss: 2.6753WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.6751 - val_loss: 2.9923\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6447 - val_loss: 3.0326\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6520 - val_loss: 3.0277\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6457 - val_loss: 3.0934\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 2.3706WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.3706 - val_loss: 2.6839\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3444 - val_loss: 2.6989\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3495 - val_loss: 2.6958\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3539 - val_loss: 2.7136\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 1.9412WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 1.9412 - val_loss: 2.1818\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9250 - val_loss: 2.2110\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9277 - val_loss: 2.1900\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9256 - val_loss: 2.2001\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "792/820 [===========================>..] - ETA: 0s - loss: 1.4222WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 1.4224 - val_loss: 1.6351\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4090 - val_loss: 1.6932\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4160 - val_loss: 1.6348\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4089 - val_loss: 1.6238\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4109 - val_loss: 1.5852\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3995 - val_loss: 1.5968\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4190 - val_loss: 1.5955\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4416 - val_loss: 1.6712\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "812/820 [============================>.] - ETA: 0s - loss: 0.8020WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.8020 - val_loss: 0.8870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7849 - val_loss: 0.9720\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7940 - val_loss: 0.9880\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7872 - val_loss: 0.8869\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7812 - val_loss: 0.8837\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7770 - val_loss: 0.8834\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7907 - val_loss: 0.8926\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.8064 - val_loss: 0.9210\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7766 - val_loss: 0.9302\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3888, 18)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -NET\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    model.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model.fit(Day, Day78, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred = pd.DataFrame(model.predict(df_test))\n",
    "    results = pd.concat([results, pred], axis=1)\n",
    "\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.173280</td>\n",
       "      <td>-0.112882</td>\n",
       "      <td>-0.026336</td>\n",
       "      <td>-0.082966</td>\n",
       "      <td>-0.142874</td>\n",
       "      <td>-0.158126</td>\n",
       "      <td>0.035049</td>\n",
       "      <td>0.105459</td>\n",
       "      <td>0.098779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.175137</td>\n",
       "      <td>-0.114583</td>\n",
       "      <td>-0.027262</td>\n",
       "      <td>-0.085657</td>\n",
       "      <td>-0.144745</td>\n",
       "      <td>-0.161045</td>\n",
       "      <td>0.033186</td>\n",
       "      <td>0.103376</td>\n",
       "      <td>0.100205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.168723</td>\n",
       "      <td>-0.115475</td>\n",
       "      <td>-0.043168</td>\n",
       "      <td>-0.092992</td>\n",
       "      <td>-0.152000</td>\n",
       "      <td>-0.166081</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.094770</td>\n",
       "      <td>0.093328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.167419</td>\n",
       "      <td>-0.114649</td>\n",
       "      <td>-0.042480</td>\n",
       "      <td>-0.090919</td>\n",
       "      <td>-0.149597</td>\n",
       "      <td>-0.164335</td>\n",
       "      <td>0.035445</td>\n",
       "      <td>0.096288</td>\n",
       "      <td>0.091852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.140366</td>\n",
       "      <td>-0.113283</td>\n",
       "      <td>-0.057419</td>\n",
       "      <td>-0.094727</td>\n",
       "      <td>-0.151547</td>\n",
       "      <td>-0.167499</td>\n",
       "      <td>0.034782</td>\n",
       "      <td>0.090059</td>\n",
       "      <td>0.083736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.143237</td>\n",
       "      <td>-0.116482</td>\n",
       "      <td>-0.058976</td>\n",
       "      <td>-0.096986</td>\n",
       "      <td>-0.157438</td>\n",
       "      <td>-0.174194</td>\n",
       "      <td>0.032678</td>\n",
       "      <td>0.086638</td>\n",
       "      <td>0.086372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.136401</td>\n",
       "      <td>-0.117417</td>\n",
       "      <td>-0.075423</td>\n",
       "      <td>-0.106786</td>\n",
       "      <td>-0.162762</td>\n",
       "      <td>-0.183641</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.082952</td>\n",
       "      <td>0.079357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.143101</td>\n",
       "      <td>-0.121938</td>\n",
       "      <td>-0.078156</td>\n",
       "      <td>-0.110703</td>\n",
       "      <td>-0.172558</td>\n",
       "      <td>-0.189191</td>\n",
       "      <td>0.021167</td>\n",
       "      <td>0.074970</td>\n",
       "      <td>0.086059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.141119</td>\n",
       "      <td>-0.119701</td>\n",
       "      <td>-0.093900</td>\n",
       "      <td>-0.123412</td>\n",
       "      <td>-0.180591</td>\n",
       "      <td>-0.188250</td>\n",
       "      <td>0.023280</td>\n",
       "      <td>0.062130</td>\n",
       "      <td>0.078949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.146155</td>\n",
       "      <td>-0.122809</td>\n",
       "      <td>-0.091415</td>\n",
       "      <td>-0.126262</td>\n",
       "      <td>-0.186476</td>\n",
       "      <td>-0.188260</td>\n",
       "      <td>0.028133</td>\n",
       "      <td>0.063044</td>\n",
       "      <td>0.078893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.142657</td>\n",
       "      <td>-0.121279</td>\n",
       "      <td>-0.093096</td>\n",
       "      <td>-0.135293</td>\n",
       "      <td>-0.192586</td>\n",
       "      <td>-0.174805</td>\n",
       "      <td>0.054481</td>\n",
       "      <td>0.068432</td>\n",
       "      <td>0.071351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.145219</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>-0.093137</td>\n",
       "      <td>-0.134889</td>\n",
       "      <td>-0.194523</td>\n",
       "      <td>-0.171696</td>\n",
       "      <td>0.059499</td>\n",
       "      <td>0.069448</td>\n",
       "      <td>0.071616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.136755</td>\n",
       "      <td>-0.119265</td>\n",
       "      <td>-0.096391</td>\n",
       "      <td>-0.129483</td>\n",
       "      <td>-0.175540</td>\n",
       "      <td>-0.151159</td>\n",
       "      <td>0.081604</td>\n",
       "      <td>0.081086</td>\n",
       "      <td>0.075268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.139141</td>\n",
       "      <td>-0.118973</td>\n",
       "      <td>-0.096385</td>\n",
       "      <td>-0.129622</td>\n",
       "      <td>-0.172387</td>\n",
       "      <td>-0.147869</td>\n",
       "      <td>0.086318</td>\n",
       "      <td>0.082668</td>\n",
       "      <td>0.075175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.125433</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>-0.095996</td>\n",
       "      <td>-0.120609</td>\n",
       "      <td>-0.141422</td>\n",
       "      <td>-0.132940</td>\n",
       "      <td>0.103127</td>\n",
       "      <td>0.092738</td>\n",
       "      <td>0.134697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.438392</td>\n",
       "      <td>1.065793</td>\n",
       "      <td>1.380874</td>\n",
       "      <td>1.825660</td>\n",
       "      <td>2.688340</td>\n",
       "      <td>2.845881</td>\n",
       "      <td>4.985045</td>\n",
       "      <td>8.806827</td>\n",
       "      <td>13.323089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.898831</td>\n",
       "      <td>3.719642</td>\n",
       "      <td>1.950130</td>\n",
       "      <td>3.546952</td>\n",
       "      <td>5.750280</td>\n",
       "      <td>7.727552</td>\n",
       "      <td>11.606936</td>\n",
       "      <td>16.527632</td>\n",
       "      <td>20.037748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.243876</td>\n",
       "      <td>6.278206</td>\n",
       "      <td>3.949213</td>\n",
       "      <td>5.976617</td>\n",
       "      <td>8.861923</td>\n",
       "      <td>10.515333</td>\n",
       "      <td>14.083081</td>\n",
       "      <td>19.906677</td>\n",
       "      <td>24.068823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.654971</td>\n",
       "      <td>12.918764</td>\n",
       "      <td>11.381062</td>\n",
       "      <td>13.949798</td>\n",
       "      <td>19.011719</td>\n",
       "      <td>22.317722</td>\n",
       "      <td>26.526697</td>\n",
       "      <td>35.115940</td>\n",
       "      <td>34.709034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.701793</td>\n",
       "      <td>15.463189</td>\n",
       "      <td>13.356946</td>\n",
       "      <td>15.069485</td>\n",
       "      <td>21.297119</td>\n",
       "      <td>25.279186</td>\n",
       "      <td>28.429756</td>\n",
       "      <td>34.322750</td>\n",
       "      <td>33.347805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.442618</td>\n",
       "      <td>17.392895</td>\n",
       "      <td>20.533186</td>\n",
       "      <td>20.075703</td>\n",
       "      <td>27.802820</td>\n",
       "      <td>32.698555</td>\n",
       "      <td>32.922352</td>\n",
       "      <td>43.336575</td>\n",
       "      <td>37.007034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.227545</td>\n",
       "      <td>17.220694</td>\n",
       "      <td>20.487322</td>\n",
       "      <td>22.618780</td>\n",
       "      <td>29.479536</td>\n",
       "      <td>32.977196</td>\n",
       "      <td>33.245724</td>\n",
       "      <td>39.623795</td>\n",
       "      <td>36.799953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.879261</td>\n",
       "      <td>23.035213</td>\n",
       "      <td>25.664602</td>\n",
       "      <td>28.548038</td>\n",
       "      <td>36.241917</td>\n",
       "      <td>43.029736</td>\n",
       "      <td>44.017544</td>\n",
       "      <td>55.169674</td>\n",
       "      <td>48.123547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11.177592</td>\n",
       "      <td>21.684195</td>\n",
       "      <td>23.435768</td>\n",
       "      <td>26.547911</td>\n",
       "      <td>33.909725</td>\n",
       "      <td>40.605057</td>\n",
       "      <td>42.816109</td>\n",
       "      <td>51.869617</td>\n",
       "      <td>45.805946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.822333</td>\n",
       "      <td>21.800146</td>\n",
       "      <td>23.607233</td>\n",
       "      <td>29.254961</td>\n",
       "      <td>35.771065</td>\n",
       "      <td>43.290833</td>\n",
       "      <td>44.941071</td>\n",
       "      <td>49.597687</td>\n",
       "      <td>42.272297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.649780</td>\n",
       "      <td>21.818579</td>\n",
       "      <td>24.269653</td>\n",
       "      <td>29.405388</td>\n",
       "      <td>36.348072</td>\n",
       "      <td>43.565762</td>\n",
       "      <td>44.191795</td>\n",
       "      <td>51.694302</td>\n",
       "      <td>42.362968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.131831</td>\n",
       "      <td>19.248322</td>\n",
       "      <td>21.088284</td>\n",
       "      <td>29.151098</td>\n",
       "      <td>36.351955</td>\n",
       "      <td>40.957409</td>\n",
       "      <td>42.294697</td>\n",
       "      <td>47.191700</td>\n",
       "      <td>38.911446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.216905</td>\n",
       "      <td>20.463072</td>\n",
       "      <td>19.872711</td>\n",
       "      <td>27.163668</td>\n",
       "      <td>34.269417</td>\n",
       "      <td>39.620754</td>\n",
       "      <td>43.306099</td>\n",
       "      <td>46.321411</td>\n",
       "      <td>40.809608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.601222</td>\n",
       "      <td>15.800512</td>\n",
       "      <td>16.171431</td>\n",
       "      <td>23.151598</td>\n",
       "      <td>30.110863</td>\n",
       "      <td>33.238125</td>\n",
       "      <td>34.393333</td>\n",
       "      <td>40.771839</td>\n",
       "      <td>33.637337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.567501</td>\n",
       "      <td>15.797671</td>\n",
       "      <td>16.519487</td>\n",
       "      <td>23.746649</td>\n",
       "      <td>30.716818</td>\n",
       "      <td>34.246277</td>\n",
       "      <td>35.108604</td>\n",
       "      <td>41.362522</td>\n",
       "      <td>33.496273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.413460</td>\n",
       "      <td>11.727010</td>\n",
       "      <td>12.003023</td>\n",
       "      <td>17.975300</td>\n",
       "      <td>24.914330</td>\n",
       "      <td>26.798233</td>\n",
       "      <td>27.273451</td>\n",
       "      <td>33.255566</td>\n",
       "      <td>26.272306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.938184</td>\n",
       "      <td>11.244136</td>\n",
       "      <td>10.773802</td>\n",
       "      <td>14.965928</td>\n",
       "      <td>20.782150</td>\n",
       "      <td>22.064165</td>\n",
       "      <td>23.843273</td>\n",
       "      <td>30.502968</td>\n",
       "      <td>24.657370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.910076</td>\n",
       "      <td>6.372747</td>\n",
       "      <td>5.624009</td>\n",
       "      <td>7.572339</td>\n",
       "      <td>11.767149</td>\n",
       "      <td>10.935492</td>\n",
       "      <td>13.122830</td>\n",
       "      <td>18.750780</td>\n",
       "      <td>17.521769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.529654</td>\n",
       "      <td>2.680435</td>\n",
       "      <td>2.728501</td>\n",
       "      <td>3.795554</td>\n",
       "      <td>7.646222</td>\n",
       "      <td>7.306982</td>\n",
       "      <td>10.006103</td>\n",
       "      <td>13.777181</td>\n",
       "      <td>19.709856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.111503</td>\n",
       "      <td>-0.052534</td>\n",
       "      <td>-0.018186</td>\n",
       "      <td>-0.037241</td>\n",
       "      <td>-0.068481</td>\n",
       "      <td>-0.105694</td>\n",
       "      <td>0.062720</td>\n",
       "      <td>0.017497</td>\n",
       "      <td>0.284665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.107732</td>\n",
       "      <td>-0.046012</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.037188</td>\n",
       "      <td>-0.067602</td>\n",
       "      <td>-0.104399</td>\n",
       "      <td>0.062948</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.271836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.094565</td>\n",
       "      <td>-0.024040</td>\n",
       "      <td>-0.014958</td>\n",
       "      <td>-0.035694</td>\n",
       "      <td>-0.066142</td>\n",
       "      <td>-0.107790</td>\n",
       "      <td>0.058190</td>\n",
       "      <td>0.027774</td>\n",
       "      <td>0.045320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.093457</td>\n",
       "      <td>-0.023164</td>\n",
       "      <td>-0.015181</td>\n",
       "      <td>-0.035649</td>\n",
       "      <td>-0.065477</td>\n",
       "      <td>-0.106988</td>\n",
       "      <td>0.058449</td>\n",
       "      <td>0.028833</td>\n",
       "      <td>0.045344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.097099</td>\n",
       "      <td>-0.021429</td>\n",
       "      <td>-0.013311</td>\n",
       "      <td>-0.034270</td>\n",
       "      <td>-0.063589</td>\n",
       "      <td>-0.108866</td>\n",
       "      <td>0.053828</td>\n",
       "      <td>0.037492</td>\n",
       "      <td>0.031296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.095971</td>\n",
       "      <td>-0.020593</td>\n",
       "      <td>-0.013667</td>\n",
       "      <td>-0.034200</td>\n",
       "      <td>-0.062874</td>\n",
       "      <td>-0.108010</td>\n",
       "      <td>0.053992</td>\n",
       "      <td>0.037493</td>\n",
       "      <td>0.031408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.079436</td>\n",
       "      <td>-0.016565</td>\n",
       "      <td>-0.011696</td>\n",
       "      <td>-0.033408</td>\n",
       "      <td>-0.054235</td>\n",
       "      <td>-0.106762</td>\n",
       "      <td>0.050779</td>\n",
       "      <td>0.036118</td>\n",
       "      <td>0.043809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.082888</td>\n",
       "      <td>-0.015838</td>\n",
       "      <td>-0.011769</td>\n",
       "      <td>-0.033062</td>\n",
       "      <td>-0.053637</td>\n",
       "      <td>-0.106015</td>\n",
       "      <td>0.051239</td>\n",
       "      <td>0.036463</td>\n",
       "      <td>0.045523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.035586</td>\n",
       "      <td>-0.014107</td>\n",
       "      <td>-0.008856</td>\n",
       "      <td>-0.039944</td>\n",
       "      <td>-0.044345</td>\n",
       "      <td>-0.097693</td>\n",
       "      <td>0.046928</td>\n",
       "      <td>0.039482</td>\n",
       "      <td>0.060551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.037804</td>\n",
       "      <td>-0.013084</td>\n",
       "      <td>-0.008881</td>\n",
       "      <td>-0.038468</td>\n",
       "      <td>-0.043124</td>\n",
       "      <td>-0.096952</td>\n",
       "      <td>0.047906</td>\n",
       "      <td>0.039676</td>\n",
       "      <td>0.062867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.014040</td>\n",
       "      <td>-0.001715</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>-0.026578</td>\n",
       "      <td>-0.046873</td>\n",
       "      <td>-0.093548</td>\n",
       "      <td>0.045883</td>\n",
       "      <td>0.036296</td>\n",
       "      <td>0.068347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.013162</td>\n",
       "      <td>-0.001404</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>-0.046690</td>\n",
       "      <td>-0.094205</td>\n",
       "      <td>0.046219</td>\n",
       "      <td>0.036476</td>\n",
       "      <td>0.068934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.042704</td>\n",
       "      <td>-0.003896</td>\n",
       "      <td>-0.002310</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>-0.033743</td>\n",
       "      <td>-0.062597</td>\n",
       "      <td>0.056553</td>\n",
       "      <td>0.036814</td>\n",
       "      <td>0.086553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.042704</td>\n",
       "      <td>-0.003896</td>\n",
       "      <td>-0.002310</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>-0.033743</td>\n",
       "      <td>-0.062597</td>\n",
       "      <td>0.056553</td>\n",
       "      <td>0.036814</td>\n",
       "      <td>0.086553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.173280  -0.112882  -0.026336  -0.082966  -0.142874  -0.158126   \n",
       "1   -0.175137  -0.114583  -0.027262  -0.085657  -0.144745  -0.161045   \n",
       "2   -0.168723  -0.115475  -0.043168  -0.092992  -0.152000  -0.166081   \n",
       "3   -0.167419  -0.114649  -0.042480  -0.090919  -0.149597  -0.164335   \n",
       "4   -0.140366  -0.113283  -0.057419  -0.094727  -0.151547  -0.167499   \n",
       "5   -0.143237  -0.116482  -0.058976  -0.096986  -0.157438  -0.174194   \n",
       "6   -0.136401  -0.117417  -0.075423  -0.106786  -0.162762  -0.183641   \n",
       "7   -0.143101  -0.121938  -0.078156  -0.110703  -0.172558  -0.189191   \n",
       "8   -0.141119  -0.119701  -0.093900  -0.123412  -0.180591  -0.188250   \n",
       "9   -0.146155  -0.122809  -0.091415  -0.126262  -0.186476  -0.188260   \n",
       "10  -0.142657  -0.121279  -0.093096  -0.135293  -0.192586  -0.174805   \n",
       "11  -0.145219  -0.121515  -0.093137  -0.134889  -0.194523  -0.171696   \n",
       "12  -0.136755  -0.119265  -0.096391  -0.129483  -0.175540  -0.151159   \n",
       "13  -0.139141  -0.118973  -0.096385  -0.129622  -0.172387  -0.147869   \n",
       "14  -0.125433  -0.108837  -0.095996  -0.120609  -0.141422  -0.132940   \n",
       "15   0.438392   1.065793   1.380874   1.825660   2.688340   2.845881   \n",
       "16   0.898831   3.719642   1.950130   3.546952   5.750280   7.727552   \n",
       "17   2.243876   6.278206   3.949213   5.976617   8.861923  10.515333   \n",
       "18   7.654971  12.918764  11.381062  13.949798  19.011719  22.317722   \n",
       "19   8.701793  15.463189  13.356946  15.069485  21.297119  25.279186   \n",
       "20   8.442618  17.392895  20.533186  20.075703  27.802820  32.698555   \n",
       "21   8.227545  17.220694  20.487322  22.618780  29.479536  32.977196   \n",
       "22  10.879261  23.035213  25.664602  28.548038  36.241917  43.029736   \n",
       "23  11.177592  21.684195  23.435768  26.547911  33.909725  40.605057   \n",
       "24  10.822333  21.800146  23.607233  29.254961  35.771065  43.290833   \n",
       "25   9.649780  21.818579  24.269653  29.405388  36.348072  43.565762   \n",
       "26   9.131831  19.248322  21.088284  29.151098  36.351955  40.957409   \n",
       "27  10.216905  20.463072  19.872711  27.163668  34.269417  39.620754   \n",
       "28   7.601222  15.800512  16.171431  23.151598  30.110863  33.238125   \n",
       "29   7.567501  15.797671  16.519487  23.746649  30.716818  34.246277   \n",
       "30   5.413460  11.727010  12.003023  17.975300  24.914330  26.798233   \n",
       "31   4.938184  11.244136  10.773802  14.965928  20.782150  22.064165   \n",
       "32   2.910076   6.372747   5.624009   7.572339  11.767149  10.935492   \n",
       "33   1.529654   2.680435   2.728501   3.795554   7.646222   7.306982   \n",
       "34  -0.111503  -0.052534  -0.018186  -0.037241  -0.068481  -0.105694   \n",
       "35  -0.107732  -0.046012  -0.018446  -0.037188  -0.067602  -0.104399   \n",
       "36  -0.094565  -0.024040  -0.014958  -0.035694  -0.066142  -0.107790   \n",
       "37  -0.093457  -0.023164  -0.015181  -0.035649  -0.065477  -0.106988   \n",
       "38  -0.097099  -0.021429  -0.013311  -0.034270  -0.063589  -0.108866   \n",
       "39  -0.095971  -0.020593  -0.013667  -0.034200  -0.062874  -0.108010   \n",
       "40  -0.079436  -0.016565  -0.011696  -0.033408  -0.054235  -0.106762   \n",
       "41  -0.082888  -0.015838  -0.011769  -0.033062  -0.053637  -0.106015   \n",
       "42  -0.035586  -0.014107  -0.008856  -0.039944  -0.044345  -0.097693   \n",
       "43  -0.037804  -0.013084  -0.008881  -0.038468  -0.043124  -0.096952   \n",
       "44  -0.014040  -0.001715   0.003089  -0.026578  -0.046873  -0.093548   \n",
       "45  -0.013162  -0.001404   0.002988  -0.026521  -0.046690  -0.094205   \n",
       "46  -0.042704  -0.003896  -0.002310   0.005785  -0.033743  -0.062597   \n",
       "47  -0.042704  -0.003896  -0.002310   0.005785  -0.033743  -0.062597   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.035049   0.105459   0.098779  \n",
       "1    0.033186   0.103376   0.100205  \n",
       "2    0.034788   0.094770   0.093328  \n",
       "3    0.035445   0.096288   0.091852  \n",
       "4    0.034782   0.090059   0.083736  \n",
       "5    0.032678   0.086638   0.086372  \n",
       "6    0.028700   0.082952   0.079357  \n",
       "7    0.021167   0.074970   0.086059  \n",
       "8    0.023280   0.062130   0.078949  \n",
       "9    0.028133   0.063044   0.078893  \n",
       "10   0.054481   0.068432   0.071351  \n",
       "11   0.059499   0.069448   0.071616  \n",
       "12   0.081604   0.081086   0.075268  \n",
       "13   0.086318   0.082668   0.075175  \n",
       "14   0.103127   0.092738   0.134697  \n",
       "15   4.985045   8.806827  13.323089  \n",
       "16  11.606936  16.527632  20.037748  \n",
       "17  14.083081  19.906677  24.068823  \n",
       "18  26.526697  35.115940  34.709034  \n",
       "19  28.429756  34.322750  33.347805  \n",
       "20  32.922352  43.336575  37.007034  \n",
       "21  33.245724  39.623795  36.799953  \n",
       "22  44.017544  55.169674  48.123547  \n",
       "23  42.816109  51.869617  45.805946  \n",
       "24  44.941071  49.597687  42.272297  \n",
       "25  44.191795  51.694302  42.362968  \n",
       "26  42.294697  47.191700  38.911446  \n",
       "27  43.306099  46.321411  40.809608  \n",
       "28  34.393333  40.771839  33.637337  \n",
       "29  35.108604  41.362522  33.496273  \n",
       "30  27.273451  33.255566  26.272306  \n",
       "31  23.843273  30.502968  24.657370  \n",
       "32  13.122830  18.750780  17.521769  \n",
       "33  10.006103  13.777181  19.709856  \n",
       "34   0.062720   0.017497   0.284665  \n",
       "35   0.062948   0.016283   0.271836  \n",
       "36   0.058190   0.027774   0.045320  \n",
       "37   0.058449   0.028833   0.045344  \n",
       "38   0.053828   0.037492   0.031296  \n",
       "39   0.053992   0.037493   0.031408  \n",
       "40   0.050779   0.036118   0.043809  \n",
       "41   0.051239   0.036463   0.045523  \n",
       "42   0.046928   0.039482   0.060551  \n",
       "43   0.047906   0.039676   0.062867  \n",
       "44   0.045883   0.036296   0.068347  \n",
       "45   0.046219   0.036476   0.068934  \n",
       "46   0.056553   0.036814   0.086553  \n",
       "47   0.056553   0.036814   0.086553  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.196271</td>\n",
       "      <td>-0.074919</td>\n",
       "      <td>-0.056662</td>\n",
       "      <td>-0.061915</td>\n",
       "      <td>-0.116000</td>\n",
       "      <td>-0.119493</td>\n",
       "      <td>0.061687</td>\n",
       "      <td>0.089609</td>\n",
       "      <td>0.069562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.198651</td>\n",
       "      <td>-0.076534</td>\n",
       "      <td>-0.058552</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.118259</td>\n",
       "      <td>-0.122723</td>\n",
       "      <td>0.059422</td>\n",
       "      <td>0.087269</td>\n",
       "      <td>0.069196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.202375</td>\n",
       "      <td>-0.086865</td>\n",
       "      <td>-0.072739</td>\n",
       "      <td>-0.064942</td>\n",
       "      <td>-0.117579</td>\n",
       "      <td>-0.122366</td>\n",
       "      <td>0.062973</td>\n",
       "      <td>0.080645</td>\n",
       "      <td>0.059048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.200499</td>\n",
       "      <td>-0.086200</td>\n",
       "      <td>-0.071209</td>\n",
       "      <td>-0.062759</td>\n",
       "      <td>-0.115310</td>\n",
       "      <td>-0.120421</td>\n",
       "      <td>0.063775</td>\n",
       "      <td>0.082415</td>\n",
       "      <td>0.058865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.202891</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>-0.083316</td>\n",
       "      <td>-0.059101</td>\n",
       "      <td>-0.108575</td>\n",
       "      <td>-0.117404</td>\n",
       "      <td>0.062292</td>\n",
       "      <td>0.069193</td>\n",
       "      <td>0.049584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.206415</td>\n",
       "      <td>-0.097278</td>\n",
       "      <td>-0.086791</td>\n",
       "      <td>-0.062549</td>\n",
       "      <td>-0.113868</td>\n",
       "      <td>-0.124359</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.067520</td>\n",
       "      <td>0.048884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.208896</td>\n",
       "      <td>-0.107616</td>\n",
       "      <td>-0.100869</td>\n",
       "      <td>-0.060932</td>\n",
       "      <td>-0.110093</td>\n",
       "      <td>-0.128710</td>\n",
       "      <td>0.056342</td>\n",
       "      <td>0.064198</td>\n",
       "      <td>0.038742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.218464</td>\n",
       "      <td>-0.111346</td>\n",
       "      <td>-0.107530</td>\n",
       "      <td>-0.062085</td>\n",
       "      <td>-0.117532</td>\n",
       "      <td>-0.130268</td>\n",
       "      <td>0.049430</td>\n",
       "      <td>0.059209</td>\n",
       "      <td>0.038563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.228049</td>\n",
       "      <td>-0.113947</td>\n",
       "      <td>-0.118512</td>\n",
       "      <td>-0.062370</td>\n",
       "      <td>-0.113471</td>\n",
       "      <td>-0.121565</td>\n",
       "      <td>0.054826</td>\n",
       "      <td>0.045247</td>\n",
       "      <td>0.024571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.234109</td>\n",
       "      <td>-0.115230</td>\n",
       "      <td>-0.115352</td>\n",
       "      <td>-0.064615</td>\n",
       "      <td>-0.117477</td>\n",
       "      <td>-0.118512</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.045332</td>\n",
       "      <td>0.021873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.241534</td>\n",
       "      <td>-0.118842</td>\n",
       "      <td>-0.109815</td>\n",
       "      <td>-0.061406</td>\n",
       "      <td>-0.113161</td>\n",
       "      <td>-0.098839</td>\n",
       "      <td>0.092695</td>\n",
       "      <td>0.040764</td>\n",
       "      <td>0.012497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.243413</td>\n",
       "      <td>-0.119305</td>\n",
       "      <td>-0.110137</td>\n",
       "      <td>-0.060455</td>\n",
       "      <td>-0.113392</td>\n",
       "      <td>-0.094318</td>\n",
       "      <td>0.098351</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>0.011789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.244977</td>\n",
       "      <td>-0.121861</td>\n",
       "      <td>-0.106654</td>\n",
       "      <td>-0.044627</td>\n",
       "      <td>-0.091494</td>\n",
       "      <td>-0.070870</td>\n",
       "      <td>0.121295</td>\n",
       "      <td>0.043598</td>\n",
       "      <td>0.014249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.246698</td>\n",
       "      <td>-0.122062</td>\n",
       "      <td>-0.106909</td>\n",
       "      <td>-0.042980</td>\n",
       "      <td>-0.086609</td>\n",
       "      <td>-0.066291</td>\n",
       "      <td>0.126661</td>\n",
       "      <td>0.043961</td>\n",
       "      <td>0.013151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.241465</td>\n",
       "      <td>-0.112306</td>\n",
       "      <td>-0.102834</td>\n",
       "      <td>-0.029472</td>\n",
       "      <td>-0.055447</td>\n",
       "      <td>-0.049305</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.045988</td>\n",
       "      <td>0.071569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.106694</td>\n",
       "      <td>0.698537</td>\n",
       "      <td>1.460856</td>\n",
       "      <td>2.321656</td>\n",
       "      <td>3.709854</td>\n",
       "      <td>3.764926</td>\n",
       "      <td>6.132225</td>\n",
       "      <td>10.367605</td>\n",
       "      <td>15.144733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.164094</td>\n",
       "      <td>3.540992</td>\n",
       "      <td>1.968588</td>\n",
       "      <td>4.585263</td>\n",
       "      <td>7.734929</td>\n",
       "      <td>9.836046</td>\n",
       "      <td>13.956335</td>\n",
       "      <td>19.377384</td>\n",
       "      <td>22.624846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.581352</td>\n",
       "      <td>6.593179</td>\n",
       "      <td>4.424622</td>\n",
       "      <td>7.695791</td>\n",
       "      <td>11.603088</td>\n",
       "      <td>13.169626</td>\n",
       "      <td>16.849009</td>\n",
       "      <td>22.955547</td>\n",
       "      <td>26.913954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.584140</td>\n",
       "      <td>14.748760</td>\n",
       "      <td>13.863460</td>\n",
       "      <td>17.585978</td>\n",
       "      <td>22.878448</td>\n",
       "      <td>25.932512</td>\n",
       "      <td>29.993963</td>\n",
       "      <td>38.716805</td>\n",
       "      <td>38.109646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.712058</td>\n",
       "      <td>17.394629</td>\n",
       "      <td>16.259808</td>\n",
       "      <td>19.121983</td>\n",
       "      <td>25.245377</td>\n",
       "      <td>28.832279</td>\n",
       "      <td>31.794573</td>\n",
       "      <td>37.603577</td>\n",
       "      <td>36.304626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.296864</td>\n",
       "      <td>19.077950</td>\n",
       "      <td>23.913515</td>\n",
       "      <td>24.385073</td>\n",
       "      <td>31.848423</td>\n",
       "      <td>36.024605</td>\n",
       "      <td>35.846745</td>\n",
       "      <td>46.183586</td>\n",
       "      <td>38.748909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.116883</td>\n",
       "      <td>18.579971</td>\n",
       "      <td>23.625925</td>\n",
       "      <td>26.859409</td>\n",
       "      <td>33.521236</td>\n",
       "      <td>36.184830</td>\n",
       "      <td>35.999237</td>\n",
       "      <td>41.905807</td>\n",
       "      <td>37.807621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.945988</td>\n",
       "      <td>24.939468</td>\n",
       "      <td>29.423132</td>\n",
       "      <td>33.494698</td>\n",
       "      <td>40.755520</td>\n",
       "      <td>46.624065</td>\n",
       "      <td>47.247871</td>\n",
       "      <td>57.867039</td>\n",
       "      <td>49.945633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11.328506</td>\n",
       "      <td>23.567522</td>\n",
       "      <td>27.106218</td>\n",
       "      <td>31.375063</td>\n",
       "      <td>38.331802</td>\n",
       "      <td>44.219810</td>\n",
       "      <td>46.184643</td>\n",
       "      <td>54.937241</td>\n",
       "      <td>48.101952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.933179</td>\n",
       "      <td>23.564522</td>\n",
       "      <td>27.106476</td>\n",
       "      <td>34.083599</td>\n",
       "      <td>40.123753</td>\n",
       "      <td>46.818756</td>\n",
       "      <td>48.211014</td>\n",
       "      <td>52.424007</td>\n",
       "      <td>44.252968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.595832</td>\n",
       "      <td>23.525238</td>\n",
       "      <td>27.695932</td>\n",
       "      <td>34.207943</td>\n",
       "      <td>40.672436</td>\n",
       "      <td>46.953598</td>\n",
       "      <td>47.202095</td>\n",
       "      <td>54.056042</td>\n",
       "      <td>43.680439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.052801</td>\n",
       "      <td>20.788586</td>\n",
       "      <td>24.202517</td>\n",
       "      <td>33.770672</td>\n",
       "      <td>40.556259</td>\n",
       "      <td>44.222900</td>\n",
       "      <td>45.273987</td>\n",
       "      <td>49.765533</td>\n",
       "      <td>40.468128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.307910</td>\n",
       "      <td>22.070257</td>\n",
       "      <td>22.992020</td>\n",
       "      <td>31.718880</td>\n",
       "      <td>38.420731</td>\n",
       "      <td>43.048779</td>\n",
       "      <td>46.573029</td>\n",
       "      <td>49.319912</td>\n",
       "      <td>43.181316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.386799</td>\n",
       "      <td>17.109886</td>\n",
       "      <td>18.613644</td>\n",
       "      <td>27.182665</td>\n",
       "      <td>33.809341</td>\n",
       "      <td>36.194267</td>\n",
       "      <td>37.068016</td>\n",
       "      <td>43.525822</td>\n",
       "      <td>35.373138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.341166</td>\n",
       "      <td>17.097147</td>\n",
       "      <td>19.044359</td>\n",
       "      <td>27.815908</td>\n",
       "      <td>34.457695</td>\n",
       "      <td>37.228348</td>\n",
       "      <td>37.777649</td>\n",
       "      <td>44.046543</td>\n",
       "      <td>35.037739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.992326</td>\n",
       "      <td>12.698444</td>\n",
       "      <td>13.493099</td>\n",
       "      <td>21.197771</td>\n",
       "      <td>28.091124</td>\n",
       "      <td>29.247869</td>\n",
       "      <td>29.449989</td>\n",
       "      <td>35.414227</td>\n",
       "      <td>27.080708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.490984</td>\n",
       "      <td>12.275197</td>\n",
       "      <td>11.782765</td>\n",
       "      <td>17.574757</td>\n",
       "      <td>23.512476</td>\n",
       "      <td>24.347723</td>\n",
       "      <td>26.034878</td>\n",
       "      <td>32.816158</td>\n",
       "      <td>25.993265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.306039</td>\n",
       "      <td>6.689831</td>\n",
       "      <td>5.561443</td>\n",
       "      <td>8.514796</td>\n",
       "      <td>13.675077</td>\n",
       "      <td>12.262683</td>\n",
       "      <td>14.678414</td>\n",
       "      <td>20.573616</td>\n",
       "      <td>18.334465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.968652</td>\n",
       "      <td>2.324132</td>\n",
       "      <td>2.782275</td>\n",
       "      <td>4.464105</td>\n",
       "      <td>9.279276</td>\n",
       "      <td>8.664996</td>\n",
       "      <td>11.268911</td>\n",
       "      <td>15.204332</td>\n",
       "      <td>21.181234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.179626</td>\n",
       "      <td>-0.052789</td>\n",
       "      <td>-0.023637</td>\n",
       "      <td>-0.009943</td>\n",
       "      <td>-0.069079</td>\n",
       "      <td>-0.090186</td>\n",
       "      <td>0.077987</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.284979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.175473</td>\n",
       "      <td>-0.046592</td>\n",
       "      <td>-0.023861</td>\n",
       "      <td>-0.009941</td>\n",
       "      <td>-0.068247</td>\n",
       "      <td>-0.089190</td>\n",
       "      <td>0.078448</td>\n",
       "      <td>-0.006294</td>\n",
       "      <td>0.271031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.158388</td>\n",
       "      <td>-0.021038</td>\n",
       "      <td>-0.024097</td>\n",
       "      <td>-0.011575</td>\n",
       "      <td>-0.070216</td>\n",
       "      <td>-0.092954</td>\n",
       "      <td>0.075498</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.021409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.156579</td>\n",
       "      <td>-0.019526</td>\n",
       "      <td>-0.024113</td>\n",
       "      <td>-0.011613</td>\n",
       "      <td>-0.069649</td>\n",
       "      <td>-0.092414</td>\n",
       "      <td>0.075924</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.021475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.154233</td>\n",
       "      <td>-0.010161</td>\n",
       "      <td>-0.025375</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>-0.070820</td>\n",
       "      <td>-0.094498</td>\n",
       "      <td>0.073322</td>\n",
       "      <td>0.011418</td>\n",
       "      <td>0.006815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.152328</td>\n",
       "      <td>-0.008550</td>\n",
       "      <td>-0.025361</td>\n",
       "      <td>-0.013297</td>\n",
       "      <td>-0.070254</td>\n",
       "      <td>-0.093906</td>\n",
       "      <td>0.073634</td>\n",
       "      <td>0.011639</td>\n",
       "      <td>0.006975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.133629</td>\n",
       "      <td>-0.007770</td>\n",
       "      <td>-0.026685</td>\n",
       "      <td>-0.015860</td>\n",
       "      <td>-0.059461</td>\n",
       "      <td>-0.090951</td>\n",
       "      <td>0.072812</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>0.019690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.136170</td>\n",
       "      <td>-0.006754</td>\n",
       "      <td>-0.026593</td>\n",
       "      <td>-0.015729</td>\n",
       "      <td>-0.058731</td>\n",
       "      <td>-0.090233</td>\n",
       "      <td>0.073226</td>\n",
       "      <td>0.018314</td>\n",
       "      <td>0.021452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.092927</td>\n",
       "      <td>-0.007522</td>\n",
       "      <td>-0.026346</td>\n",
       "      <td>-0.026505</td>\n",
       "      <td>-0.043280</td>\n",
       "      <td>-0.073578</td>\n",
       "      <td>0.064432</td>\n",
       "      <td>0.034553</td>\n",
       "      <td>0.035402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.094239</td>\n",
       "      <td>-0.006003</td>\n",
       "      <td>-0.025941</td>\n",
       "      <td>-0.024076</td>\n",
       "      <td>-0.041546</td>\n",
       "      <td>-0.071455</td>\n",
       "      <td>0.065942</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>0.037669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.068762</td>\n",
       "      <td>-0.000687</td>\n",
       "      <td>-0.015398</td>\n",
       "      <td>-0.007464</td>\n",
       "      <td>-0.048291</td>\n",
       "      <td>-0.076988</td>\n",
       "      <td>0.059209</td>\n",
       "      <td>0.022795</td>\n",
       "      <td>0.046146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.068997</td>\n",
       "      <td>-0.000290</td>\n",
       "      <td>-0.015567</td>\n",
       "      <td>-0.007402</td>\n",
       "      <td>-0.047907</td>\n",
       "      <td>-0.077154</td>\n",
       "      <td>0.059813</td>\n",
       "      <td>0.023091</td>\n",
       "      <td>0.046815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.081831</td>\n",
       "      <td>-0.009206</td>\n",
       "      <td>-0.019093</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>-0.041316</td>\n",
       "      <td>-0.051198</td>\n",
       "      <td>0.063016</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.081831</td>\n",
       "      <td>-0.009206</td>\n",
       "      <td>-0.019093</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>-0.041316</td>\n",
       "      <td>-0.051198</td>\n",
       "      <td>0.063016</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1          1          1          1          1          1  \\\n",
       "0   -0.196271  -0.074919  -0.056662  -0.061915  -0.116000  -0.119493   \n",
       "1   -0.198651  -0.076534  -0.058552  -0.064448  -0.118259  -0.122723   \n",
       "2   -0.202375  -0.086865  -0.072739  -0.064942  -0.117579  -0.122366   \n",
       "3   -0.200499  -0.086200  -0.071209  -0.062759  -0.115310  -0.120421   \n",
       "4   -0.202891  -0.094238  -0.083316  -0.059101  -0.108575  -0.117404   \n",
       "5   -0.206415  -0.097278  -0.086791  -0.062549  -0.113868  -0.124359   \n",
       "6   -0.208896  -0.107616  -0.100869  -0.060932  -0.110093  -0.128710   \n",
       "7   -0.218464  -0.111346  -0.107530  -0.062085  -0.117532  -0.130268   \n",
       "8   -0.228049  -0.113947  -0.118512  -0.062370  -0.113471  -0.121565   \n",
       "9   -0.234109  -0.115230  -0.115352  -0.064615  -0.117477  -0.118512   \n",
       "10  -0.241534  -0.118842  -0.109815  -0.061406  -0.113161  -0.098839   \n",
       "11  -0.243413  -0.119305  -0.110137  -0.060455  -0.113392  -0.094318   \n",
       "12  -0.244977  -0.121861  -0.106654  -0.044627  -0.091494  -0.070870   \n",
       "13  -0.246698  -0.122062  -0.106909  -0.042980  -0.086609  -0.066291   \n",
       "14  -0.241465  -0.112306  -0.102834  -0.029472  -0.055447  -0.049305   \n",
       "15  -0.106694   0.698537   1.460856   2.321656   3.709854   3.764926   \n",
       "16   0.164094   3.540992   1.968588   4.585263   7.734929   9.836046   \n",
       "17   1.581352   6.593179   4.424622   7.695791  11.603088  13.169626   \n",
       "18   7.584140  14.748760  13.863460  17.585978  22.878448  25.932512   \n",
       "19   8.712058  17.394629  16.259808  19.121983  25.245377  28.832279   \n",
       "20   8.296864  19.077950  23.913515  24.385073  31.848423  36.024605   \n",
       "21   8.116883  18.579971  23.625925  26.859409  33.521236  36.184830   \n",
       "22  10.945988  24.939468  29.423132  33.494698  40.755520  46.624065   \n",
       "23  11.328506  23.567522  27.106218  31.375063  38.331802  44.219810   \n",
       "24  10.933179  23.564522  27.106476  34.083599  40.123753  46.818756   \n",
       "25   9.595832  23.525238  27.695932  34.207943  40.672436  46.953598   \n",
       "26   9.052801  20.788586  24.202517  33.770672  40.556259  44.222900   \n",
       "27  10.307910  22.070257  22.992020  31.718880  38.420731  43.048779   \n",
       "28   7.386799  17.109886  18.613644  27.182665  33.809341  36.194267   \n",
       "29   7.341166  17.097147  19.044359  27.815908  34.457695  37.228348   \n",
       "30   4.992326  12.698444  13.493099  21.197771  28.091124  29.247869   \n",
       "31   4.490984  12.275197  11.782765  17.574757  23.512476  24.347723   \n",
       "32   2.306039   6.689831   5.561443   8.514796  13.675077  12.262683   \n",
       "33   0.968652   2.324132   2.782275   4.464105   9.279276   8.664996   \n",
       "34  -0.179626  -0.052789  -0.023637  -0.009943  -0.069079  -0.090186   \n",
       "35  -0.175473  -0.046592  -0.023861  -0.009941  -0.068247  -0.089190   \n",
       "36  -0.158388  -0.021038  -0.024097  -0.011575  -0.070216  -0.092954   \n",
       "37  -0.156579  -0.019526  -0.024113  -0.011613  -0.069649  -0.092414   \n",
       "38  -0.154233  -0.010161  -0.025375  -0.013346  -0.070820  -0.094498   \n",
       "39  -0.152328  -0.008550  -0.025361  -0.013297  -0.070254  -0.093906   \n",
       "40  -0.133629  -0.007770  -0.026685  -0.015860  -0.059461  -0.090951   \n",
       "41  -0.136170  -0.006754  -0.026593  -0.015729  -0.058731  -0.090233   \n",
       "42  -0.092927  -0.007522  -0.026346  -0.026505  -0.043280  -0.073578   \n",
       "43  -0.094239  -0.006003  -0.025941  -0.024076  -0.041546  -0.071455   \n",
       "44  -0.068762  -0.000687  -0.015398  -0.007464  -0.048291  -0.076988   \n",
       "45  -0.068997  -0.000290  -0.015567  -0.007402  -0.047907  -0.077154   \n",
       "46  -0.081831  -0.009206  -0.019093   0.023726  -0.041316  -0.051198   \n",
       "47  -0.081831  -0.009206  -0.019093   0.023726  -0.041316  -0.051198   \n",
       "\n",
       "            1          1          1  \n",
       "0    0.061687   0.089609   0.069562  \n",
       "1    0.059422   0.087269   0.069196  \n",
       "2    0.062973   0.080645   0.059048  \n",
       "3    0.063775   0.082415   0.058865  \n",
       "4    0.062292   0.069193   0.049584  \n",
       "5    0.060374   0.067520   0.048884  \n",
       "6    0.056342   0.064198   0.038742  \n",
       "7    0.049430   0.059209   0.038563  \n",
       "8    0.054826   0.045247   0.024571  \n",
       "9    0.061561   0.045332   0.021873  \n",
       "10   0.092695   0.040764   0.012497  \n",
       "11   0.098351   0.040380   0.011789  \n",
       "12   0.121295   0.043598   0.014249  \n",
       "13   0.126661   0.043961   0.013151  \n",
       "14   0.143061   0.045988   0.071569  \n",
       "15   6.132225  10.367605  15.144733  \n",
       "16  13.956335  19.377384  22.624846  \n",
       "17  16.849009  22.955547  26.913954  \n",
       "18  29.993963  38.716805  38.109646  \n",
       "19  31.794573  37.603577  36.304626  \n",
       "20  35.846745  46.183586  38.748909  \n",
       "21  35.999237  41.905807  37.807621  \n",
       "22  47.247871  57.867039  49.945633  \n",
       "23  46.184643  54.937241  48.101952  \n",
       "24  48.211014  52.424007  44.252968  \n",
       "25  47.202095  54.056042  43.680439  \n",
       "26  45.273987  49.765533  40.468128  \n",
       "27  46.573029  49.319912  43.181316  \n",
       "28  37.068016  43.525822  35.373138  \n",
       "29  37.777649  44.046543  35.037739  \n",
       "30  29.449989  35.414227  27.080708  \n",
       "31  26.034878  32.816158  25.993265  \n",
       "32  14.678414  20.573616  18.334465  \n",
       "33  11.268911  15.204332  21.181234  \n",
       "34   0.077987  -0.005164   0.284979  \n",
       "35   0.078448  -0.006294   0.271031  \n",
       "36   0.075498   0.003987   0.021409  \n",
       "37   0.075924   0.004993   0.021475  \n",
       "38   0.073322   0.011418   0.006815  \n",
       "39   0.073634   0.011639   0.006975  \n",
       "40   0.072812   0.017525   0.019690  \n",
       "41   0.073226   0.018314   0.021452  \n",
       "42   0.064432   0.034553   0.035402  \n",
       "43   0.065942   0.034688   0.037669  \n",
       "44   0.059209   0.022795   0.046146  \n",
       "45   0.059813   0.023091   0.046815  \n",
       "46   0.063016   0.017415   0.064400  \n",
       "47   0.063016   0.017415   0.064400  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1][:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34889\n",
      "Early stopping, best iteration is:\n",
      "[418]\tvalid_0's quantile: 1.34812\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.14466\n",
      "[1000]\tvalid_0's quantile: 2.13764\n",
      "[1500]\tvalid_0's quantile: 2.13582\n",
      "[2000]\tvalid_0's quantile: 2.1334\n",
      "Early stopping, best iteration is:\n",
      "[1749]\tvalid_0's quantile: 2.13312\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.53565\n",
      "[1000]\tvalid_0's quantile: 2.50726\n",
      "[1500]\tvalid_0's quantile: 2.49216\n",
      "Early stopping, best iteration is:\n",
      "[1604]\tvalid_0's quantile: 2.48959\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.66191\n",
      "[1000]\tvalid_0's quantile: 2.62846\n",
      "[1500]\tvalid_0's quantile: 2.61266\n",
      "[2000]\tvalid_0's quantile: 2.6059\n",
      "[2500]\tvalid_0's quantile: 2.59923\n",
      "[3000]\tvalid_0's quantile: 2.59644\n",
      "Early stopping, best iteration is:\n",
      "[2707]\tvalid_0's quantile: 2.59598\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.55537\n",
      "[1000]\tvalid_0's quantile: 2.54183\n",
      "[1500]\tvalid_0's quantile: 2.52395\n",
      "[2000]\tvalid_0's quantile: 2.5191\n",
      "[2500]\tvalid_0's quantile: 2.51605\n",
      "[3000]\tvalid_0's quantile: 2.51386\n",
      "[3500]\tvalid_0's quantile: 2.50859\n",
      "[4000]\tvalid_0's quantile: 2.50447\n",
      "[4500]\tvalid_0's quantile: 2.50256\n",
      "[5000]\tvalid_0's quantile: 2.50036\n",
      "[5500]\tvalid_0's quantile: 2.498\n",
      "[6000]\tvalid_0's quantile: 2.49693\n",
      "[6500]\tvalid_0's quantile: 2.49595\n",
      "Early stopping, best iteration is:\n",
      "[6563]\tvalid_0's quantile: 2.49582\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.28838\n",
      "[1000]\tvalid_0's quantile: 2.26798\n",
      "[1500]\tvalid_0's quantile: 2.25775\n",
      "[2000]\tvalid_0's quantile: 2.25273\n",
      "[2500]\tvalid_0's quantile: 2.24936\n",
      "[3000]\tvalid_0's quantile: 2.24642\n",
      "[3500]\tvalid_0's quantile: 2.24451\n",
      "[4000]\tvalid_0's quantile: 2.24427\n",
      "Early stopping, best iteration is:\n",
      "[3735]\tvalid_0's quantile: 2.24391\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.87856\n",
      "[1000]\tvalid_0's quantile: 1.86236\n",
      "[1500]\tvalid_0's quantile: 1.85737\n",
      "[2000]\tvalid_0's quantile: 1.85491\n",
      "Early stopping, best iteration is:\n",
      "[1840]\tvalid_0's quantile: 1.85404\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34949\n",
      "[1000]\tvalid_0's quantile: 1.34333\n",
      "Early stopping, best iteration is:\n",
      "[898]\tvalid_0's quantile: 1.34271\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.748569\n",
      "[1000]\tvalid_0's quantile: 0.746232\n",
      "[1500]\tvalid_0's quantile: 0.745479\n",
      "Early stopping, best iteration is:\n",
      "[1284]\tvalid_0's quantile: 0.745153\n",
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.39392\n",
      "Early stopping, best iteration is:\n",
      "[331]\tvalid_0's quantile: 1.3933\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.19349\n",
      "Early stopping, best iteration is:\n",
      "[616]\tvalid_0's quantile: 2.18579\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.61892\n",
      "[1000]\tvalid_0's quantile: 2.57944\n",
      "[1500]\tvalid_0's quantile: 2.578\n",
      "Early stopping, best iteration is:\n",
      "[1380]\tvalid_0's quantile: 2.57786\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.74331\n",
      "[1000]\tvalid_0's quantile: 2.70696\n",
      "[1500]\tvalid_0's quantile: 2.70093\n",
      "[2000]\tvalid_0's quantile: 2.69465\n",
      "[2500]\tvalid_0's quantile: 2.68928\n",
      "[3000]\tvalid_0's quantile: 2.68205\n",
      "[3500]\tvalid_0's quantile: 2.67219\n",
      "[4000]\tvalid_0's quantile: 2.66516\n",
      "[4500]\tvalid_0's quantile: 2.6608\n",
      "Early stopping, best iteration is:\n",
      "[4475]\tvalid_0's quantile: 2.6608\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.6458\n",
      "[1000]\tvalid_0's quantile: 2.62814\n",
      "[1500]\tvalid_0's quantile: 2.61626\n",
      "[2000]\tvalid_0's quantile: 2.60023\n",
      "[2500]\tvalid_0's quantile: 2.58706\n",
      "[3000]\tvalid_0's quantile: 2.58403\n",
      "Early stopping, best iteration is:\n",
      "[2807]\tvalid_0's quantile: 2.58403\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.36786\n",
      "[1000]\tvalid_0's quantile: 2.35343\n",
      "[1500]\tvalid_0's quantile: 2.33476\n",
      "[2000]\tvalid_0's quantile: 2.32426\n",
      "[2500]\tvalid_0's quantile: 2.31984\n",
      "[3000]\tvalid_0's quantile: 2.31736\n",
      "Early stopping, best iteration is:\n",
      "[2871]\tvalid_0's quantile: 2.31729\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.96038\n",
      "[1000]\tvalid_0's quantile: 1.93778\n",
      "[1500]\tvalid_0's quantile: 1.92682\n",
      "[2000]\tvalid_0's quantile: 1.91583\n",
      "[2500]\tvalid_0's quantile: 1.91245\n",
      "Early stopping, best iteration is:\n",
      "[2560]\tvalid_0's quantile: 1.91213\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.40785\n",
      "[1000]\tvalid_0's quantile: 1.39944\n",
      "[1500]\tvalid_0's quantile: 1.39438\n",
      "[2000]\tvalid_0's quantile: 1.39224\n",
      "[2500]\tvalid_0's quantile: 1.39124\n",
      "Early stopping, best iteration is:\n",
      "[2308]\tvalid_0's quantile: 1.39107\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.780449\n",
      "[1000]\tvalid_0's quantile: 0.777177\n",
      "[1500]\tvalid_0's quantile: 0.775629\n",
      "[2000]\tvalid_0's quantile: 0.77524\n",
      "Early stopping, best iteration is:\n",
      "[1831]\tvalid_0's quantile: 0.774899\n"
     ]
    }
   ],
   "source": [
    "# -NET\n",
    "def LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "    \n",
    "    # (a) Modeling  \n",
    "    model = LGBMRegressor(objective='quantile', alpha=q,\n",
    "                         n_estimators=10000, bagging_fraction=0.7, learning_rate=0.027, subsample=0.7)                   \n",
    "                         \n",
    "                         \n",
    "    model.fit(X_train, Y_train, eval_metric = ['quantile'], \n",
    "          eval_set=[(X_valid, Y_valid)], early_stopping_rounds=300, verbose=500)\n",
    "\n",
    "    # (b) Predictions\n",
    "    pred = pd.Series(model.predict(X_test).round(2))\n",
    "    return pred, model\n",
    "\n",
    "def train_data(X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "\n",
    "    LGBM_models=[]\n",
    "    LGBM_actual_pred = pd.DataFrame()\n",
    "\n",
    "    for q in q_lst:\n",
    "        print(q)\n",
    "        pred , model = LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test)\n",
    "        LGBM_models.append(model)\n",
    "        LGBM_actual_pred = pd.concat([LGBM_actual_pred,pred],axis=1)\n",
    "\n",
    "    LGBM_actual_pred.columns=q_lst\n",
    "    \n",
    "    return LGBM_models, LGBM_actual_pred\n",
    "\n",
    "models_1, results_1 = train_data(X_train_1, Y_train_1, X_valid_1, Y_valid_1, df_test)\n",
    "models_2, results_2 = train_data(X_train_2, Y_train_2, X_valid_2, Y_valid_2, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45\n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51\n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11\n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34\n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51\n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17\n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03\n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17\n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82\n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14\n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61\n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92\n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20\n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73\n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94\n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20\n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59\n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49\n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13\n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_1[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>18.24</td>\n",
       "      <td>25.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.74</td>\n",
       "      <td>14.78</td>\n",
       "      <td>19.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>22.60</td>\n",
       "      <td>20.57</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.66</td>\n",
       "      <td>32.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.47</td>\n",
       "      <td>19.06</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.62</td>\n",
       "      <td>20.20</td>\n",
       "      <td>17.15</td>\n",
       "      <td>22.85</td>\n",
       "      <td>33.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.33</td>\n",
       "      <td>24.78</td>\n",
       "      <td>24.52</td>\n",
       "      <td>23.02</td>\n",
       "      <td>28.58</td>\n",
       "      <td>28.55</td>\n",
       "      <td>25.06</td>\n",
       "      <td>26.39</td>\n",
       "      <td>27.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.26</td>\n",
       "      <td>26.20</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.70</td>\n",
       "      <td>29.22</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.05</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.96</td>\n",
       "      <td>29.89</td>\n",
       "      <td>33.58</td>\n",
       "      <td>33.28</td>\n",
       "      <td>38.13</td>\n",
       "      <td>35.95</td>\n",
       "      <td>34.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.24</td>\n",
       "      <td>24.45</td>\n",
       "      <td>29.77</td>\n",
       "      <td>29.36</td>\n",
       "      <td>34.56</td>\n",
       "      <td>30.81</td>\n",
       "      <td>26.23</td>\n",
       "      <td>27.65</td>\n",
       "      <td>34.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.30</td>\n",
       "      <td>25.22</td>\n",
       "      <td>31.45</td>\n",
       "      <td>29.95</td>\n",
       "      <td>34.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>29.10</td>\n",
       "      <td>27.16</td>\n",
       "      <td>31.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.03</td>\n",
       "      <td>30.02</td>\n",
       "      <td>34.72</td>\n",
       "      <td>32.49</td>\n",
       "      <td>35.12</td>\n",
       "      <td>35.82</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.21</td>\n",
       "      <td>27.84</td>\n",
       "      <td>33.51</td>\n",
       "      <td>32.70</td>\n",
       "      <td>37.10</td>\n",
       "      <td>34.75</td>\n",
       "      <td>29.17</td>\n",
       "      <td>27.61</td>\n",
       "      <td>29.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.50</td>\n",
       "      <td>22.99</td>\n",
       "      <td>30.11</td>\n",
       "      <td>31.82</td>\n",
       "      <td>32.80</td>\n",
       "      <td>33.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>36.02</td>\n",
       "      <td>44.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.97</td>\n",
       "      <td>21.03</td>\n",
       "      <td>25.88</td>\n",
       "      <td>24.37</td>\n",
       "      <td>28.26</td>\n",
       "      <td>26.10</td>\n",
       "      <td>21.49</td>\n",
       "      <td>20.61</td>\n",
       "      <td>26.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>22.61</td>\n",
       "      <td>27.12</td>\n",
       "      <td>27.53</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.98</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.92</td>\n",
       "      <td>25.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.57</td>\n",
       "      <td>17.69</td>\n",
       "      <td>21.43</td>\n",
       "      <td>22.13</td>\n",
       "      <td>22.79</td>\n",
       "      <td>24.23</td>\n",
       "      <td>23.32</td>\n",
       "      <td>21.47</td>\n",
       "      <td>23.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.19</td>\n",
       "      <td>15.84</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.86</td>\n",
       "      <td>15.93</td>\n",
       "      <td>16.22</td>\n",
       "      <td>15.55</td>\n",
       "      <td>15.47</td>\n",
       "      <td>24.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.95</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>15.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>13.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "15   0.48   0.70   1.80   2.18   0.98   3.56   2.99   6.80   9.87\n",
       "16   2.67   7.45   8.62  10.08  10.06  11.52  17.00  19.79  20.92\n",
       "17   3.80   8.55   8.15  10.49  10.84  12.03  15.05  18.24  25.56\n",
       "18   7.74  14.78  19.70  18.93  22.60  20.57  16.19  15.66  32.96\n",
       "19   9.47  19.06  21.52  21.87  23.62  20.20  17.15  22.85  33.97\n",
       "20  14.33  24.78  24.52  23.02  28.58  28.55  25.06  26.39  27.24\n",
       "21  15.26  26.20  24.57  24.70  29.22  31.11  31.05  31.59  32.45\n",
       "22  15.96  29.89  33.58  33.28  38.13  35.95  34.24  33.24  33.78\n",
       "23  14.24  24.45  29.77  29.36  34.56  30.81  26.23  27.65  34.53\n",
       "24  14.30  25.22  31.45  29.95  34.70  31.31  29.10  27.16  31.12\n",
       "25  16.03  30.02  34.72  32.49  35.12  35.82  33.60  32.73  34.11\n",
       "26  15.21  27.84  33.51  32.70  37.10  34.75  29.17  27.61  29.70\n",
       "27   9.50  22.99  30.11  31.82  32.80  33.88  19.43  36.02  44.45\n",
       "28  10.97  21.03  25.88  24.37  28.26  26.10  21.49  20.61  26.21\n",
       "29  12.63  22.61  27.12  27.53  31.02  29.98  27.37  22.92  25.82\n",
       "30   8.57  17.69  21.43  22.13  22.79  24.23  23.32  21.47  23.67\n",
       "31   7.19  15.84  18.83  17.86  15.93  16.22  15.55  15.47  24.20\n",
       "32   2.95   8.16   8.37   7.24   6.39   6.81   6.33   6.51  15.83\n",
       "33   1.71   4.94   4.62   3.24   3.42   1.19   3.41  -0.50  13.80\n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17\n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17\n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4362 - val_loss: 1.6001\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4215 - val_loss: 1.5991\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4143 - val_loss: 1.5978\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4161 - val_loss: 1.5971\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4239 - val_loss: 1.6024\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4159 - val_loss: 1.6165\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4175 - val_loss: 1.6275\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4330 - val_loss: 1.5965\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4209 - val_loss: 1.5990\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4345 - val_loss: 1.5939\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4041 - val_loss: 1.6000\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4041 - val_loss: 1.6084\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4135 - val_loss: 1.5891\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3899 - val_loss: 1.5889\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4230 - val_loss: 1.5853\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4160 - val_loss: 1.5842\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4118 - val_loss: 1.5912\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4093 - val_loss: 1.5960\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4069 - val_loss: 1.5816\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4042 - val_loss: 1.5771\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4120 - val_loss: 1.5848\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3806 - val_loss: 1.5800\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3786 - val_loss: 1.5923\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4045 - val_loss: 1.5988\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3833 - val_loss: 1.5754\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3782 - val_loss: 1.5685\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3886 - val_loss: 1.5940\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3836 - val_loss: 1.6098\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3705 - val_loss: 1.5726\n",
      "Epoch 30/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3791 - val_loss: 1.5708\n",
      "Epoch 31/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3888 - val_loss: 1.5656\n",
      "Epoch 32/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3862 - val_loss: 1.5860\n",
      "Epoch 33/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3670 - val_loss: 1.5631\n",
      "Epoch 34/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3695 - val_loss: 1.6042\n",
      "Epoch 35/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3752 - val_loss: 1.5629\n",
      "Epoch 36/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3901 - val_loss: 1.5750\n",
      "Epoch 37/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3723 - val_loss: 1.5831\n",
      "Epoch 38/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3998 - val_loss: 1.5752\n",
      "Epoch 39/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3893 - val_loss: 1.5884\n",
      "Epoch 40/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3791 - val_loss: 1.5749\n",
      "Epoch 00040: early stopping\n",
      "0.2\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2100 - val_loss: 2.5246\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.1883 - val_loss: 2.5494\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1822 - val_loss: 2.5312\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1692 - val_loss: 2.5392\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2000 - val_loss: 2.5430\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.1941 - val_loss: 2.5752\n",
      "Epoch 00006: early stopping\n",
      "0.3\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6144 - val_loss: 2.9748\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5884 - val_loss: 3.0102\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5833 - val_loss: 2.9824\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5583 - val_loss: 3.0067\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5997 - val_loss: 3.0038\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5966 - val_loss: 2.9884\n",
      "Epoch 00006: early stopping\n",
      "0.4\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7111 - val_loss: 3.0767\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6837 - val_loss: 3.0657\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6844 - val_loss: 3.1184\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6633 - val_loss: 3.1004\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6946 - val_loss: 3.1293\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6963 - val_loss: 3.0627\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7166 - val_loss: 3.0647\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7458 - val_loss: 3.0826\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6774 - val_loss: 3.0688\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7409 - val_loss: 3.0505\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6196 - val_loss: 3.0427\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6236 - val_loss: 3.0816\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6741 - val_loss: 3.0896\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6740 - val_loss: 3.1414\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6805 - val_loss: 3.0867\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6702 - val_loss: 3.1019\n",
      "Epoch 00016: early stopping\n",
      "0.5\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5746 - val_loss: 2.9060\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5356 - val_loss: 2.9305\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5381 - val_loss: 2.9402\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5295 - val_loss: 2.9512\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5487 - val_loss: 2.9732\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5591 - val_loss: 2.9765\n",
      "Epoch 00006: early stopping\n",
      "0.6\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2885 - val_loss: 2.6033\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2533 - val_loss: 2.6425\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2570 - val_loss: 2.6475\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2519 - val_loss: 2.6502\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2571 - val_loss: 2.6604\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2838 - val_loss: 2.6047\n",
      "Epoch 00006: early stopping\n",
      "0.7\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8741 - val_loss: 2.1431\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8510 - val_loss: 2.1834\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8588 - val_loss: 2.1971\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8568 - val_loss: 2.1538\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8524 - val_loss: 2.1844\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8717 - val_loss: 2.1307\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8831 - val_loss: 2.1272\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9120 - val_loss: 2.1280\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8543 - val_loss: 2.1766\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8962 - val_loss: 2.1079\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8134 - val_loss: 2.1077\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8286 - val_loss: 2.1607\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8550 - val_loss: 2.1547\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8457 - val_loss: 2.1464\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8524 - val_loss: 2.1381\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8387 - val_loss: 2.1124\n",
      "Epoch 00016: early stopping\n",
      "0.8\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3688 - val_loss: 1.5684\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3486 - val_loss: 1.5508\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3545 - val_loss: 1.5782\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3516 - val_loss: 1.5948\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3465 - val_loss: 1.6018\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3593 - val_loss: 1.5580\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3629 - val_loss: 1.5442\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3894 - val_loss: 1.5548\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3461 - val_loss: 1.5696\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3743 - val_loss: 1.5237\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3142 - val_loss: 1.5352\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3351 - val_loss: 1.5619\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3438 - val_loss: 1.5699\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3478 - val_loss: 1.5539\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3446 - val_loss: 1.5744\n",
      "Epoch 00015: early stopping\n",
      "0.9\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7722 - val_loss: 0.8642\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7505 - val_loss: 0.8689\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7526 - val_loss: 0.8901\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7517 - val_loss: 0.8858\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7508 - val_loss: 0.8571\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7548 - val_loss: 0.8568\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7601 - val_loss: 0.8587\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7689 - val_loss: 0.8907\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7414 - val_loss: 0.8599\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7636 - val_loss: 0.8530\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7340 - val_loss: 0.8731\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7430 - val_loss: 0.8462\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7513 - val_loss: 0.8647\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7477 - val_loss: 0.8677\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7353 - val_loss: 0.8856\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7418 - val_loss: 0.8651\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7519 - val_loss: 0.8505\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000981</td>\n",
       "      <td>-0.001112</td>\n",
       "      <td>-0.003431</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.009807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000987</td>\n",
       "      <td>-0.001112</td>\n",
       "      <td>-0.003427</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.000765</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.009963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001030</td>\n",
       "      <td>-0.001076</td>\n",
       "      <td>-0.003394</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>-0.000702</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.007079</td>\n",
       "      <td>0.009766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001021</td>\n",
       "      <td>-0.001075</td>\n",
       "      <td>-0.003396</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>-0.000704</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.009648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001060</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>-0.003368</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.006880</td>\n",
       "      <td>0.009285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001070</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>-0.003364</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.006928</td>\n",
       "      <td>0.009577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001092</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.003370</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>-0.000590</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.006778</td>\n",
       "      <td>0.009381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001129</td>\n",
       "      <td>-0.001039</td>\n",
       "      <td>-0.003381</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.000596</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>0.009999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001150</td>\n",
       "      <td>-0.001033</td>\n",
       "      <td>-0.003411</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.006851</td>\n",
       "      <td>0.010179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001160</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.003422</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>-0.000616</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>0.010612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001171</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>-0.003446</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>-0.000622</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>0.010867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001157</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>-0.000649</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.007134</td>\n",
       "      <td>0.011068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001166</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.010918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001153</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>-0.003459</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>0.011068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001185</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.003457</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.010632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.277814</td>\n",
       "      <td>1.892144</td>\n",
       "      <td>2.336471</td>\n",
       "      <td>1.575912</td>\n",
       "      <td>2.553114</td>\n",
       "      <td>2.984629</td>\n",
       "      <td>3.406966</td>\n",
       "      <td>4.161805</td>\n",
       "      <td>6.066886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.596247</td>\n",
       "      <td>3.369259</td>\n",
       "      <td>4.079478</td>\n",
       "      <td>4.579057</td>\n",
       "      <td>9.994737</td>\n",
       "      <td>9.321777</td>\n",
       "      <td>10.112558</td>\n",
       "      <td>10.987202</td>\n",
       "      <td>18.135616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.211020</td>\n",
       "      <td>5.926815</td>\n",
       "      <td>7.992480</td>\n",
       "      <td>7.936148</td>\n",
       "      <td>13.576334</td>\n",
       "      <td>12.762942</td>\n",
       "      <td>13.297204</td>\n",
       "      <td>14.384868</td>\n",
       "      <td>22.066248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.901821</td>\n",
       "      <td>12.924910</td>\n",
       "      <td>17.783718</td>\n",
       "      <td>18.130297</td>\n",
       "      <td>27.299345</td>\n",
       "      <td>25.516918</td>\n",
       "      <td>26.384823</td>\n",
       "      <td>28.608940</td>\n",
       "      <td>35.918919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.352436</td>\n",
       "      <td>15.851948</td>\n",
       "      <td>22.318243</td>\n",
       "      <td>21.294773</td>\n",
       "      <td>30.413143</td>\n",
       "      <td>28.398863</td>\n",
       "      <td>27.846165</td>\n",
       "      <td>28.063604</td>\n",
       "      <td>35.780533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9.193002</td>\n",
       "      <td>23.371794</td>\n",
       "      <td>32.200249</td>\n",
       "      <td>30.389294</td>\n",
       "      <td>41.278748</td>\n",
       "      <td>39.592838</td>\n",
       "      <td>36.769352</td>\n",
       "      <td>34.676987</td>\n",
       "      <td>35.993568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.419835</td>\n",
       "      <td>25.274857</td>\n",
       "      <td>34.501099</td>\n",
       "      <td>29.641644</td>\n",
       "      <td>41.238434</td>\n",
       "      <td>39.195076</td>\n",
       "      <td>39.053970</td>\n",
       "      <td>37.391430</td>\n",
       "      <td>36.896603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11.597852</td>\n",
       "      <td>26.911787</td>\n",
       "      <td>35.883839</td>\n",
       "      <td>40.150242</td>\n",
       "      <td>46.861118</td>\n",
       "      <td>47.006824</td>\n",
       "      <td>46.319504</td>\n",
       "      <td>45.059067</td>\n",
       "      <td>44.990307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.878353</td>\n",
       "      <td>24.271925</td>\n",
       "      <td>32.472130</td>\n",
       "      <td>37.289196</td>\n",
       "      <td>45.583008</td>\n",
       "      <td>45.176064</td>\n",
       "      <td>43.066113</td>\n",
       "      <td>40.579834</td>\n",
       "      <td>45.016575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9.353634</td>\n",
       "      <td>24.289660</td>\n",
       "      <td>32.193962</td>\n",
       "      <td>39.438850</td>\n",
       "      <td>43.642975</td>\n",
       "      <td>43.210911</td>\n",
       "      <td>43.463104</td>\n",
       "      <td>41.606277</td>\n",
       "      <td>44.647137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11.158473</td>\n",
       "      <td>26.093935</td>\n",
       "      <td>35.025955</td>\n",
       "      <td>40.263733</td>\n",
       "      <td>44.579262</td>\n",
       "      <td>43.642223</td>\n",
       "      <td>44.915600</td>\n",
       "      <td>42.534332</td>\n",
       "      <td>42.818630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.957242</td>\n",
       "      <td>23.937160</td>\n",
       "      <td>31.833084</td>\n",
       "      <td>37.620693</td>\n",
       "      <td>42.272728</td>\n",
       "      <td>40.863369</td>\n",
       "      <td>41.424412</td>\n",
       "      <td>38.954784</td>\n",
       "      <td>40.320183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.720070</td>\n",
       "      <td>20.529909</td>\n",
       "      <td>26.604097</td>\n",
       "      <td>34.318192</td>\n",
       "      <td>38.466068</td>\n",
       "      <td>37.695938</td>\n",
       "      <td>36.600182</td>\n",
       "      <td>36.595123</td>\n",
       "      <td>42.898113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.572620</td>\n",
       "      <td>21.122602</td>\n",
       "      <td>27.036594</td>\n",
       "      <td>31.454798</td>\n",
       "      <td>36.539165</td>\n",
       "      <td>35.983421</td>\n",
       "      <td>34.843071</td>\n",
       "      <td>33.443188</td>\n",
       "      <td>36.650894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.247270</td>\n",
       "      <td>21.408068</td>\n",
       "      <td>27.884686</td>\n",
       "      <td>31.638470</td>\n",
       "      <td>37.100254</td>\n",
       "      <td>36.360622</td>\n",
       "      <td>35.378487</td>\n",
       "      <td>33.911995</td>\n",
       "      <td>36.392975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7.515310</td>\n",
       "      <td>19.388920</td>\n",
       "      <td>25.034458</td>\n",
       "      <td>25.827593</td>\n",
       "      <td>31.677567</td>\n",
       "      <td>30.303867</td>\n",
       "      <td>30.240147</td>\n",
       "      <td>29.785484</td>\n",
       "      <td>31.811010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.194511</td>\n",
       "      <td>16.528847</td>\n",
       "      <td>21.256250</td>\n",
       "      <td>22.645445</td>\n",
       "      <td>26.458563</td>\n",
       "      <td>26.706375</td>\n",
       "      <td>26.602577</td>\n",
       "      <td>26.533855</td>\n",
       "      <td>29.762812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.708854</td>\n",
       "      <td>10.607141</td>\n",
       "      <td>11.801850</td>\n",
       "      <td>10.594795</td>\n",
       "      <td>12.137362</td>\n",
       "      <td>11.613779</td>\n",
       "      <td>17.240088</td>\n",
       "      <td>16.600706</td>\n",
       "      <td>20.455534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.933818</td>\n",
       "      <td>3.994242</td>\n",
       "      <td>5.192430</td>\n",
       "      <td>4.989479</td>\n",
       "      <td>4.710385</td>\n",
       "      <td>4.882284</td>\n",
       "      <td>8.434773</td>\n",
       "      <td>7.784291</td>\n",
       "      <td>14.034584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>-0.001158</td>\n",
       "      <td>-0.004102</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.006858</td>\n",
       "      <td>0.009213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.001517</td>\n",
       "      <td>-0.001153</td>\n",
       "      <td>-0.004093</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>-0.001347</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.009151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.001559</td>\n",
       "      <td>-0.001242</td>\n",
       "      <td>-0.004246</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>-0.001540</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.006978</td>\n",
       "      <td>0.009242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.001553</td>\n",
       "      <td>-0.001240</td>\n",
       "      <td>-0.004238</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.001539</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.009205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.001601</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.004387</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>-0.001714</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.009308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.001598</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>-0.004379</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>-0.001713</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>0.009265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001830</td>\n",
       "      <td>-0.001399</td>\n",
       "      <td>-0.004536</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>0.009298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.001837</td>\n",
       "      <td>-0.001397</td>\n",
       "      <td>-0.004528</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>0.009306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.002006</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>-0.004745</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>-0.001951</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>0.009415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.001991</td>\n",
       "      <td>-0.001478</td>\n",
       "      <td>-0.004731</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>-0.001897</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.009381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.002123</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004917</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>-0.001143</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.009524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.002127</td>\n",
       "      <td>-0.001594</td>\n",
       "      <td>-0.004914</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>-0.001154</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.007451</td>\n",
       "      <td>0.009541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.002284</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.005119</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0.009784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.002284</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.005119</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>0.009784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0    0.000981  -0.001112  -0.003431   0.001779   0.000407  -0.000764   \n",
       "1    0.000987  -0.001112  -0.003427   0.001779   0.000406  -0.000765   \n",
       "2    0.001030  -0.001076  -0.003394   0.001791   0.000415  -0.000702   \n",
       "3    0.001021  -0.001075  -0.003396   0.001790   0.000415  -0.000704   \n",
       "4    0.001060  -0.001047  -0.003368   0.001801   0.000424  -0.000637   \n",
       "5    0.001070  -0.001047  -0.003364   0.001802   0.000422  -0.000637   \n",
       "6    0.001092  -0.001034  -0.003370   0.001812   0.000426  -0.000590   \n",
       "7    0.001129  -0.001039  -0.003381   0.001812   0.000416  -0.000596   \n",
       "8    0.001150  -0.001033  -0.003411   0.001813   0.000396  -0.000578   \n",
       "9    0.001160  -0.001034  -0.003422   0.001813   0.000374  -0.000616   \n",
       "10   0.001171  -0.001027  -0.003446   0.001812   0.000362  -0.000622   \n",
       "11   0.001157  -0.001027  -0.003450   0.001811   0.000352  -0.000649   \n",
       "12   0.001166  -0.001016  -0.003456   0.001812   0.000361  -0.000609   \n",
       "13   0.001153  -0.001015  -0.003459   0.001811   0.000353  -0.000638   \n",
       "14   0.001185  -0.001002  -0.003457   0.001814   0.000371  -0.000561   \n",
       "15   0.277814   1.892144   2.336471   1.575912   2.553114   2.984629   \n",
       "16   0.596247   3.369259   4.079478   4.579057   9.994737   9.321777   \n",
       "17   1.211020   5.926815   7.992480   7.936148  13.576334  12.762942   \n",
       "18   1.901821  12.924910  17.783718  18.130297  27.299345  25.516918   \n",
       "19   3.352436  15.851948  22.318243  21.294773  30.413143  28.398863   \n",
       "20   9.193002  23.371794  32.200249  30.389294  41.278748  39.592838   \n",
       "21  10.419835  25.274857  34.501099  29.641644  41.238434  39.195076   \n",
       "22  11.597852  26.911787  35.883839  40.150242  46.861118  47.006824   \n",
       "23   8.878353  24.271925  32.472130  37.289196  45.583008  45.176064   \n",
       "24   9.353634  24.289660  32.193962  39.438850  43.642975  43.210911   \n",
       "25  11.158473  26.093935  35.025955  40.263733  44.579262  43.642223   \n",
       "26   9.957242  23.937160  31.833084  37.620693  42.272728  40.863369   \n",
       "27   5.720070  20.529909  26.604097  34.318192  38.466068  37.695938   \n",
       "28   7.572620  21.122602  27.036594  31.454798  36.539165  35.983421   \n",
       "29   8.247270  21.408068  27.884686  31.638470  37.100254  36.360622   \n",
       "30   7.515310  19.388920  25.034458  25.827593  31.677567  30.303867   \n",
       "31   4.194511  16.528847  21.256250  22.645445  26.458563  26.706375   \n",
       "32   2.708854  10.607141  11.801850  10.594795  12.137362  11.613779   \n",
       "33   0.933818   3.994242   5.192430   4.989479   4.710385   4.882284   \n",
       "34   0.001524  -0.001158  -0.004102   0.001632   0.000532  -0.001348   \n",
       "35   0.001517  -0.001153  -0.004093   0.001631   0.000580  -0.001347   \n",
       "36   0.001559  -0.001242  -0.004246   0.001585   0.001066  -0.001540   \n",
       "37   0.001553  -0.001240  -0.004238   0.001585   0.001098  -0.001539   \n",
       "38   0.001601  -0.001333  -0.004387   0.001541   0.001612  -0.001714   \n",
       "39   0.001598  -0.001329  -0.004379   0.001540   0.001648  -0.001713   \n",
       "40   0.001830  -0.001399  -0.004536   0.001497   0.002168  -0.001880   \n",
       "41   0.001837  -0.001397  -0.004528   0.001497   0.002172  -0.001877   \n",
       "42   0.002006  -0.001485  -0.004745   0.001454   0.002677  -0.001951   \n",
       "43   0.001991  -0.001478  -0.004731   0.001454   0.002702  -0.001897   \n",
       "44   0.002123  -0.001596  -0.004917   0.001412   0.003144  -0.001143   \n",
       "45   0.002127  -0.001594  -0.004914   0.001412   0.003141  -0.001154   \n",
       "46   0.002284  -0.001711  -0.005119   0.001375   0.003522  -0.000528   \n",
       "47   0.002284  -0.001711  -0.005119   0.001375   0.003522  -0.000528   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.000766   0.007204   0.009807  \n",
       "1    0.000766   0.007228   0.009963  \n",
       "2    0.000758   0.007079   0.009766  \n",
       "3    0.000758   0.007068   0.009648  \n",
       "4    0.000749   0.006880   0.009285  \n",
       "5    0.000749   0.006928   0.009577  \n",
       "6    0.000741   0.006778   0.009381  \n",
       "7    0.000741   0.006861   0.009999  \n",
       "8    0.000735   0.006851   0.010179  \n",
       "9    0.000737   0.007040   0.010612  \n",
       "10   0.000734   0.007050   0.010867  \n",
       "11   0.000737   0.007134   0.011068  \n",
       "12   0.000725   0.007018   0.010918  \n",
       "13   0.000728   0.007132   0.011068  \n",
       "14   0.000711   0.006863   0.010632  \n",
       "15   3.406966   4.161805   6.066886  \n",
       "16  10.112558  10.987202  18.135616  \n",
       "17  13.297204  14.384868  22.066248  \n",
       "18  26.384823  28.608940  35.918919  \n",
       "19  27.846165  28.063604  35.780533  \n",
       "20  36.769352  34.676987  35.993568  \n",
       "21  39.053970  37.391430  36.896603  \n",
       "22  46.319504  45.059067  44.990307  \n",
       "23  43.066113  40.579834  45.016575  \n",
       "24  43.463104  41.606277  44.647137  \n",
       "25  44.915600  42.534332  42.818630  \n",
       "26  41.424412  38.954784  40.320183  \n",
       "27  36.600182  36.595123  42.898113  \n",
       "28  34.843071  33.443188  36.650894  \n",
       "29  35.378487  33.911995  36.392975  \n",
       "30  30.240147  29.785484  31.811010  \n",
       "31  26.602577  26.533855  29.762812  \n",
       "32  17.240088  16.600706  20.455534  \n",
       "33   8.434773   7.784291  14.034584  \n",
       "34   0.000726   0.006858   0.009213  \n",
       "35   0.000726   0.006836   0.009151  \n",
       "36   0.000744   0.006978   0.009242  \n",
       "37   0.000745   0.006967   0.009205  \n",
       "38   0.000759   0.007060   0.009308  \n",
       "39   0.000760   0.007044   0.009265  \n",
       "40   0.000779   0.007177   0.009298  \n",
       "41   0.000779   0.007185   0.009306  \n",
       "42   0.000798   0.007322   0.009415  \n",
       "43   0.000799   0.007314   0.009381  \n",
       "44   0.000816   0.007442   0.009524  \n",
       "45   0.000816   0.007451   0.009541  \n",
       "46   0.000828   0.007571   0.009784  \n",
       "47   0.000828   0.007571   0.009784  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + NET\n",
    "model7 = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model7.fit(np.array(Day).reshape(52464, 1, 8), np.array(Day7).reshape(52464, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred7 = np.squeeze(model7.predict(np.array(df_test).reshape(3888, 1, 8)))\n",
    "    pred7 = pd.DataFrame(pred7)\n",
    "    result7 = pd.concat([result7, pred7], axis=1)\n",
    "    \n",
    "result7[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4553 - val_loss: 1.6462\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4443 - val_loss: 1.6418\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4341 - val_loss: 1.6406\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4258 - val_loss: 1.6402\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4430 - val_loss: 1.6403\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4354 - val_loss: 1.6465\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4442 - val_loss: 1.6470\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4518 - val_loss: 1.6387\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4510 - val_loss: 1.6402\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4608 - val_loss: 1.6362\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4260 - val_loss: 1.6435\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4377 - val_loss: 1.6352\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4409 - val_loss: 1.6301\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4207 - val_loss: 1.6457\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4469 - val_loss: 1.6394\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4347 - val_loss: 1.6250\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4252 - val_loss: 1.6342\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4378 - val_loss: 1.6246\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4291 - val_loss: 1.6205\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4309 - val_loss: 1.6245\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4264 - val_loss: 1.6151\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4156 - val_loss: 1.6207\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4098 - val_loss: 1.6326\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4128 - val_loss: 1.6119\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4264 - val_loss: 1.6120\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4146 - val_loss: 1.6108\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4221 - val_loss: 1.6319\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4099 - val_loss: 1.6338\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4002 - val_loss: 1.6119\n",
      "Epoch 30/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4241 - val_loss: 1.6182\n",
      "Epoch 31/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4254 - val_loss: 1.6064\n",
      "Epoch 32/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4121 - val_loss: 1.6163\n",
      "Epoch 33/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4110 - val_loss: 1.6053\n",
      "Epoch 34/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4029 - val_loss: 1.6017\n",
      "Epoch 35/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3962 - val_loss: 1.5981\n",
      "Epoch 36/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4126 - val_loss: 1.6160\n",
      "Epoch 37/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4070 - val_loss: 1.5941\n",
      "Epoch 38/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4125 - val_loss: 1.6176\n",
      "Epoch 39/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4220 - val_loss: 1.5947\n",
      "Epoch 40/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4270 - val_loss: 1.6122\n",
      "Epoch 41/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3895 - val_loss: 1.5906\n",
      "Epoch 42/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4004 - val_loss: 1.5957\n",
      "Epoch 43/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4056 - val_loss: 1.5969\n",
      "Epoch 44/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3873 - val_loss: 1.5985\n",
      "Epoch 45/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3929 - val_loss: 1.5989\n",
      "Epoch 46/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4071 - val_loss: 1.5941\n",
      "Epoch 00046: early stopping\n",
      "0.2\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3298 - val_loss: 2.6311\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3131 - val_loss: 2.6248\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2973 - val_loss: 2.6428\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2902 - val_loss: 2.6341\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3056 - val_loss: 2.6205\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3005 - val_loss: 2.6175\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3137 - val_loss: 2.6740\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3079 - val_loss: 2.6671\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3123 - val_loss: 2.6227\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3247 - val_loss: 2.6255\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2722 - val_loss: 2.6352\n",
      "Epoch 00011: early stopping\n",
      "0.3\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7776 - val_loss: 3.1015\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7544 - val_loss: 3.1530\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7517 - val_loss: 3.1907\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7396 - val_loss: 3.1167\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7579 - val_loss: 3.2194\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7472 - val_loss: 3.1124\n",
      "Epoch 00006: early stopping\n",
      "0.4\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8948 - val_loss: 3.2353\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8829 - val_loss: 3.2863\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8707 - val_loss: 3.2912\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8730 - val_loss: 3.2549\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8847 - val_loss: 3.3260\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8586 - val_loss: 3.2342\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8968 - val_loss: 3.2521\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8885 - val_loss: 3.2336\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8717 - val_loss: 3.2246\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.9004 - val_loss: 3.2170\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8295 - val_loss: 3.2606\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8507 - val_loss: 3.2584\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8616 - val_loss: 3.2600\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8350 - val_loss: 3.2876\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8584 - val_loss: 3.2422\n",
      "Epoch 00015: early stopping\n",
      "0.5\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7301 - val_loss: 3.0739\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7233 - val_loss: 3.0800\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7260 - val_loss: 3.0978\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7274 - val_loss: 3.1748\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7421 - val_loss: 3.1169\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6985 - val_loss: 3.0875\n",
      "Epoch 00006: early stopping\n",
      "0.6\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4140 - val_loss: 2.7751\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4128 - val_loss: 2.7858\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4055 - val_loss: 2.7868\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4099 - val_loss: 2.8709\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4278 - val_loss: 2.7454\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3798 - val_loss: 2.7474\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4221 - val_loss: 2.7260\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4262 - val_loss: 2.7517\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3968 - val_loss: 2.7271\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4236 - val_loss: 2.7392\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3661 - val_loss: 2.7305\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3928 - val_loss: 2.7609\n",
      "Epoch 00012: early stopping\n",
      "0.7\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9729 - val_loss: 2.2369\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9688 - val_loss: 2.2480\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9624 - val_loss: 2.2988\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9682 - val_loss: 2.2231\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9611 - val_loss: 2.2692\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9410 - val_loss: 2.2507\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9718 - val_loss: 2.2282\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9768 - val_loss: 2.2994\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9402 - val_loss: 2.2229\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9771 - val_loss: 2.2203\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9159 - val_loss: 2.2176\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9485 - val_loss: 2.2902\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9554 - val_loss: 2.3068\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9574 - val_loss: 2.2351\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9424 - val_loss: 2.2214\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9369 - val_loss: 2.2866\n",
      "Epoch 00016: early stopping\n",
      "0.8\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4308 - val_loss: 1.6161\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4252 - val_loss: 1.6431\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4228 - val_loss: 1.6441\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4246 - val_loss: 1.6006\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4228 - val_loss: 1.6054\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4102 - val_loss: 1.6172\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4294 - val_loss: 1.7122\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4351 - val_loss: 1.6359\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4092 - val_loss: 1.6353\n",
      "Epoch 00009: early stopping\n",
      "0.9\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.8098 - val_loss: 0.8836\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7891 - val_loss: 0.9302\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7972 - val_loss: 0.8952\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7926 - val_loss: 0.8881\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7894 - val_loss: 0.8792\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7793 - val_loss: 0.8826\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7961 - val_loss: 0.8878\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7980 - val_loss: 0.9001\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7885 - val_loss: 0.8863\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7938 - val_loss: 0.8768\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7712 - val_loss: 0.8931\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7810 - val_loss: 0.9228\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7833 - val_loss: 0.9065\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7888 - val_loss: 0.8966\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7798 - val_loss: 0.9093\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.838833</td>\n",
       "      <td>1.872875</td>\n",
       "      <td>2.652690</td>\n",
       "      <td>2.563535</td>\n",
       "      <td>3.177830</td>\n",
       "      <td>3.518117</td>\n",
       "      <td>5.366771</td>\n",
       "      <td>8.707663</td>\n",
       "      <td>12.951659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.659178</td>\n",
       "      <td>2.218027</td>\n",
       "      <td>3.535518</td>\n",
       "      <td>7.064044</td>\n",
       "      <td>8.324739</td>\n",
       "      <td>9.510727</td>\n",
       "      <td>9.623656</td>\n",
       "      <td>15.889924</td>\n",
       "      <td>21.976980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.154090</td>\n",
       "      <td>4.446464</td>\n",
       "      <td>7.885289</td>\n",
       "      <td>11.133677</td>\n",
       "      <td>12.242085</td>\n",
       "      <td>13.344284</td>\n",
       "      <td>14.042044</td>\n",
       "      <td>19.296478</td>\n",
       "      <td>27.853649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.796607</td>\n",
       "      <td>13.101029</td>\n",
       "      <td>16.827868</td>\n",
       "      <td>21.542751</td>\n",
       "      <td>22.255865</td>\n",
       "      <td>22.897476</td>\n",
       "      <td>25.232986</td>\n",
       "      <td>31.414845</td>\n",
       "      <td>35.042206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.000425</td>\n",
       "      <td>16.992016</td>\n",
       "      <td>20.815216</td>\n",
       "      <td>23.695372</td>\n",
       "      <td>25.245981</td>\n",
       "      <td>25.295961</td>\n",
       "      <td>26.926001</td>\n",
       "      <td>30.770344</td>\n",
       "      <td>35.032894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.770158</td>\n",
       "      <td>29.347290</td>\n",
       "      <td>32.824226</td>\n",
       "      <td>30.754402</td>\n",
       "      <td>34.677612</td>\n",
       "      <td>35.002193</td>\n",
       "      <td>35.364227</td>\n",
       "      <td>36.518063</td>\n",
       "      <td>36.113331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.987258</td>\n",
       "      <td>31.861113</td>\n",
       "      <td>34.652355</td>\n",
       "      <td>33.377571</td>\n",
       "      <td>36.516121</td>\n",
       "      <td>36.604343</td>\n",
       "      <td>35.560852</td>\n",
       "      <td>35.962517</td>\n",
       "      <td>36.012573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16.833771</td>\n",
       "      <td>32.910103</td>\n",
       "      <td>38.616417</td>\n",
       "      <td>37.945419</td>\n",
       "      <td>43.584106</td>\n",
       "      <td>42.591648</td>\n",
       "      <td>44.448170</td>\n",
       "      <td>44.689697</td>\n",
       "      <td>46.872456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.152499</td>\n",
       "      <td>29.521324</td>\n",
       "      <td>35.072994</td>\n",
       "      <td>34.740162</td>\n",
       "      <td>40.514679</td>\n",
       "      <td>38.977818</td>\n",
       "      <td>40.616646</td>\n",
       "      <td>41.249439</td>\n",
       "      <td>43.017323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.795631</td>\n",
       "      <td>29.979588</td>\n",
       "      <td>36.091236</td>\n",
       "      <td>36.952869</td>\n",
       "      <td>42.332359</td>\n",
       "      <td>39.241123</td>\n",
       "      <td>42.435589</td>\n",
       "      <td>40.727562</td>\n",
       "      <td>42.913898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.285149</td>\n",
       "      <td>31.480104</td>\n",
       "      <td>37.289249</td>\n",
       "      <td>38.622551</td>\n",
       "      <td>43.767998</td>\n",
       "      <td>41.183220</td>\n",
       "      <td>44.403770</td>\n",
       "      <td>41.979027</td>\n",
       "      <td>42.494213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.405815</td>\n",
       "      <td>28.466372</td>\n",
       "      <td>33.665218</td>\n",
       "      <td>37.585602</td>\n",
       "      <td>41.441113</td>\n",
       "      <td>38.648064</td>\n",
       "      <td>42.140896</td>\n",
       "      <td>40.371284</td>\n",
       "      <td>41.045647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.765247</td>\n",
       "      <td>26.021435</td>\n",
       "      <td>32.091324</td>\n",
       "      <td>37.436367</td>\n",
       "      <td>41.520302</td>\n",
       "      <td>36.551182</td>\n",
       "      <td>40.000031</td>\n",
       "      <td>45.034145</td>\n",
       "      <td>41.852371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.712000</td>\n",
       "      <td>24.234007</td>\n",
       "      <td>28.552221</td>\n",
       "      <td>33.281284</td>\n",
       "      <td>36.159477</td>\n",
       "      <td>32.052452</td>\n",
       "      <td>34.736275</td>\n",
       "      <td>36.647259</td>\n",
       "      <td>33.935818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.106812</td>\n",
       "      <td>24.641754</td>\n",
       "      <td>28.730062</td>\n",
       "      <td>33.377098</td>\n",
       "      <td>36.338745</td>\n",
       "      <td>32.804916</td>\n",
       "      <td>35.413300</td>\n",
       "      <td>36.570621</td>\n",
       "      <td>33.593460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9.659010</td>\n",
       "      <td>20.125034</td>\n",
       "      <td>22.298351</td>\n",
       "      <td>27.305052</td>\n",
       "      <td>29.413151</td>\n",
       "      <td>27.096096</td>\n",
       "      <td>29.118158</td>\n",
       "      <td>28.623482</td>\n",
       "      <td>27.624105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8.562587</td>\n",
       "      <td>19.017601</td>\n",
       "      <td>21.093645</td>\n",
       "      <td>25.944487</td>\n",
       "      <td>27.885679</td>\n",
       "      <td>23.950676</td>\n",
       "      <td>26.104979</td>\n",
       "      <td>27.582188</td>\n",
       "      <td>24.860441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.412905</td>\n",
       "      <td>13.301614</td>\n",
       "      <td>12.671147</td>\n",
       "      <td>14.601254</td>\n",
       "      <td>15.103045</td>\n",
       "      <td>15.467596</td>\n",
       "      <td>15.584498</td>\n",
       "      <td>16.800259</td>\n",
       "      <td>15.927103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.444198</td>\n",
       "      <td>5.961945</td>\n",
       "      <td>4.434711</td>\n",
       "      <td>8.473547</td>\n",
       "      <td>8.328035</td>\n",
       "      <td>9.349297</td>\n",
       "      <td>6.975707</td>\n",
       "      <td>11.073524</td>\n",
       "      <td>11.855604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.002400</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.013456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "1    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "2    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "3    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "4    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "5    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "6    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "7    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "8    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "9    0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "10   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "11   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "12   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "13   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "14   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "15   0.838833   1.872875   2.652690   2.563535   3.177830   3.518117   \n",
       "16   0.659178   2.218027   3.535518   7.064044   8.324739   9.510727   \n",
       "17   1.154090   4.446464   7.885289  11.133677  12.242085  13.344284   \n",
       "18   3.796607  13.101029  16.827868  21.542751  22.255865  22.897476   \n",
       "19   6.000425  16.992016  20.815216  23.695372  25.245981  25.295961   \n",
       "20  13.770158  29.347290  32.824226  30.754402  34.677612  35.002193   \n",
       "21  15.987258  31.861113  34.652355  33.377571  36.516121  36.604343   \n",
       "22  16.833771  32.910103  38.616417  37.945419  43.584106  42.591648   \n",
       "23  14.152499  29.521324  35.072994  34.740162  40.514679  38.977818   \n",
       "24  14.795631  29.979588  36.091236  36.952869  42.332359  39.241123   \n",
       "25  16.285149  31.480104  37.289249  38.622551  43.767998  41.183220   \n",
       "26  14.405815  28.466372  33.665218  37.585602  41.441113  38.648064   \n",
       "27  11.765247  26.021435  32.091324  37.436367  41.520302  36.551182   \n",
       "28  11.712000  24.234007  28.552221  33.281284  36.159477  32.052452   \n",
       "29  12.106812  24.641754  28.730062  33.377098  36.338745  32.804916   \n",
       "30   9.659010  20.125034  22.298351  27.305052  29.413151  27.096096   \n",
       "31   8.562587  19.017601  21.093645  25.944487  27.885679  23.950676   \n",
       "32   5.412905  13.301614  12.671147  14.601254  15.103045  15.467596   \n",
       "33   1.444198   5.961945   4.434711   8.473547   8.328035   9.349297   \n",
       "34   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "35   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "36   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "37   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "38   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "39   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "40   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "41   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "42   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "43   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "44   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "45   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "46   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "47   0.000177  -0.005943  -0.002400  -0.001468   0.000945   0.000029   \n",
       "\n",
       "            0          0          0  \n",
       "0   -0.001986   0.005396   0.013456  \n",
       "1   -0.001986   0.005396   0.013456  \n",
       "2   -0.001986   0.005396   0.013456  \n",
       "3   -0.001986   0.005396   0.013456  \n",
       "4   -0.001986   0.005396   0.013456  \n",
       "5   -0.001986   0.005396   0.013456  \n",
       "6   -0.001986   0.005396   0.013456  \n",
       "7   -0.001986   0.005396   0.013456  \n",
       "8   -0.001986   0.005396   0.013456  \n",
       "9   -0.001986   0.005396   0.013456  \n",
       "10  -0.001986   0.005396   0.013456  \n",
       "11  -0.001986   0.005396   0.013456  \n",
       "12  -0.001986   0.005396   0.013456  \n",
       "13  -0.001986   0.005396   0.013456  \n",
       "14  -0.001986   0.005396   0.013456  \n",
       "15   5.366771   8.707663  12.951659  \n",
       "16   9.623656  15.889924  21.976980  \n",
       "17  14.042044  19.296478  27.853649  \n",
       "18  25.232986  31.414845  35.042206  \n",
       "19  26.926001  30.770344  35.032894  \n",
       "20  35.364227  36.518063  36.113331  \n",
       "21  35.560852  35.962517  36.012573  \n",
       "22  44.448170  44.689697  46.872456  \n",
       "23  40.616646  41.249439  43.017323  \n",
       "24  42.435589  40.727562  42.913898  \n",
       "25  44.403770  41.979027  42.494213  \n",
       "26  42.140896  40.371284  41.045647  \n",
       "27  40.000031  45.034145  41.852371  \n",
       "28  34.736275  36.647259  33.935818  \n",
       "29  35.413300  36.570621  33.593460  \n",
       "30  29.118158  28.623482  27.624105  \n",
       "31  26.104979  27.582188  24.860441  \n",
       "32  15.584498  16.800259  15.927103  \n",
       "33   6.975707  11.073524  11.855604  \n",
       "34  -0.001986   0.005396   0.013456  \n",
       "35  -0.001986   0.005396   0.013456  \n",
       "36  -0.001986   0.005396   0.013456  \n",
       "37  -0.001986   0.005396   0.013456  \n",
       "38  -0.001986   0.005396   0.013456  \n",
       "39  -0.001986   0.005396   0.013456  \n",
       "40  -0.001986   0.005396   0.013456  \n",
       "41  -0.001986   0.005396   0.013456  \n",
       "42  -0.001986   0.005396   0.013456  \n",
       "43  -0.001986   0.005396   0.013456  \n",
       "44  -0.001986   0.005396   0.013456  \n",
       "45  -0.001986   0.005396   0.013456  \n",
       "46  -0.001986   0.005396   0.013456  \n",
       "47  -0.001986   0.005396   0.013456  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8 = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model8.fit(np.array(Day).reshape(52464, 1, 8), np.array(Day8).reshape(52464, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred8 = np.squeeze(model8.predict(np.array(df_test).reshape(3888, 1, 8)))\n",
    "    pred8 = pd.DataFrame(pred8)\n",
    "    result8 = pd.concat([result8, pred8], axis=1)\n",
    "    \n",
    "result8[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.GRU(units=64, return_sequences=True, input_shape=[39348, 8]),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(np.array(X_train_1).reshape(39348, 1, 8), np.array(Y_train_1).reshape(39348, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "pred = np.squeeze(model8.predict(np.array(X_valid_1).reshape(13116, 1, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "815/820 [============================>.] - ETA: 0s - loss: 1.5211WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 5s 3ms/step - loss: 1.5208 - val_loss: 1.6270\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4344 - val_loss: 1.6103\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4214 - val_loss: 1.6054\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4114 - val_loss: 1.5974\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4156 - val_loss: 1.5884\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4008 - val_loss: 1.6034\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4022 - val_loss: 1.5911\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4172 - val_loss: 1.5799\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3998 - val_loss: 1.5808\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4212 - val_loss: 1.5819\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3878 - val_loss: 1.5795\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3832 - val_loss: 1.5766\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3919 - val_loss: 1.6015\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3805 - val_loss: 1.5766\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4032 - val_loss: 1.5807\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3991 - val_loss: 1.5771\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3916 - val_loss: 1.5730\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3909 - val_loss: 1.5745\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3906 - val_loss: 1.5928\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3957 - val_loss: 1.5733\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4017 - val_loss: 1.6191\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3752 - val_loss: 1.5724\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3753 - val_loss: 1.6120\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3891 - val_loss: 1.5789\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3793 - val_loss: 1.5851\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3669 - val_loss: 1.5696\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3823 - val_loss: 1.5699\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3768 - val_loss: 1.5828\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3656 - val_loss: 1.5713\n",
      "Epoch 30/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3751 - val_loss: 1.5795\n",
      "Epoch 31/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3841 - val_loss: 1.5671\n",
      "Epoch 32/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3839 - val_loss: 1.5766\n",
      "Epoch 33/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3652 - val_loss: 1.5705\n",
      "Epoch 34/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3686 - val_loss: 1.5980\n",
      "Epoch 35/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3723 - val_loss: 1.5651\n",
      "Epoch 36/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3893 - val_loss: 1.5672\n",
      "Epoch 37/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3732 - val_loss: 1.5814\n",
      "Epoch 38/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4000 - val_loss: 1.5683\n",
      "Epoch 39/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3907 - val_loss: 1.5693\n",
      "Epoch 40/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3782 - val_loss: 1.5692\n",
      "Epoch 00040: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "812/820 [============================>.] - ETA: 0s - loss: 2.2401WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.2401 - val_loss: 2.5556\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2139 - val_loss: 2.5411\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2085 - val_loss: 2.5354\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1857 - val_loss: 2.5410\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2102 - val_loss: 2.5281\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1925 - val_loss: 2.5642\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2044 - val_loss: 2.5274\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2222 - val_loss: 2.5392\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1919 - val_loss: 2.5291\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2277 - val_loss: 2.5238\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1563 - val_loss: 2.5313\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1586 - val_loss: 2.5208\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1919 - val_loss: 2.5358\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1895 - val_loss: 2.5265\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1914 - val_loss: 2.5360\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1900 - val_loss: 2.5306\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1888 - val_loss: 2.5371\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 2.6305WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6305 - val_loss: 2.9986\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6031 - val_loss: 2.9877\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5995 - val_loss: 2.9991\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5707 - val_loss: 2.9971\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6129 - val_loss: 2.9779\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6055 - val_loss: 3.0341\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6078 - val_loss: 2.9769\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6413 - val_loss: 2.9844\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5880 - val_loss: 2.9757\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6480 - val_loss: 2.9739\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5403 - val_loss: 2.9737\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5462 - val_loss: 2.9702\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5899 - val_loss: 2.9846\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5922 - val_loss: 2.9776\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5845 - val_loss: 3.0358\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5817 - val_loss: 2.9712\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5886 - val_loss: 3.0161\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "800/820 [============================>.] - ETA: 0s - loss: 2.7290WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.7291 - val_loss: 3.0990\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7040 - val_loss: 3.0862\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6916 - val_loss: 3.0962\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6717 - val_loss: 3.0837\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7153 - val_loss: 3.1096\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7120 - val_loss: 3.0865\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7220 - val_loss: 3.0775\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7519 - val_loss: 3.0566\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6790 - val_loss: 3.0559\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7561 - val_loss: 3.0522\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6344 - val_loss: 3.0566\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6408 - val_loss: 3.0553\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6803 - val_loss: 3.0730\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6829 - val_loss: 3.0550\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6799 - val_loss: 3.0747\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "819/820 [============================>.] - ETA: 0s - loss: 2.6022WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6022 - val_loss: 2.9494\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5611 - val_loss: 2.9214\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5567 - val_loss: 2.9264\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5396 - val_loss: 2.9501\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5667 - val_loss: 2.9500\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5764 - val_loss: 2.9522\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5881 - val_loss: 2.8991\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6119 - val_loss: 2.9126\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5417 - val_loss: 2.9290\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6060 - val_loss: 2.8980\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4985 - val_loss: 2.9103\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5085 - val_loss: 2.8916\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5425 - val_loss: 2.9464\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5356 - val_loss: 2.9029\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5443 - val_loss: 2.9037\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5253 - val_loss: 2.9002\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5594 - val_loss: 2.9160\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "808/820 [============================>.] - ETA: 0s - loss: 2.2829WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.2829 - val_loss: 2.6288\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2534 - val_loss: 2.6022\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2569 - val_loss: 2.6000\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2452 - val_loss: 2.6415\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2586 - val_loss: 2.6187\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2694 - val_loss: 2.6235\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2854 - val_loss: 2.5803\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3111 - val_loss: 2.5954\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2444 - val_loss: 2.5844\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3052 - val_loss: 2.5784\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2060 - val_loss: 2.5871\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2169 - val_loss: 2.5925\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2550 - val_loss: 2.6692\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2419 - val_loss: 2.6016\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2397 - val_loss: 2.5896\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "812/820 [============================>.] - ETA: 0s - loss: 1.8577WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 1.8578 - val_loss: 2.1334\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8293 - val_loss: 2.1457\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8452 - val_loss: 2.1516\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8365 - val_loss: 2.1650\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8367 - val_loss: 2.1750\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8551 - val_loss: 2.1468\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "805/820 [============================>.] - ETA: 0s - loss: 1.3611WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.3611 - val_loss: 1.5351\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3273 - val_loss: 1.5448\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3476 - val_loss: 1.5545\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3381 - val_loss: 1.5882\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3398 - val_loss: 1.6165\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3536 - val_loss: 1.5728\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "815/820 [============================>.] - ETA: 0s - loss: 0.7579WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 6s 4ms/step - loss: 0.7579 - val_loss: 0.8550\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7437 - val_loss: 0.8649\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7505 - val_loss: 0.8536\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7472 - val_loss: 0.8764\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7449 - val_loss: 0.8749\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7519 - val_loss: 0.8692\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7575 - val_loss: 0.8570\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7598 - val_loss: 0.8570\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_input'), name='gru_input', description=\"created by layer 'gru_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004687</td>\n",
       "      <td>-0.004363</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>-0.001327</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.008211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005000</td>\n",
       "      <td>-0.004383</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>-0.001109</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004861</td>\n",
       "      <td>-0.005005</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>-0.000469</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.008420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004595</td>\n",
       "      <td>-0.004791</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>-0.000532</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.008217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004322</td>\n",
       "      <td>-0.005377</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.002515</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.008652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.004676</td>\n",
       "      <td>-0.005581</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>-0.000746</td>\n",
       "      <td>-0.001925</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.009237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004451</td>\n",
       "      <td>-0.006127</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>-0.001293</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>-0.002360</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>0.010883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.004600</td>\n",
       "      <td>-0.007874</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>-0.001656</td>\n",
       "      <td>-0.002441</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.012211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003826</td>\n",
       "      <td>-0.010022</td>\n",
       "      <td>-0.001114</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>-0.001245</td>\n",
       "      <td>-0.004002</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.015793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.003711</td>\n",
       "      <td>-0.011384</td>\n",
       "      <td>-0.002156</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>-0.001726</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.004375</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>0.016471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002774</td>\n",
       "      <td>-0.013201</td>\n",
       "      <td>-0.002616</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>-0.001831</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>-0.003283</td>\n",
       "      <td>0.006358</td>\n",
       "      <td>0.020646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002977</td>\n",
       "      <td>-0.013176</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.003154</td>\n",
       "      <td>-0.001783</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>-0.003474</td>\n",
       "      <td>0.006130</td>\n",
       "      <td>0.020226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001786</td>\n",
       "      <td>-0.014337</td>\n",
       "      <td>-0.002159</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>-0.002516</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>0.007260</td>\n",
       "      <td>0.024040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002067</td>\n",
       "      <td>-0.014332</td>\n",
       "      <td>-0.002159</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>-0.001987</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.023545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.016163</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>0.008415</td>\n",
       "      <td>-0.000866</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>0.028736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.196520</td>\n",
       "      <td>0.433772</td>\n",
       "      <td>0.949138</td>\n",
       "      <td>2.602817</td>\n",
       "      <td>2.749799</td>\n",
       "      <td>3.137343</td>\n",
       "      <td>4.599199</td>\n",
       "      <td>5.793758</td>\n",
       "      <td>10.337276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.155243</td>\n",
       "      <td>2.087821</td>\n",
       "      <td>2.706264</td>\n",
       "      <td>4.470170</td>\n",
       "      <td>8.093823</td>\n",
       "      <td>8.256138</td>\n",
       "      <td>16.028807</td>\n",
       "      <td>20.249676</td>\n",
       "      <td>29.208799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.523247</td>\n",
       "      <td>4.501017</td>\n",
       "      <td>7.342732</td>\n",
       "      <td>8.394903</td>\n",
       "      <td>11.737983</td>\n",
       "      <td>10.526447</td>\n",
       "      <td>18.361208</td>\n",
       "      <td>22.661707</td>\n",
       "      <td>32.273144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.400953</td>\n",
       "      <td>8.184858</td>\n",
       "      <td>14.163469</td>\n",
       "      <td>15.869653</td>\n",
       "      <td>21.940584</td>\n",
       "      <td>19.106361</td>\n",
       "      <td>31.808430</td>\n",
       "      <td>36.256374</td>\n",
       "      <td>44.152420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.456755</td>\n",
       "      <td>11.853502</td>\n",
       "      <td>18.130033</td>\n",
       "      <td>17.058529</td>\n",
       "      <td>22.592703</td>\n",
       "      <td>21.248283</td>\n",
       "      <td>32.123714</td>\n",
       "      <td>35.918018</td>\n",
       "      <td>43.151070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.404998</td>\n",
       "      <td>19.270964</td>\n",
       "      <td>23.780542</td>\n",
       "      <td>23.497410</td>\n",
       "      <td>30.438509</td>\n",
       "      <td>32.026566</td>\n",
       "      <td>37.192410</td>\n",
       "      <td>36.615826</td>\n",
       "      <td>39.635620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.067183</td>\n",
       "      <td>21.246008</td>\n",
       "      <td>25.557644</td>\n",
       "      <td>26.069136</td>\n",
       "      <td>33.591351</td>\n",
       "      <td>36.171242</td>\n",
       "      <td>40.062347</td>\n",
       "      <td>38.446075</td>\n",
       "      <td>38.851074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12.031821</td>\n",
       "      <td>22.503199</td>\n",
       "      <td>29.078463</td>\n",
       "      <td>30.453629</td>\n",
       "      <td>38.107117</td>\n",
       "      <td>38.258907</td>\n",
       "      <td>46.438362</td>\n",
       "      <td>46.043453</td>\n",
       "      <td>54.107361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.183082</td>\n",
       "      <td>18.990759</td>\n",
       "      <td>25.549398</td>\n",
       "      <td>27.608761</td>\n",
       "      <td>36.124695</td>\n",
       "      <td>34.355255</td>\n",
       "      <td>43.250286</td>\n",
       "      <td>43.459400</td>\n",
       "      <td>52.995789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9.804872</td>\n",
       "      <td>18.490503</td>\n",
       "      <td>24.644875</td>\n",
       "      <td>28.127367</td>\n",
       "      <td>36.597183</td>\n",
       "      <td>34.090473</td>\n",
       "      <td>40.664547</td>\n",
       "      <td>39.196209</td>\n",
       "      <td>49.077896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11.016861</td>\n",
       "      <td>20.876259</td>\n",
       "      <td>26.475733</td>\n",
       "      <td>29.272205</td>\n",
       "      <td>36.514874</td>\n",
       "      <td>36.777195</td>\n",
       "      <td>42.616138</td>\n",
       "      <td>40.919144</td>\n",
       "      <td>46.703388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.592793</td>\n",
       "      <td>18.230631</td>\n",
       "      <td>23.879011</td>\n",
       "      <td>27.716736</td>\n",
       "      <td>35.573135</td>\n",
       "      <td>33.768604</td>\n",
       "      <td>38.929638</td>\n",
       "      <td>36.712856</td>\n",
       "      <td>43.204292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.966336</td>\n",
       "      <td>13.542813</td>\n",
       "      <td>20.960894</td>\n",
       "      <td>27.542809</td>\n",
       "      <td>36.721863</td>\n",
       "      <td>33.787163</td>\n",
       "      <td>39.753044</td>\n",
       "      <td>40.551540</td>\n",
       "      <td>53.884148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.513880</td>\n",
       "      <td>14.492043</td>\n",
       "      <td>20.191967</td>\n",
       "      <td>24.687429</td>\n",
       "      <td>31.171419</td>\n",
       "      <td>29.656572</td>\n",
       "      <td>33.506741</td>\n",
       "      <td>32.332012</td>\n",
       "      <td>37.988602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.902815</td>\n",
       "      <td>15.137506</td>\n",
       "      <td>20.389429</td>\n",
       "      <td>24.513546</td>\n",
       "      <td>31.237070</td>\n",
       "      <td>29.946663</td>\n",
       "      <td>33.721508</td>\n",
       "      <td>32.043621</td>\n",
       "      <td>36.058998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6.801871</td>\n",
       "      <td>13.290809</td>\n",
       "      <td>17.668383</td>\n",
       "      <td>21.017509</td>\n",
       "      <td>26.051279</td>\n",
       "      <td>25.848503</td>\n",
       "      <td>28.099558</td>\n",
       "      <td>27.252434</td>\n",
       "      <td>28.707012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.663581</td>\n",
       "      <td>9.556680</td>\n",
       "      <td>14.984216</td>\n",
       "      <td>19.419327</td>\n",
       "      <td>22.806505</td>\n",
       "      <td>23.194220</td>\n",
       "      <td>24.387678</td>\n",
       "      <td>25.251835</td>\n",
       "      <td>30.306799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.023002</td>\n",
       "      <td>4.603984</td>\n",
       "      <td>8.919163</td>\n",
       "      <td>13.600180</td>\n",
       "      <td>14.116351</td>\n",
       "      <td>14.427646</td>\n",
       "      <td>13.772533</td>\n",
       "      <td>15.374203</td>\n",
       "      <td>21.081171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.369149</td>\n",
       "      <td>0.954422</td>\n",
       "      <td>2.104420</td>\n",
       "      <td>5.022897</td>\n",
       "      <td>6.834011</td>\n",
       "      <td>9.264600</td>\n",
       "      <td>8.436179</td>\n",
       "      <td>11.176038</td>\n",
       "      <td>20.914684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.012010</td>\n",
       "      <td>-0.015844</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.014173</td>\n",
       "      <td>-0.010442</td>\n",
       "      <td>0.013471</td>\n",
       "      <td>-0.002327</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.016110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.010780</td>\n",
       "      <td>-0.015063</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>-0.010471</td>\n",
       "      <td>0.012984</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>0.016097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.011452</td>\n",
       "      <td>-0.015002</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>-0.009256</td>\n",
       "      <td>0.011910</td>\n",
       "      <td>-0.002287</td>\n",
       "      <td>0.011867</td>\n",
       "      <td>0.012737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.010482</td>\n",
       "      <td>-0.014461</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>-0.009386</td>\n",
       "      <td>0.011379</td>\n",
       "      <td>-0.002214</td>\n",
       "      <td>0.011798</td>\n",
       "      <td>0.012634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.011298</td>\n",
       "      <td>-0.014676</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>-0.007840</td>\n",
       "      <td>0.010449</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.009986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.010249</td>\n",
       "      <td>-0.014129</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.006681</td>\n",
       "      <td>-0.007974</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.009882</td>\n",
       "      <td>0.009819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.011036</td>\n",
       "      <td>-0.014597</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>-0.006183</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>-0.002796</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.008106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.010160</td>\n",
       "      <td>-0.014164</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>-0.006520</td>\n",
       "      <td>0.008582</td>\n",
       "      <td>-0.002944</td>\n",
       "      <td>0.008187</td>\n",
       "      <td>0.008024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.011064</td>\n",
       "      <td>-0.014959</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>-0.004272</td>\n",
       "      <td>0.008289</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>0.007043</td>\n",
       "      <td>0.007233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.009667</td>\n",
       "      <td>-0.014256</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>-0.004515</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>0.007029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.010183</td>\n",
       "      <td>-0.015082</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>-0.002048</td>\n",
       "      <td>0.007534</td>\n",
       "      <td>-0.002490</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>0.007028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.010042</td>\n",
       "      <td>-0.014985</td>\n",
       "      <td>-0.000348</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>-0.002142</td>\n",
       "      <td>0.007483</td>\n",
       "      <td>-0.002516</td>\n",
       "      <td>0.006261</td>\n",
       "      <td>0.007072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.011291</td>\n",
       "      <td>-0.016256</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.008450</td>\n",
       "      <td>-0.001688</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.007955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.011291</td>\n",
       "      <td>-0.016256</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.008450</td>\n",
       "      <td>-0.001688</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.007955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0    0.004687  -0.004363   0.005436  -0.001327   0.002000   0.000578   \n",
       "1    0.005000  -0.004383   0.005192  -0.001109   0.002059   0.001268   \n",
       "2    0.004861  -0.005005   0.004361   0.000297   0.000408  -0.000454   \n",
       "3    0.004595  -0.004791   0.004693   0.000259   0.000636  -0.000532   \n",
       "4    0.004322  -0.005377   0.004034   0.001073  -0.000515  -0.002515   \n",
       "5    0.004676  -0.005581   0.003488   0.001255  -0.000746  -0.001925   \n",
       "6    0.004451  -0.006127   0.002628   0.001916  -0.001293  -0.003043   \n",
       "7    0.004600  -0.007874   0.000706   0.002031  -0.001656  -0.002441   \n",
       "8    0.003826  -0.010022  -0.001114   0.002588  -0.001600  -0.001245   \n",
       "9    0.003711  -0.011384  -0.002156   0.002659  -0.001726  -0.000128   \n",
       "10   0.002774  -0.013201  -0.002616   0.003294  -0.001831   0.003377   \n",
       "11   0.002977  -0.013176  -0.002686   0.003154  -0.001783   0.003565   \n",
       "12   0.001786  -0.014337  -0.002159   0.003635  -0.002516   0.006340   \n",
       "13   0.002067  -0.014332  -0.002159   0.003497  -0.002580   0.006494   \n",
       "14   0.000042  -0.016163  -0.001219   0.004168  -0.004439   0.008415   \n",
       "15   0.196520   0.433772   0.949138   2.602817   2.749799   3.137343   \n",
       "16   1.155243   2.087821   2.706264   4.470170   8.093823   8.256138   \n",
       "17   2.523247   4.501017   7.342732   8.394903  11.737983  10.526447   \n",
       "18   4.400953   8.184858  14.163469  15.869653  21.940584  19.106361   \n",
       "19   6.456755  11.853502  18.130033  17.058529  22.592703  21.248283   \n",
       "20  10.404998  19.270964  23.780542  23.497410  30.438509  32.026566   \n",
       "21  11.067183  21.246008  25.557644  26.069136  33.591351  36.171242   \n",
       "22  12.031821  22.503199  29.078463  30.453629  38.107117  38.258907   \n",
       "23  10.183082  18.990759  25.549398  27.608761  36.124695  34.355255   \n",
       "24   9.804872  18.490503  24.644875  28.127367  36.597183  34.090473   \n",
       "25  11.016861  20.876259  26.475733  29.272205  36.514874  36.777195   \n",
       "26   9.592793  18.230631  23.879011  27.716736  35.573135  33.768604   \n",
       "27   6.966336  13.542813  20.960894  27.542809  36.721863  33.787163   \n",
       "28   7.513880  14.492043  20.191967  24.687429  31.171419  29.656572   \n",
       "29   7.902815  15.137506  20.389429  24.513546  31.237070  29.946663   \n",
       "30   6.801871  13.290809  17.668383  21.017509  26.051279  25.848503   \n",
       "31   4.663581   9.556680  14.984216  19.419327  22.806505  23.194220   \n",
       "32   2.023002   4.603984   8.919163  13.600180  14.116351  14.427646   \n",
       "33   0.369149   0.954422   2.104420   5.022897   6.834011   9.264600   \n",
       "34  -0.012010  -0.015844   0.007668   0.014173  -0.010442   0.013471   \n",
       "35  -0.010780  -0.015063   0.006710   0.012531  -0.010471   0.012984   \n",
       "36  -0.011452  -0.015002   0.005635   0.010102  -0.009256   0.011910   \n",
       "37  -0.010482  -0.014461   0.005084   0.009185  -0.009386   0.011379   \n",
       "38  -0.011298  -0.014676   0.003854   0.007375  -0.007840   0.010449   \n",
       "39  -0.010249  -0.014129   0.003399   0.006681  -0.007974   0.009833   \n",
       "40  -0.011036  -0.014597   0.002132   0.005371  -0.006183   0.009107   \n",
       "41  -0.010160  -0.014164   0.002067   0.004926  -0.006520   0.008582   \n",
       "42  -0.011064  -0.014959   0.000795   0.004193  -0.004272   0.008289   \n",
       "43  -0.009667  -0.014256   0.000655   0.003744  -0.004515   0.007524   \n",
       "44  -0.010183  -0.015082  -0.000437   0.003480  -0.002048   0.007534   \n",
       "45  -0.010042  -0.014985  -0.000348   0.003411  -0.002142   0.007483   \n",
       "46  -0.011291  -0.016256  -0.001418   0.003507   0.000445   0.008450   \n",
       "47  -0.011291  -0.016256  -0.001418   0.003507   0.000445   0.008450   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.000604   0.001120   0.008211  \n",
       "1    0.000633   0.001313   0.008475  \n",
       "2   -0.000469   0.001341   0.008420  \n",
       "3   -0.000228   0.001228   0.008217  \n",
       "4   -0.001196   0.001387   0.008652  \n",
       "5   -0.001427   0.001871   0.009237  \n",
       "6   -0.002360   0.002682   0.010883  \n",
       "7   -0.003437   0.003469   0.012211  \n",
       "8   -0.004002   0.004862   0.015793  \n",
       "9   -0.004375   0.005038   0.016471  \n",
       "10  -0.003283   0.006358   0.020646  \n",
       "11  -0.003474   0.006130   0.020226  \n",
       "12  -0.001808   0.007260   0.024040  \n",
       "13  -0.001987   0.007046   0.023545  \n",
       "14  -0.000866   0.008271   0.028736  \n",
       "15   4.599199   5.793758  10.337276  \n",
       "16  16.028807  20.249676  29.208799  \n",
       "17  18.361208  22.661707  32.273144  \n",
       "18  31.808430  36.256374  44.152420  \n",
       "19  32.123714  35.918018  43.151070  \n",
       "20  37.192410  36.615826  39.635620  \n",
       "21  40.062347  38.446075  38.851074  \n",
       "22  46.438362  46.043453  54.107361  \n",
       "23  43.250286  43.459400  52.995789  \n",
       "24  40.664547  39.196209  49.077896  \n",
       "25  42.616138  40.919144  46.703388  \n",
       "26  38.929638  36.712856  43.204292  \n",
       "27  39.753044  40.551540  53.884148  \n",
       "28  33.506741  32.332012  37.988602  \n",
       "29  33.721508  32.043621  36.058998  \n",
       "30  28.099558  27.252434  28.707012  \n",
       "31  24.387678  25.251835  30.306799  \n",
       "32  13.772533  15.374203  21.081171  \n",
       "33   8.436179  11.176038  20.914684  \n",
       "34  -0.002327   0.013222   0.016110  \n",
       "35  -0.001932   0.013384   0.016097  \n",
       "36  -0.002287   0.011867   0.012737  \n",
       "37  -0.002214   0.011798   0.012634  \n",
       "38  -0.002566   0.010045   0.009986  \n",
       "39  -0.002592   0.009882   0.009819  \n",
       "40  -0.002796   0.008304   0.008106  \n",
       "41  -0.002944   0.008187   0.008024  \n",
       "42  -0.002797   0.007043   0.007233  \n",
       "43  -0.002933   0.006856   0.007029  \n",
       "44  -0.002490   0.006244   0.007028  \n",
       "45  -0.002516   0.006261   0.007072  \n",
       "46  -0.001688   0.006375   0.007955  \n",
       "47  -0.001688   0.006375   0.007955  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# +NET\n",
    "model_G7 = tf.keras.Sequential([\n",
    "    layers.GRU(units=64, return_sequences=True, input_shape=[52464, 8]),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_G7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_G7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_G7.fit(np.array(Day).reshape(52464, 1, 8), np.array(Day7).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_G7 = np.squeeze(model_G7.predict(np.array(df_test).reshape(3888, 1, 8)))\n",
    "    pred_G7 = pd.DataFrame(pred_G7)\n",
    "    result_G7 = pd.concat([result_G7, pred_G7], axis=1)\n",
    "    \n",
    "result_G7[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 1.5278WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.5275 - val_loss: 1.6281\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4352 - val_loss: 1.6098\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4217 - val_loss: 1.6018\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4105 - val_loss: 1.5957\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4160 - val_loss: 1.5894\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4016 - val_loss: 1.6057\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4030 - val_loss: 1.5896\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4185 - val_loss: 1.5791\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4016 - val_loss: 1.5785\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4223 - val_loss: 1.5829\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3881 - val_loss: 1.5792\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3830 - val_loss: 1.5758\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3918 - val_loss: 1.6053\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3806 - val_loss: 1.5786\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4041 - val_loss: 1.5811\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3990 - val_loss: 1.5763\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3923 - val_loss: 1.5736\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3914 - val_loss: 1.5816\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3930 - val_loss: 1.5857\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3948 - val_loss: 1.5728\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3997 - val_loss: 1.6153\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3745 - val_loss: 1.5704\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3754 - val_loss: 1.6003\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3872 - val_loss: 1.5781\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3781 - val_loss: 1.5826\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3666 - val_loss: 1.5709\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3812 - val_loss: 1.5718\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 2.2604WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.2604 - val_loss: 2.5670\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2237 - val_loss: 2.5515\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2161 - val_loss: 2.5488\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1958 - val_loss: 2.5501\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2210 - val_loss: 2.5420\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2006 - val_loss: 2.5834\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2143 - val_loss: 2.5383\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2291 - val_loss: 2.5598\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2004 - val_loss: 2.5380\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2351 - val_loss: 2.5350\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1640 - val_loss: 2.5474\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1663 - val_loss: 2.5318\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1985 - val_loss: 2.5410\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1977 - val_loss: 2.5400\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1997 - val_loss: 2.5458\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1984 - val_loss: 2.5400\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1978 - val_loss: 2.5445\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "817/820 [============================>.] - ETA: 0s - loss: 2.6428WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 2.6428 - val_loss: 3.0034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6125 - val_loss: 2.9899\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6091 - val_loss: 2.9900\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5841 - val_loss: 2.9926\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6238 - val_loss: 2.9787\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6116 - val_loss: 3.0118\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6184 - val_loss: 2.9843\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6497 - val_loss: 2.9888\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5958 - val_loss: 2.9765\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6585 - val_loss: 2.9735\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5494 - val_loss: 3.0081\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5571 - val_loss: 2.9726\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5988 - val_loss: 2.9898\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6031 - val_loss: 2.9714\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5898 - val_loss: 3.0340\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5925 - val_loss: 2.9805\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5979 - val_loss: 3.0117\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6039 - val_loss: 2.9844\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6077 - val_loss: 2.9656\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6187 - val_loss: 2.9681\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6451 - val_loss: 2.9773\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5863 - val_loss: 2.9743\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5890 - val_loss: 2.9939\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6153 - val_loss: 2.9684\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - ETA: 0s - loss: 2.7344WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.7344 - val_loss: 3.0966\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7069 - val_loss: 3.0817\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6962 - val_loss: 3.0892\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6787 - val_loss: 3.0724\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7193 - val_loss: 3.0970\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7143 - val_loss: 3.0927\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7282 - val_loss: 3.0843\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7581 - val_loss: 3.0583\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6873 - val_loss: 3.0541\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7621 - val_loss: 3.0452\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6398 - val_loss: 3.0565\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6451 - val_loss: 3.0528\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6848 - val_loss: 3.0755\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6868 - val_loss: 3.0417\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6764 - val_loss: 3.0866\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6759 - val_loss: 3.0529\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6907 - val_loss: 3.0905\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6888 - val_loss: 3.0487\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6876 - val_loss: 3.0308\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7044 - val_loss: 3.0365\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7290 - val_loss: 3.0622\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6772 - val_loss: 3.0602\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6747 - val_loss: 3.0772\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6909 - val_loss: 3.0715\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "806/820 [============================>.] - ETA: 0s - loss: 2.5822WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.5821 - val_loss: 2.9320\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5368 - val_loss: 2.9114\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5384 - val_loss: 2.9125\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5208 - val_loss: 2.9370\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5476 - val_loss: 2.9489\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5607 - val_loss: 2.9620\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5729 - val_loss: 2.8961\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6042 - val_loss: 2.9108\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5335 - val_loss: 2.9502\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5947 - val_loss: 2.8943\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4849 - val_loss: 2.9194\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4961 - val_loss: 2.8865\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5315 - val_loss: 2.9637\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5241 - val_loss: 2.9061\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5291 - val_loss: 2.8907\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5185 - val_loss: 2.8982\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5420 - val_loss: 2.9272\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 2.2671WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.2672 - val_loss: 2.5974\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2326 - val_loss: 2.5871\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2429 - val_loss: 2.6257\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2370 - val_loss: 2.6084\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2410 - val_loss: 2.6369\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2518 - val_loss: 2.6065\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2724 - val_loss: 2.5745\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3023 - val_loss: 2.6026\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2392 - val_loss: 2.6046\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2949 - val_loss: 2.5989\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1958 - val_loss: 2.6140\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2088 - val_loss: 2.5719\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2428 - val_loss: 2.6494\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2308 - val_loss: 2.5959\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2332 - val_loss: 2.6149\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2276 - val_loss: 2.5723\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2465 - val_loss: 2.6044\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "818/820 [============================>.] - ETA: 0s - loss: 1.8551WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.8551 - val_loss: 2.1472\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8256 - val_loss: 2.1574\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8369 - val_loss: 2.1652\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8290 - val_loss: 2.1583\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8304 - val_loss: 2.1840\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8468 - val_loss: 2.1277\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8621 - val_loss: 2.1303\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8879 - val_loss: 2.1418\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8307 - val_loss: 2.1495\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8684 - val_loss: 2.1084\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.7995 - val_loss: 2.1180\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8117 - val_loss: 2.1220\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8384 - val_loss: 2.1619\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8248 - val_loss: 2.1378\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.8238 - val_loss: 2.1530\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "806/820 [============================>.] - ETA: 0s - loss: 1.3529WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 6s 4ms/step - loss: 1.3529 - val_loss: 1.5451\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3186 - val_loss: 1.5690\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3386 - val_loss: 1.5711\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3308 - val_loss: 1.5979\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3292 - val_loss: 1.5711\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3433 - val_loss: 1.5586\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 0.7508WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 0.7508 - val_loss: 0.8625\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7377 - val_loss: 0.8706\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7466 - val_loss: 0.8528\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7430 - val_loss: 0.8780\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7405 - val_loss: 0.8784\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7460 - val_loss: 0.8577\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7584 - val_loss: 0.8504\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7563 - val_loss: 0.8510\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7350 - val_loss: 0.8644\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7559 - val_loss: 0.8512\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7244 - val_loss: 0.8505\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7355 - val_loss: 0.8693\n",
      "Epoch 00012: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='gru_2_input'), name='gru_2_input', description=\"created by layer 'gru_2_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001016</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>-0.002918</td>\n",
       "      <td>-0.003773</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.005168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001304</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.007772</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>-0.002319</td>\n",
       "      <td>-0.003451</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.005025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001727</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.001520</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.003990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>-0.000451</td>\n",
       "      <td>-0.001742</td>\n",
       "      <td>-0.001864</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.003994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>-0.000606</td>\n",
       "      <td>-0.002111</td>\n",
       "      <td>-0.000677</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.004420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.001571</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>0.004595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.007543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002652</td>\n",
       "      <td>-0.000535</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.007584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002606</td>\n",
       "      <td>-0.001188</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>-0.005337</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.010557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002697</td>\n",
       "      <td>-0.001649</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>-0.000910</td>\n",
       "      <td>-0.006365</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.011016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002508</td>\n",
       "      <td>-0.001938</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>-0.003467</td>\n",
       "      <td>-0.008502</td>\n",
       "      <td>0.009687</td>\n",
       "      <td>-0.000567</td>\n",
       "      <td>0.012708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002781</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>-0.000796</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.003556</td>\n",
       "      <td>-0.009039</td>\n",
       "      <td>0.010017</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>0.013036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002250</td>\n",
       "      <td>-0.001945</td>\n",
       "      <td>-0.000948</td>\n",
       "      <td>-0.000617</td>\n",
       "      <td>-0.005100</td>\n",
       "      <td>-0.009424</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.013974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002538</td>\n",
       "      <td>-0.001959</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>-0.010047</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.014256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001410</td>\n",
       "      <td>-0.001744</td>\n",
       "      <td>-0.000836</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>-0.002442</td>\n",
       "      <td>-0.010104</td>\n",
       "      <td>0.018103</td>\n",
       "      <td>0.010196</td>\n",
       "      <td>0.017818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.348398</td>\n",
       "      <td>0.422454</td>\n",
       "      <td>1.001151</td>\n",
       "      <td>1.861117</td>\n",
       "      <td>2.262722</td>\n",
       "      <td>2.766608</td>\n",
       "      <td>3.966168</td>\n",
       "      <td>5.778690</td>\n",
       "      <td>8.146265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.920866</td>\n",
       "      <td>2.594185</td>\n",
       "      <td>2.619781</td>\n",
       "      <td>5.079319</td>\n",
       "      <td>7.372400</td>\n",
       "      <td>8.788484</td>\n",
       "      <td>10.673055</td>\n",
       "      <td>17.614235</td>\n",
       "      <td>23.949827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.966311</td>\n",
       "      <td>5.636774</td>\n",
       "      <td>6.027803</td>\n",
       "      <td>6.771685</td>\n",
       "      <td>10.695585</td>\n",
       "      <td>12.786259</td>\n",
       "      <td>14.534142</td>\n",
       "      <td>22.731358</td>\n",
       "      <td>29.552259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.338504</td>\n",
       "      <td>9.621580</td>\n",
       "      <td>11.224017</td>\n",
       "      <td>13.747108</td>\n",
       "      <td>20.769327</td>\n",
       "      <td>24.191347</td>\n",
       "      <td>25.570431</td>\n",
       "      <td>35.785370</td>\n",
       "      <td>40.693188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.976093</td>\n",
       "      <td>13.582341</td>\n",
       "      <td>14.797583</td>\n",
       "      <td>14.972063</td>\n",
       "      <td>22.789495</td>\n",
       "      <td>25.769928</td>\n",
       "      <td>25.787281</td>\n",
       "      <td>33.674194</td>\n",
       "      <td>38.026020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.290026</td>\n",
       "      <td>21.127226</td>\n",
       "      <td>21.731472</td>\n",
       "      <td>22.745955</td>\n",
       "      <td>30.496012</td>\n",
       "      <td>33.892185</td>\n",
       "      <td>32.602612</td>\n",
       "      <td>33.576073</td>\n",
       "      <td>33.450459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.612039</td>\n",
       "      <td>22.547388</td>\n",
       "      <td>23.554691</td>\n",
       "      <td>25.269272</td>\n",
       "      <td>32.965340</td>\n",
       "      <td>37.166637</td>\n",
       "      <td>36.986721</td>\n",
       "      <td>37.970631</td>\n",
       "      <td>36.857723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.200477</td>\n",
       "      <td>24.394768</td>\n",
       "      <td>26.292133</td>\n",
       "      <td>29.284180</td>\n",
       "      <td>37.405754</td>\n",
       "      <td>40.993732</td>\n",
       "      <td>40.205631</td>\n",
       "      <td>43.503403</td>\n",
       "      <td>43.487072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.682407</td>\n",
       "      <td>20.894377</td>\n",
       "      <td>22.618172</td>\n",
       "      <td>26.439381</td>\n",
       "      <td>34.563240</td>\n",
       "      <td>36.743404</td>\n",
       "      <td>33.916241</td>\n",
       "      <td>37.799271</td>\n",
       "      <td>40.628624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12.108068</td>\n",
       "      <td>20.273237</td>\n",
       "      <td>22.358967</td>\n",
       "      <td>28.052643</td>\n",
       "      <td>34.679779</td>\n",
       "      <td>36.330624</td>\n",
       "      <td>33.337120</td>\n",
       "      <td>36.927574</td>\n",
       "      <td>37.329742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.081763</td>\n",
       "      <td>22.585484</td>\n",
       "      <td>24.695408</td>\n",
       "      <td>29.015863</td>\n",
       "      <td>35.595566</td>\n",
       "      <td>38.595314</td>\n",
       "      <td>36.819237</td>\n",
       "      <td>39.718102</td>\n",
       "      <td>38.761215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11.679707</td>\n",
       "      <td>19.915539</td>\n",
       "      <td>22.307480</td>\n",
       "      <td>28.434395</td>\n",
       "      <td>33.794792</td>\n",
       "      <td>35.413467</td>\n",
       "      <td>32.570709</td>\n",
       "      <td>36.444962</td>\n",
       "      <td>36.849087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.748847</td>\n",
       "      <td>14.913725</td>\n",
       "      <td>18.382086</td>\n",
       "      <td>28.167633</td>\n",
       "      <td>33.754383</td>\n",
       "      <td>34.433029</td>\n",
       "      <td>31.536037</td>\n",
       "      <td>38.330193</td>\n",
       "      <td>43.257210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9.413871</td>\n",
       "      <td>15.918719</td>\n",
       "      <td>18.843739</td>\n",
       "      <td>25.809891</td>\n",
       "      <td>29.206945</td>\n",
       "      <td>30.542290</td>\n",
       "      <td>28.154091</td>\n",
       "      <td>31.890915</td>\n",
       "      <td>33.641750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9.848680</td>\n",
       "      <td>16.591578</td>\n",
       "      <td>19.409109</td>\n",
       "      <td>25.679176</td>\n",
       "      <td>29.131289</td>\n",
       "      <td>30.743382</td>\n",
       "      <td>28.514111</td>\n",
       "      <td>31.750280</td>\n",
       "      <td>33.069721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.683677</td>\n",
       "      <td>14.533494</td>\n",
       "      <td>17.436890</td>\n",
       "      <td>22.358164</td>\n",
       "      <td>24.214502</td>\n",
       "      <td>26.725729</td>\n",
       "      <td>25.693913</td>\n",
       "      <td>28.167412</td>\n",
       "      <td>29.133614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.075298</td>\n",
       "      <td>10.546567</td>\n",
       "      <td>14.104691</td>\n",
       "      <td>19.818132</td>\n",
       "      <td>21.221964</td>\n",
       "      <td>23.490231</td>\n",
       "      <td>22.198608</td>\n",
       "      <td>25.480581</td>\n",
       "      <td>27.370882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.805512</td>\n",
       "      <td>5.327620</td>\n",
       "      <td>8.653732</td>\n",
       "      <td>12.335212</td>\n",
       "      <td>12.276530</td>\n",
       "      <td>13.557099</td>\n",
       "      <td>12.604587</td>\n",
       "      <td>14.939553</td>\n",
       "      <td>18.221132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.555528</td>\n",
       "      <td>1.018590</td>\n",
       "      <td>2.580467</td>\n",
       "      <td>5.292529</td>\n",
       "      <td>6.583253</td>\n",
       "      <td>8.365084</td>\n",
       "      <td>8.368850</td>\n",
       "      <td>10.688811</td>\n",
       "      <td>16.878769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000238</td>\n",
       "      <td>-0.002648</td>\n",
       "      <td>-0.001395</td>\n",
       "      <td>-0.001620</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.012649</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>0.026779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.003015</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>-0.001866</td>\n",
       "      <td>0.009040</td>\n",
       "      <td>0.039679</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>0.011954</td>\n",
       "      <td>0.025720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.000629</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>-0.002305</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>0.038522</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.017769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.000664</td>\n",
       "      <td>-0.003911</td>\n",
       "      <td>-0.000338</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>0.037848</td>\n",
       "      <td>0.011077</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.017574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.001408</td>\n",
       "      <td>-0.004634</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>0.007683</td>\n",
       "      <td>0.037757</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.014865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.001371</td>\n",
       "      <td>-0.004787</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>-0.002957</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.036912</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>0.014816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.002142</td>\n",
       "      <td>-0.005591</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>-0.002487</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>0.038186</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>0.009355</td>\n",
       "      <td>0.013565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.001934</td>\n",
       "      <td>-0.005617</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>-0.002893</td>\n",
       "      <td>0.008390</td>\n",
       "      <td>0.037732</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.013723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.002789</td>\n",
       "      <td>-0.006539</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>-0.002560</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.040493</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>0.012314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.002505</td>\n",
       "      <td>-0.006548</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>-0.003382</td>\n",
       "      <td>0.010329</td>\n",
       "      <td>0.039082</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.009677</td>\n",
       "      <td>0.012180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.003183</td>\n",
       "      <td>-0.007507</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>-0.003682</td>\n",
       "      <td>0.014191</td>\n",
       "      <td>0.043129</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>0.010161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.003096</td>\n",
       "      <td>-0.007470</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>-0.003695</td>\n",
       "      <td>0.014215</td>\n",
       "      <td>0.043045</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.010188</td>\n",
       "      <td>0.010136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.003714</td>\n",
       "      <td>-0.008342</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>-0.003824</td>\n",
       "      <td>0.020245</td>\n",
       "      <td>0.050175</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>0.008082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.003714</td>\n",
       "      <td>-0.008342</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>-0.003824</td>\n",
       "      <td>0.020245</td>\n",
       "      <td>0.050175</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>0.008082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0    0.001016  -0.000389   0.002657   0.007510   0.000228  -0.002918   \n",
       "1    0.001304  -0.000423   0.002826   0.007772   0.000905  -0.002319   \n",
       "2    0.001727  -0.000029   0.001942   0.003326  -0.000123  -0.001520   \n",
       "3    0.001506   0.000084   0.001775   0.003025  -0.000451  -0.001742   \n",
       "4    0.001630   0.000416   0.000709   0.002038  -0.000606  -0.002111   \n",
       "5    0.002042   0.000231   0.000983   0.002631  -0.000164  -0.001571   \n",
       "6    0.002250   0.000381  -0.000027   0.003555   0.000431  -0.002713   \n",
       "7    0.002652  -0.000535   0.000378   0.004027   0.000429  -0.002913   \n",
       "8    0.002606  -0.001188  -0.000373   0.003354  -0.000232  -0.005337   \n",
       "9    0.002697  -0.001649  -0.000315   0.002830  -0.000910  -0.006365   \n",
       "10   0.002508  -0.001938  -0.000700   0.000061  -0.003467  -0.008502   \n",
       "11   0.002781  -0.001936  -0.000796  -0.000117  -0.003556  -0.009039   \n",
       "12   0.002250  -0.001945  -0.000948  -0.000617  -0.005100  -0.009424   \n",
       "13   0.002538  -0.001959  -0.000990  -0.000632  -0.005164  -0.010047   \n",
       "14   0.001410  -0.001744  -0.000836   0.002993  -0.002442  -0.010104   \n",
       "15   0.348398   0.422454   1.001151   1.861117   2.262722   2.766608   \n",
       "16   1.920866   2.594185   2.619781   5.079319   7.372400   8.788484   \n",
       "17   3.966311   5.636774   6.027803   6.771685  10.695585  12.786259   \n",
       "18   6.338504   9.621580  11.224017  13.747108  20.769327  24.191347   \n",
       "19   8.976093  13.582341  14.797583  14.972063  22.789495  25.769928   \n",
       "20  13.290026  21.127226  21.731472  22.745955  30.496012  33.892185   \n",
       "21  13.612039  22.547388  23.554691  25.269272  32.965340  37.166637   \n",
       "22  14.200477  24.394768  26.292133  29.284180  37.405754  40.993732   \n",
       "23  12.682407  20.894377  22.618172  26.439381  34.563240  36.743404   \n",
       "24  12.108068  20.273237  22.358967  28.052643  34.679779  36.330624   \n",
       "25  13.081763  22.585484  24.695408  29.015863  35.595566  38.595314   \n",
       "26  11.679707  19.915539  22.307480  28.434395  33.794792  35.413467   \n",
       "27   8.748847  14.913725  18.382086  28.167633  33.754383  34.433029   \n",
       "28   9.413871  15.918719  18.843739  25.809891  29.206945  30.542290   \n",
       "29   9.848680  16.591578  19.409109  25.679176  29.131289  30.743382   \n",
       "30   8.683677  14.533494  17.436890  22.358164  24.214502  26.725729   \n",
       "31   6.075298  10.546567  14.104691  19.818132  21.221964  23.490231   \n",
       "32   2.805512   5.327620   8.653732  12.335212  12.276530  13.557099   \n",
       "33   0.555528   1.018590   2.580467   5.292529   6.583253   8.365084   \n",
       "34   0.000238  -0.002648  -0.001395  -0.001620   0.009835   0.040600   \n",
       "35   0.000029  -0.003015  -0.001191  -0.001866   0.009040   0.039679   \n",
       "36  -0.000629  -0.003708  -0.000437  -0.002305   0.007854   0.038522   \n",
       "37  -0.000664  -0.003911  -0.000338  -0.002661   0.007222   0.037848   \n",
       "38  -0.001408  -0.004634   0.000297  -0.002431   0.007683   0.037757   \n",
       "39  -0.001371  -0.004787   0.000306  -0.002957   0.006910   0.036912   \n",
       "40  -0.002142  -0.005591   0.000877  -0.002487   0.008774   0.038186   \n",
       "41  -0.001934  -0.005617   0.000978  -0.002893   0.008390   0.037732   \n",
       "42  -0.002789  -0.006539   0.001404  -0.002560   0.011434   0.040493   \n",
       "43  -0.002505  -0.006548   0.001255  -0.003382   0.010329   0.039082   \n",
       "44  -0.003183  -0.007507   0.001632  -0.003682   0.014191   0.043129   \n",
       "45  -0.003096  -0.007470   0.001663  -0.003695   0.014215   0.043045   \n",
       "46  -0.003714  -0.008342   0.002426  -0.003824   0.020245   0.050175   \n",
       "47  -0.003714  -0.008342   0.002426  -0.003824   0.020245   0.050175   \n",
       "\n",
       "            0          0          0  \n",
       "0   -0.003773   0.005013   0.005168  \n",
       "1   -0.003451   0.005149   0.005025  \n",
       "2   -0.001634   0.003704   0.003990  \n",
       "3   -0.001864   0.003543   0.003994  \n",
       "4   -0.000677   0.002234   0.004420  \n",
       "5    0.000109   0.002712   0.004595  \n",
       "6    0.001590   0.002497   0.007543  \n",
       "7    0.002743   0.002721   0.007584  \n",
       "8    0.004918   0.001468   0.010557  \n",
       "9    0.005653   0.001312   0.011016  \n",
       "10   0.009687  -0.000567   0.012708  \n",
       "11   0.010017  -0.000517   0.013036  \n",
       "12   0.015259   0.000729   0.013974  \n",
       "13   0.015210   0.000596   0.014256  \n",
       "14   0.018103   0.010196   0.017818  \n",
       "15   3.966168   5.778690   8.146265  \n",
       "16  10.673055  17.614235  23.949827  \n",
       "17  14.534142  22.731358  29.552259  \n",
       "18  25.570431  35.785370  40.693188  \n",
       "19  25.787281  33.674194  38.026020  \n",
       "20  32.602612  33.576073  33.450459  \n",
       "21  36.986721  37.970631  36.857723  \n",
       "22  40.205631  43.503403  43.487072  \n",
       "23  33.916241  37.799271  40.628624  \n",
       "24  33.337120  36.927574  37.329742  \n",
       "25  36.819237  39.718102  38.761215  \n",
       "26  32.570709  36.444962  36.849087  \n",
       "27  31.536037  38.330193  43.257210  \n",
       "28  28.154091  31.890915  33.641750  \n",
       "29  28.514111  31.750280  33.069721  \n",
       "30  25.693913  28.167412  29.133614  \n",
       "31  22.198608  25.480581  27.370882  \n",
       "32  12.604587  14.939553  18.221132  \n",
       "33   8.368850  10.688811  16.878769  \n",
       "34   0.012649   0.011633   0.026779  \n",
       "35   0.012469   0.011954   0.025720  \n",
       "36   0.011123   0.009763   0.017769  \n",
       "37   0.011077   0.010068   0.017574  \n",
       "38   0.009167   0.009294   0.014865  \n",
       "39   0.009007   0.009516   0.014816  \n",
       "40   0.007008   0.009355   0.013565  \n",
       "41   0.007407   0.009625   0.013723  \n",
       "42   0.005462   0.009743   0.012314  \n",
       "43   0.005218   0.009677   0.012180  \n",
       "44   0.003922   0.010165   0.010161  \n",
       "45   0.004169   0.010188   0.010136  \n",
       "46   0.005064   0.012137   0.008082  \n",
       "47   0.005064   0.012137   0.008082  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_G8 = tf.keras.Sequential([\n",
    "    layers.GRU(units=64, return_sequences=True, input_shape=[52464, 8]),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_G8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_G8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_G8.fit(np.array(Day).reshape(52464, 1, 8), np.array(Day7).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_G8 = np.squeeze(model_G8.predict(np.array(df_test).reshape(3888, 1, 8)))\n",
    "    pred_G8 = pd.DataFrame(pred_G8)\n",
    "    result_G8 = pd.concat([result_G8, pred_G8], axis=1)\n",
    "    \n",
    "result_G8[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "814/820 [============================>.] - ETA: 0s - loss: 1.5516WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 1.5512 - val_loss: 1.6329\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4394 - val_loss: 1.6113\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4198 - val_loss: 1.6146\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4062 - val_loss: 1.6264\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4108 - val_loss: 1.5832\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3948 - val_loss: 1.6424\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3948 - val_loss: 1.6169\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4110 - val_loss: 1.6025\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "805/820 [============================>.] - ETA: 0s - loss: 2.3310WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.3308 - val_loss: 2.6054\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2763 - val_loss: 2.5826\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2483 - val_loss: 2.5836\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2260 - val_loss: 2.5734\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2416 - val_loss: 2.5644\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2155 - val_loss: 2.6551\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2275 - val_loss: 2.5825\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2411 - val_loss: 2.6181\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "807/820 [============================>.] - ETA: 0s - loss: 2.7029WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.7029 - val_loss: 3.0600\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6603 - val_loss: 3.0396\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6477 - val_loss: 3.0182\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6164 - val_loss: 3.0059\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6511 - val_loss: 3.0029\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6233 - val_loss: 3.0617\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6323 - val_loss: 3.0521\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6616 - val_loss: 3.0110\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "818/820 [============================>.] - ETA: 0s - loss: 2.7895WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 2.7895 - val_loss: 3.1738\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7498 - val_loss: 3.1186\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7376 - val_loss: 3.1373\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7265 - val_loss: 3.1030\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7594 - val_loss: 3.1142\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7496 - val_loss: 3.1867\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7663 - val_loss: 3.1129\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "809/820 [============================>.] - ETA: 0s - loss: 2.6916WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 2.6916 - val_loss: 3.0115\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6513 - val_loss: 2.9731\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6387 - val_loss: 2.9769\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6425 - val_loss: 2.9933\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6620 - val_loss: 2.9949\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "805/820 [============================>.] - ETA: 0s - loss: 2.4303WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.4304 - val_loss: 2.7259\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3988 - val_loss: 2.6921\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3855 - val_loss: 2.7057\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3849 - val_loss: 2.7324\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4068 - val_loss: 2.6887\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4112 - val_loss: 2.7975\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4118 - val_loss: 2.6774\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4410 - val_loss: 2.7110\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3806 - val_loss: 2.6767\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.4286 - val_loss: 2.6940\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3413 - val_loss: 2.6709\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3562 - val_loss: 2.6734\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3794 - val_loss: 2.7169\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3622 - val_loss: 2.6718\n",
      "Epoch 00014: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 2.0018WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 2.0019 - val_loss: 2.2366\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9742 - val_loss: 2.2325\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9778 - val_loss: 2.2775\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9717 - val_loss: 2.2618\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9967 - val_loss: 2.3055\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 1.4886WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 1.4886 - val_loss: 1.6537\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4678 - val_loss: 1.6705\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4715 - val_loss: 1.6615\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4618 - val_loss: 1.6879\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "815/820 [============================>.] - ETA: 0s - loss: 0.8505WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 0.8505 - val_loss: 0.9536\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8302 - val_loss: 0.9688\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8415 - val_loss: 0.9495\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8337 - val_loss: 0.9575\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8427 - val_loss: 0.9806\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8336 - val_loss: 0.9380\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8434 - val_loss: 0.9370\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8438 - val_loss: 0.9401\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8177 - val_loss: 0.9631\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8388 - val_loss: 0.9935\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.025156</td>\n",
       "      <td>-0.013253</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>-0.010397</td>\n",
       "      <td>-0.009584</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.023365</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>0.061637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.025134</td>\n",
       "      <td>-0.013022</td>\n",
       "      <td>-0.002140</td>\n",
       "      <td>-0.010277</td>\n",
       "      <td>-0.009148</td>\n",
       "      <td>0.011234</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.060183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.024511</td>\n",
       "      <td>-0.012978</td>\n",
       "      <td>-0.002520</td>\n",
       "      <td>-0.011473</td>\n",
       "      <td>-0.011140</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.058211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.024558</td>\n",
       "      <td>-0.013143</td>\n",
       "      <td>-0.002306</td>\n",
       "      <td>-0.011226</td>\n",
       "      <td>-0.011087</td>\n",
       "      <td>0.006331</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.059596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.023976</td>\n",
       "      <td>-0.013308</td>\n",
       "      <td>-0.002698</td>\n",
       "      <td>-0.012568</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>-0.006729</td>\n",
       "      <td>0.058327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.023961</td>\n",
       "      <td>-0.012929</td>\n",
       "      <td>-0.002689</td>\n",
       "      <td>-0.012271</td>\n",
       "      <td>-0.012604</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.005386</td>\n",
       "      <td>0.057103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.023437</td>\n",
       "      <td>-0.012877</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>-0.012813</td>\n",
       "      <td>-0.013736</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.004479</td>\n",
       "      <td>-0.011112</td>\n",
       "      <td>0.056343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.023514</td>\n",
       "      <td>-0.011895</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>-0.012304</td>\n",
       "      <td>-0.012060</td>\n",
       "      <td>-0.003623</td>\n",
       "      <td>-0.004381</td>\n",
       "      <td>-0.008891</td>\n",
       "      <td>0.055771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.023176</td>\n",
       "      <td>-0.011118</td>\n",
       "      <td>-0.000895</td>\n",
       "      <td>-0.011551</td>\n",
       "      <td>-0.011026</td>\n",
       "      <td>-0.005771</td>\n",
       "      <td>-0.009467</td>\n",
       "      <td>-0.010279</td>\n",
       "      <td>0.059837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.023502</td>\n",
       "      <td>-0.010486</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.010635</td>\n",
       "      <td>-0.009500</td>\n",
       "      <td>-0.006829</td>\n",
       "      <td>-0.009509</td>\n",
       "      <td>-0.009076</td>\n",
       "      <td>0.060506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.023278</td>\n",
       "      <td>-0.009931</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>-0.009511</td>\n",
       "      <td>-0.008121</td>\n",
       "      <td>-0.006645</td>\n",
       "      <td>-0.011970</td>\n",
       "      <td>-0.007594</td>\n",
       "      <td>0.069285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.023365</td>\n",
       "      <td>-0.009879</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.009500</td>\n",
       "      <td>-0.008156</td>\n",
       "      <td>-0.006855</td>\n",
       "      <td>-0.011897</td>\n",
       "      <td>-0.007724</td>\n",
       "      <td>0.067764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.023074</td>\n",
       "      <td>-0.009769</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>-0.008633</td>\n",
       "      <td>-0.007188</td>\n",
       "      <td>-0.005436</td>\n",
       "      <td>-0.012513</td>\n",
       "      <td>-0.004907</td>\n",
       "      <td>0.077859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.023172</td>\n",
       "      <td>-0.009738</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>-0.008737</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-0.005816</td>\n",
       "      <td>-0.012639</td>\n",
       "      <td>-0.005269</td>\n",
       "      <td>0.076412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.022994</td>\n",
       "      <td>-0.009570</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>-0.006896</td>\n",
       "      <td>-0.004854</td>\n",
       "      <td>-0.003355</td>\n",
       "      <td>-0.011098</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.089936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.278376</td>\n",
       "      <td>0.533850</td>\n",
       "      <td>0.851767</td>\n",
       "      <td>1.135873</td>\n",
       "      <td>1.901088</td>\n",
       "      <td>3.958309</td>\n",
       "      <td>5.576889</td>\n",
       "      <td>8.032295</td>\n",
       "      <td>11.671991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.382670</td>\n",
       "      <td>2.785345</td>\n",
       "      <td>4.602665</td>\n",
       "      <td>6.151273</td>\n",
       "      <td>9.155155</td>\n",
       "      <td>12.433868</td>\n",
       "      <td>14.405657</td>\n",
       "      <td>20.825100</td>\n",
       "      <td>29.005898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.625676</td>\n",
       "      <td>5.551159</td>\n",
       "      <td>9.029684</td>\n",
       "      <td>11.174472</td>\n",
       "      <td>14.575258</td>\n",
       "      <td>18.475082</td>\n",
       "      <td>19.700142</td>\n",
       "      <td>27.711407</td>\n",
       "      <td>36.345173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.949140</td>\n",
       "      <td>8.700056</td>\n",
       "      <td>13.569352</td>\n",
       "      <td>15.761546</td>\n",
       "      <td>19.214750</td>\n",
       "      <td>24.219654</td>\n",
       "      <td>24.601713</td>\n",
       "      <td>33.837654</td>\n",
       "      <td>42.674992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.421922</td>\n",
       "      <td>12.015749</td>\n",
       "      <td>17.605280</td>\n",
       "      <td>19.513790</td>\n",
       "      <td>22.503637</td>\n",
       "      <td>27.247337</td>\n",
       "      <td>25.901115</td>\n",
       "      <td>34.847942</td>\n",
       "      <td>41.016430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.717727</td>\n",
       "      <td>18.720308</td>\n",
       "      <td>24.695587</td>\n",
       "      <td>24.716814</td>\n",
       "      <td>26.385412</td>\n",
       "      <td>31.913935</td>\n",
       "      <td>29.011997</td>\n",
       "      <td>34.564465</td>\n",
       "      <td>34.320408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.944949</td>\n",
       "      <td>20.628939</td>\n",
       "      <td>26.114195</td>\n",
       "      <td>25.301535</td>\n",
       "      <td>27.607227</td>\n",
       "      <td>34.258957</td>\n",
       "      <td>32.246815</td>\n",
       "      <td>36.913647</td>\n",
       "      <td>36.626324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.114343</td>\n",
       "      <td>20.005499</td>\n",
       "      <td>27.539772</td>\n",
       "      <td>28.713305</td>\n",
       "      <td>30.506847</td>\n",
       "      <td>37.443073</td>\n",
       "      <td>34.428154</td>\n",
       "      <td>42.832264</td>\n",
       "      <td>45.281658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7.772636</td>\n",
       "      <td>17.395922</td>\n",
       "      <td>24.138544</td>\n",
       "      <td>25.423502</td>\n",
       "      <td>27.575884</td>\n",
       "      <td>33.603493</td>\n",
       "      <td>30.786173</td>\n",
       "      <td>39.545959</td>\n",
       "      <td>43.069870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.626348</td>\n",
       "      <td>17.201427</td>\n",
       "      <td>23.625643</td>\n",
       "      <td>24.337145</td>\n",
       "      <td>26.427290</td>\n",
       "      <td>32.402966</td>\n",
       "      <td>29.736628</td>\n",
       "      <td>37.601875</td>\n",
       "      <td>40.233265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.852440</td>\n",
       "      <td>19.440535</td>\n",
       "      <td>26.092402</td>\n",
       "      <td>26.135069</td>\n",
       "      <td>28.116590</td>\n",
       "      <td>34.983261</td>\n",
       "      <td>32.498928</td>\n",
       "      <td>39.111118</td>\n",
       "      <td>40.255474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.633096</td>\n",
       "      <td>17.258287</td>\n",
       "      <td>23.392521</td>\n",
       "      <td>23.490526</td>\n",
       "      <td>25.597803</td>\n",
       "      <td>31.786926</td>\n",
       "      <td>29.368408</td>\n",
       "      <td>36.334438</td>\n",
       "      <td>38.169403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.426360</td>\n",
       "      <td>12.573776</td>\n",
       "      <td>18.299803</td>\n",
       "      <td>19.517847</td>\n",
       "      <td>22.470036</td>\n",
       "      <td>28.079905</td>\n",
       "      <td>26.910032</td>\n",
       "      <td>35.878826</td>\n",
       "      <td>42.546635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.129554</td>\n",
       "      <td>14.129081</td>\n",
       "      <td>19.424168</td>\n",
       "      <td>19.508886</td>\n",
       "      <td>21.832975</td>\n",
       "      <td>26.938946</td>\n",
       "      <td>24.951799</td>\n",
       "      <td>31.463928</td>\n",
       "      <td>33.699268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.561932</td>\n",
       "      <td>14.943178</td>\n",
       "      <td>20.139486</td>\n",
       "      <td>19.879492</td>\n",
       "      <td>22.147516</td>\n",
       "      <td>27.619825</td>\n",
       "      <td>25.590836</td>\n",
       "      <td>31.595079</td>\n",
       "      <td>33.061008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6.047947</td>\n",
       "      <td>13.872070</td>\n",
       "      <td>18.218056</td>\n",
       "      <td>17.434427</td>\n",
       "      <td>19.661671</td>\n",
       "      <td>24.608288</td>\n",
       "      <td>22.910669</td>\n",
       "      <td>27.609596</td>\n",
       "      <td>28.168516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.250319</td>\n",
       "      <td>9.801942</td>\n",
       "      <td>13.945416</td>\n",
       "      <td>14.109744</td>\n",
       "      <td>16.555128</td>\n",
       "      <td>20.302433</td>\n",
       "      <td>19.118517</td>\n",
       "      <td>24.372580</td>\n",
       "      <td>26.528730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.166086</td>\n",
       "      <td>4.919737</td>\n",
       "      <td>7.904614</td>\n",
       "      <td>8.732598</td>\n",
       "      <td>11.176006</td>\n",
       "      <td>13.758587</td>\n",
       "      <td>13.877783</td>\n",
       "      <td>18.237036</td>\n",
       "      <td>21.240238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.447172</td>\n",
       "      <td>0.955420</td>\n",
       "      <td>1.710934</td>\n",
       "      <td>2.428085</td>\n",
       "      <td>4.241768</td>\n",
       "      <td>7.769482</td>\n",
       "      <td>9.957658</td>\n",
       "      <td>13.421550</td>\n",
       "      <td>18.445862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.019087</td>\n",
       "      <td>-0.010179</td>\n",
       "      <td>0.008234</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.016144</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>0.042048</td>\n",
       "      <td>0.127486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.019074</td>\n",
       "      <td>-0.010748</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>-0.001864</td>\n",
       "      <td>0.014825</td>\n",
       "      <td>0.018191</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.121346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.018880</td>\n",
       "      <td>-0.011613</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>-0.003298</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.016217</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>0.111468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.018895</td>\n",
       "      <td>-0.012059</td>\n",
       "      <td>0.005537</td>\n",
       "      <td>-0.000337</td>\n",
       "      <td>-0.004792</td>\n",
       "      <td>0.010570</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>0.030661</td>\n",
       "      <td>0.107538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.018718</td>\n",
       "      <td>-0.012907</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>-0.000382</td>\n",
       "      <td>-0.005830</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.025599</td>\n",
       "      <td>0.097012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.018743</td>\n",
       "      <td>-0.013371</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.023280</td>\n",
       "      <td>0.093670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.018607</td>\n",
       "      <td>-0.014290</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>-0.008551</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>0.082149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.018672</td>\n",
       "      <td>-0.014690</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>-0.002696</td>\n",
       "      <td>-0.010221</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>0.015344</td>\n",
       "      <td>0.080675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.018561</td>\n",
       "      <td>-0.015644</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>-0.002864</td>\n",
       "      <td>-0.011361</td>\n",
       "      <td>-0.005251</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>0.068372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.018628</td>\n",
       "      <td>-0.016210</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.004417</td>\n",
       "      <td>-0.013515</td>\n",
       "      <td>-0.005692</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.066969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.018579</td>\n",
       "      <td>-0.017376</td>\n",
       "      <td>-0.001560</td>\n",
       "      <td>-0.005358</td>\n",
       "      <td>-0.015743</td>\n",
       "      <td>-0.012552</td>\n",
       "      <td>-0.004824</td>\n",
       "      <td>-0.001577</td>\n",
       "      <td>0.054842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.018607</td>\n",
       "      <td>-0.017457</td>\n",
       "      <td>-0.001805</td>\n",
       "      <td>-0.005639</td>\n",
       "      <td>-0.016169</td>\n",
       "      <td>-0.012681</td>\n",
       "      <td>-0.004998</td>\n",
       "      <td>-0.001723</td>\n",
       "      <td>0.055233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.018571</td>\n",
       "      <td>-0.018396</td>\n",
       "      <td>-0.002754</td>\n",
       "      <td>-0.006238</td>\n",
       "      <td>-0.018174</td>\n",
       "      <td>-0.021020</td>\n",
       "      <td>-0.012369</td>\n",
       "      <td>-0.011444</td>\n",
       "      <td>0.043390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.018571</td>\n",
       "      <td>-0.018396</td>\n",
       "      <td>-0.002754</td>\n",
       "      <td>-0.006238</td>\n",
       "      <td>-0.018174</td>\n",
       "      <td>-0.021020</td>\n",
       "      <td>-0.012369</td>\n",
       "      <td>-0.011444</td>\n",
       "      <td>0.043390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          0          0          0          0          0  \\\n",
       "0  -0.025156  -0.013253  -0.002063  -0.010397  -0.009584   0.011747   \n",
       "1  -0.025134  -0.013022  -0.002140  -0.010277  -0.009148   0.011234   \n",
       "2  -0.024511  -0.012978  -0.002520  -0.011473  -0.011140   0.005508   \n",
       "3  -0.024558  -0.013143  -0.002306  -0.011226  -0.011087   0.006331   \n",
       "4  -0.023976  -0.013308  -0.002698  -0.012568  -0.013493   0.001924   \n",
       "5  -0.023961  -0.012929  -0.002689  -0.012271  -0.012604   0.001386   \n",
       "6  -0.023437  -0.012877  -0.002717  -0.012813  -0.013736  -0.001402   \n",
       "7  -0.023514  -0.011895  -0.002166  -0.012304  -0.012060  -0.003623   \n",
       "8  -0.023176  -0.011118  -0.000895  -0.011551  -0.011026  -0.005771   \n",
       "9  -0.023502  -0.010486  -0.000027  -0.010635  -0.009500  -0.006829   \n",
       "10 -0.023278  -0.009931   0.001432  -0.009511  -0.008121  -0.006645   \n",
       "11 -0.023365  -0.009879   0.001393  -0.009500  -0.008156  -0.006855   \n",
       "12 -0.023074  -0.009769   0.002691  -0.008633  -0.007188  -0.005436   \n",
       "13 -0.023172  -0.009738   0.002620  -0.008737  -0.007413  -0.005816   \n",
       "14 -0.022994  -0.009570   0.004730  -0.006896  -0.004854  -0.003355   \n",
       "15  0.278376   0.533850   0.851767   1.135873   1.901088   3.958309   \n",
       "16  1.382670   2.785345   4.602665   6.151273   9.155155  12.433868   \n",
       "17  2.625676   5.551159   9.029684  11.174472  14.575258  18.475082   \n",
       "18  3.949140   8.700056  13.569352  15.761546  19.214750  24.219654   \n",
       "19  5.421922  12.015749  17.605280  19.513790  22.503637  27.247337   \n",
       "20  8.717727  18.720308  24.695587  24.716814  26.385412  31.913935   \n",
       "21  9.944949  20.628939  26.114195  25.301535  27.607227  34.258957   \n",
       "22  9.114343  20.005499  27.539772  28.713305  30.506847  37.443073   \n",
       "23  7.772636  17.395922  24.138544  25.423502  27.575884  33.603493   \n",
       "24  7.626348  17.201427  23.625643  24.337145  26.427290  32.402966   \n",
       "25  8.852440  19.440535  26.092402  26.135069  28.116590  34.983261   \n",
       "26  7.633096  17.258287  23.392521  23.490526  25.597803  31.786926   \n",
       "27  5.426360  12.573776  18.299803  19.517847  22.470036  28.079905   \n",
       "28  6.129554  14.129081  19.424168  19.508886  21.832975  26.938946   \n",
       "29  6.561932  14.943178  20.139486  19.879492  22.147516  27.619825   \n",
       "30  6.047947  13.872070  18.218056  17.434427  19.661671  24.608288   \n",
       "31  4.250319   9.801942  13.945416  14.109744  16.555128  20.302433   \n",
       "32  2.166086   4.919737   7.904614   8.732598  11.176006  13.758587   \n",
       "33  0.447172   0.955420   1.710934   2.428085   4.241768   7.769482   \n",
       "34 -0.019087  -0.010179   0.008234   0.001928  -0.000037   0.016144   \n",
       "35 -0.019074  -0.010748   0.007256   0.000840  -0.001864   0.014825   \n",
       "36 -0.018880  -0.011613   0.006416   0.000609  -0.003298   0.011465   \n",
       "37 -0.018895  -0.012059   0.005537  -0.000337  -0.004792   0.010570   \n",
       "38 -0.018718  -0.012907   0.004820  -0.000382  -0.005830   0.006707   \n",
       "39 -0.018743  -0.013371   0.003830  -0.001418  -0.007380   0.005987   \n",
       "40 -0.018607  -0.014290   0.003038  -0.001584  -0.008551   0.001186   \n",
       "41 -0.018672  -0.014690   0.002030  -0.002696  -0.010221   0.000430   \n",
       "42 -0.018561  -0.015644   0.001293  -0.002864  -0.011361  -0.005251   \n",
       "43 -0.018628  -0.016210  -0.000190  -0.004417  -0.013515  -0.005692   \n",
       "44 -0.018579  -0.017376  -0.001560  -0.005358  -0.015743  -0.012552   \n",
       "45 -0.018607  -0.017457  -0.001805  -0.005639  -0.016169  -0.012681   \n",
       "46 -0.018571  -0.018396  -0.002754  -0.006238  -0.018174  -0.021020   \n",
       "47 -0.018571  -0.018396  -0.002754  -0.006238  -0.018174  -0.021020   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.023365   0.010626   0.061637  \n",
       "1    0.023427   0.010931   0.060183  \n",
       "2    0.012225   0.001916   0.058211  \n",
       "3    0.012539   0.001968   0.059596  \n",
       "4    0.002373  -0.006729   0.058327  \n",
       "5    0.002988  -0.005386   0.057103  \n",
       "6   -0.004479  -0.011112   0.056343  \n",
       "7   -0.004381  -0.008891   0.055771  \n",
       "8   -0.009467  -0.010279   0.059837  \n",
       "9   -0.009509  -0.009076   0.060506  \n",
       "10  -0.011970  -0.007594   0.069285  \n",
       "11  -0.011897  -0.007724   0.067764  \n",
       "12  -0.012513  -0.004907   0.077859  \n",
       "13  -0.012639  -0.005269   0.076412  \n",
       "14  -0.011098   0.000618   0.089936  \n",
       "15   5.576889   8.032295  11.671991  \n",
       "16  14.405657  20.825100  29.005898  \n",
       "17  19.700142  27.711407  36.345173  \n",
       "18  24.601713  33.837654  42.674992  \n",
       "19  25.901115  34.847942  41.016430  \n",
       "20  29.011997  34.564465  34.320408  \n",
       "21  32.246815  36.913647  36.626324  \n",
       "22  34.428154  42.832264  45.281658  \n",
       "23  30.786173  39.545959  43.069870  \n",
       "24  29.736628  37.601875  40.233265  \n",
       "25  32.498928  39.111118  40.255474  \n",
       "26  29.368408  36.334438  38.169403  \n",
       "27  26.910032  35.878826  42.546635  \n",
       "28  24.951799  31.463928  33.699268  \n",
       "29  25.590836  31.595079  33.061008  \n",
       "30  22.910669  27.609596  28.168516  \n",
       "31  19.118517  24.372580  26.528730  \n",
       "32  13.877783  18.237036  21.240238  \n",
       "33   9.957658  13.421550  18.445862  \n",
       "34   0.020384   0.042048   0.127486  \n",
       "35   0.018191   0.038080   0.121346  \n",
       "36   0.016217   0.033354   0.111468  \n",
       "37   0.014691   0.030661   0.107538  \n",
       "38   0.012276   0.025599   0.097012  \n",
       "39   0.010971   0.023280   0.093670  \n",
       "40   0.007525   0.017151   0.082149  \n",
       "41   0.006318   0.015344   0.080675  \n",
       "42   0.001912   0.008329   0.068372  \n",
       "43   0.000982   0.006755   0.066969  \n",
       "44  -0.004824  -0.001577   0.054842  \n",
       "45  -0.004998  -0.001723   0.055233  \n",
       "46  -0.012369  -0.011444   0.043390  \n",
       "47  -0.012369  -0.011444   0.043390  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# +NET\n",
    "\n",
    "model_M7 = tf.keras.Sequential([\n",
    "    layers.LSTM(units=64, return_sequences=True, input_shape=[52464, 9]),\n",
    "    layers.LSTM(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_M7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_M7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_M7.fit(np.array(Day0).reshape(52464, 1, 9), np.array(Day7).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_M7 = np.squeeze(model_M7.predict(np.array(df_test0).reshape(3888, 1, 9)))\n",
    "    pred_M7 = pd.DataFrame(pred_M7)\n",
    "    result_M7 = pd.concat([result_M7, pred_M7], axis=1)\n",
    "    \n",
    "result_M7[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "815/820 [============================>.] - ETA: 0s - loss: 1.5626WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.5622 - val_loss: 1.6496\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4543 - val_loss: 1.6489\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4303 - val_loss: 1.6523\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4176 - val_loss: 1.6257\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4274 - val_loss: 1.6347\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4148 - val_loss: 1.6385\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4237 - val_loss: 1.6590\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "807/820 [============================>.] - ETA: 0s - loss: 2.4312WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.4309 - val_loss: 2.7301\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3799 - val_loss: 2.7068\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3530 - val_loss: 2.7187\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3315 - val_loss: 2.6789\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3352 - val_loss: 2.6861\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3224 - val_loss: 2.7478\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3384 - val_loss: 2.6606\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3456 - val_loss: 2.7118\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3415 - val_loss: 2.6589\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3549 - val_loss: 2.7078\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3014 - val_loss: 2.7542\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3153 - val_loss: 2.6576\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3138 - val_loss: 2.7330\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2974 - val_loss: 2.7290\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3140 - val_loss: 2.7203\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "809/820 [============================>.] - ETA: 0s - loss: 2.8536WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.8534 - val_loss: 3.2593\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8281 - val_loss: 3.1785\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7956 - val_loss: 3.1740\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7958 - val_loss: 3.1541\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7982 - val_loss: 3.1491\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7797 - val_loss: 3.2074\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8088 - val_loss: 3.1545\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7979 - val_loss: 3.2138\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "817/820 [============================>.] - ETA: 0s - loss: 2.9939WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 2.9938 - val_loss: 3.3417\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9484 - val_loss: 3.3142\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9344 - val_loss: 3.2937\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9375 - val_loss: 3.3274\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9290 - val_loss: 3.3090\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.9126 - val_loss: 3.2943\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - ETA: 0s - loss: 2.8793WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.8793 - val_loss: 3.2226\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8425 - val_loss: 3.1930\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8309 - val_loss: 3.1994\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8295 - val_loss: 3.1871\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8350 - val_loss: 3.2176\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8065 - val_loss: 3.1564\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8508 - val_loss: 3.2353\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8444 - val_loss: 3.3436\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.8212 - val_loss: 3.1775\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "815/820 [============================>.] - ETA: 0s - loss: 2.5727WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.5726 - val_loss: 2.8659\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5356 - val_loss: 2.8515\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5281 - val_loss: 2.8694\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5256 - val_loss: 2.9211\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5418 - val_loss: 2.8984\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "815/820 [============================>.] - ETA: 0s - loss: 2.1316WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.1315 - val_loss: 2.4022\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1036 - val_loss: 2.3812\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0973 - val_loss: 2.3666\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0841 - val_loss: 2.3597\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0992 - val_loss: 2.3889\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0635 - val_loss: 2.3523\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1064 - val_loss: 2.4470\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0933 - val_loss: 2.3713\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0667 - val_loss: 2.3500\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0842 - val_loss: 2.3784\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0400 - val_loss: 2.3311\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0733 - val_loss: 2.3453\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0656 - val_loss: 2.4104\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0620 - val_loss: 2.3257\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0614 - val_loss: 2.3234\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.0206 - val_loss: 2.3572\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0694 - val_loss: 2.4465\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0518 - val_loss: 2.3082\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0342 - val_loss: 2.3547\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0208 - val_loss: 2.3118\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0442 - val_loss: 2.2841\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0260 - val_loss: 2.3477\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0066 - val_loss: 2.3089\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.0339 - val_loss: 2.3437\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "814/820 [============================>.] - ETA: 0s - loss: 1.4834WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 5s 4ms/step - loss: 1.4834 - val_loss: 1.6419\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4674 - val_loss: 1.6700\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4638 - val_loss: 1.7555\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4697 - val_loss: 1.6418\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4609 - val_loss: 1.6783\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4371 - val_loss: 1.6343\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4701 - val_loss: 1.6498\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4591 - val_loss: 1.6783\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4403 - val_loss: 1.6269\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4543 - val_loss: 1.6476\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4244 - val_loss: 1.6401\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4377 - val_loss: 1.6285\n",
      "Epoch 00012: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - ETA: 0s - loss: 0.8193WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 0.8192 - val_loss: 0.9307\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8070 - val_loss: 0.9210\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8026 - val_loss: 0.9174\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8077 - val_loss: 0.9036\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8021 - val_loss: 0.9018\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7866 - val_loss: 0.8913\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.8034 - val_loss: 0.9352\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7982 - val_loss: 0.9076\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7895 - val_loss: 0.8940\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 9), dtype=tf.float32, name='lstm_2_input'), name='lstm_2_input', description=\"created by layer 'lstm_2_input'\"), but it was called on an input with incompatible shape (None, 1, 9).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.008475</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>-0.010970</td>\n",
       "      <td>-0.009766</td>\n",
       "      <td>-0.001435</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.007745</td>\n",
       "      <td>-0.001998</td>\n",
       "      <td>0.013644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.008172</td>\n",
       "      <td>-0.002848</td>\n",
       "      <td>-0.011345</td>\n",
       "      <td>-0.008944</td>\n",
       "      <td>-0.001234</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>-0.001692</td>\n",
       "      <td>0.013986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.007855</td>\n",
       "      <td>-0.002624</td>\n",
       "      <td>-0.011701</td>\n",
       "      <td>-0.007621</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.007745</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.018128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.008168</td>\n",
       "      <td>-0.003216</td>\n",
       "      <td>-0.011477</td>\n",
       "      <td>-0.008326</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.007057</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.017873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.003836</td>\n",
       "      <td>-0.011515</td>\n",
       "      <td>-0.008049</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.020448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.007627</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>-0.011969</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.020618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.007451</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>-0.012095</td>\n",
       "      <td>-0.004981</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>-0.003716</td>\n",
       "      <td>-0.004533</td>\n",
       "      <td>0.022629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.006254</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.013250</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.011593</td>\n",
       "      <td>-0.000667</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>0.023953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.005532</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>-0.014068</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>-0.005948</td>\n",
       "      <td>-0.011721</td>\n",
       "      <td>0.032198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.005155</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.015745</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>0.011386</td>\n",
       "      <td>0.015880</td>\n",
       "      <td>-0.004026</td>\n",
       "      <td>-0.010507</td>\n",
       "      <td>0.036050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.004814</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>-0.016484</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.015319</td>\n",
       "      <td>0.018398</td>\n",
       "      <td>-0.007913</td>\n",
       "      <td>-0.012019</td>\n",
       "      <td>0.061850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.004856</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>-0.017231</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>0.015328</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>-0.007909</td>\n",
       "      <td>-0.010908</td>\n",
       "      <td>0.065133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.004640</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>-0.015750</td>\n",
       "      <td>0.010525</td>\n",
       "      <td>0.020229</td>\n",
       "      <td>0.021559</td>\n",
       "      <td>-0.007937</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.126479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>-0.016534</td>\n",
       "      <td>0.010466</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.021086</td>\n",
       "      <td>-0.008539</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.129978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>0.014647</td>\n",
       "      <td>0.026919</td>\n",
       "      <td>0.028038</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.065109</td>\n",
       "      <td>0.273918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.224555</td>\n",
       "      <td>0.493217</td>\n",
       "      <td>0.769333</td>\n",
       "      <td>1.291518</td>\n",
       "      <td>2.884746</td>\n",
       "      <td>4.748418</td>\n",
       "      <td>8.300235</td>\n",
       "      <td>13.855488</td>\n",
       "      <td>19.790730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.064352</td>\n",
       "      <td>3.094429</td>\n",
       "      <td>5.176425</td>\n",
       "      <td>8.452856</td>\n",
       "      <td>11.382863</td>\n",
       "      <td>13.115429</td>\n",
       "      <td>20.796421</td>\n",
       "      <td>27.747461</td>\n",
       "      <td>33.285465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.085540</td>\n",
       "      <td>6.851115</td>\n",
       "      <td>11.054815</td>\n",
       "      <td>16.175102</td>\n",
       "      <td>18.505974</td>\n",
       "      <td>19.281204</td>\n",
       "      <td>27.036049</td>\n",
       "      <td>30.534048</td>\n",
       "      <td>35.522842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.243448</td>\n",
       "      <td>10.825759</td>\n",
       "      <td>16.592876</td>\n",
       "      <td>22.638235</td>\n",
       "      <td>24.353527</td>\n",
       "      <td>24.096861</td>\n",
       "      <td>34.109833</td>\n",
       "      <td>36.764412</td>\n",
       "      <td>43.473362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.668878</td>\n",
       "      <td>14.533233</td>\n",
       "      <td>20.736805</td>\n",
       "      <td>26.693420</td>\n",
       "      <td>26.814178</td>\n",
       "      <td>24.969471</td>\n",
       "      <td>31.520792</td>\n",
       "      <td>31.285065</td>\n",
       "      <td>36.117672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.403633</td>\n",
       "      <td>21.809557</td>\n",
       "      <td>25.825407</td>\n",
       "      <td>29.473806</td>\n",
       "      <td>27.048195</td>\n",
       "      <td>27.283840</td>\n",
       "      <td>34.561665</td>\n",
       "      <td>34.875008</td>\n",
       "      <td>37.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.052383</td>\n",
       "      <td>23.739573</td>\n",
       "      <td>25.210020</td>\n",
       "      <td>27.425840</td>\n",
       "      <td>28.081005</td>\n",
       "      <td>30.603060</td>\n",
       "      <td>37.294102</td>\n",
       "      <td>37.275146</td>\n",
       "      <td>39.570808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.884293</td>\n",
       "      <td>23.614380</td>\n",
       "      <td>28.908573</td>\n",
       "      <td>35.237713</td>\n",
       "      <td>33.645550</td>\n",
       "      <td>32.784695</td>\n",
       "      <td>42.524303</td>\n",
       "      <td>41.881824</td>\n",
       "      <td>46.127567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7.170529</td>\n",
       "      <td>20.149168</td>\n",
       "      <td>26.042404</td>\n",
       "      <td>32.277306</td>\n",
       "      <td>30.443672</td>\n",
       "      <td>28.663486</td>\n",
       "      <td>37.694725</td>\n",
       "      <td>37.923538</td>\n",
       "      <td>43.073772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.006989</td>\n",
       "      <td>19.508602</td>\n",
       "      <td>24.621035</td>\n",
       "      <td>30.483465</td>\n",
       "      <td>28.035336</td>\n",
       "      <td>27.140644</td>\n",
       "      <td>35.836781</td>\n",
       "      <td>36.353233</td>\n",
       "      <td>40.748795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.630649</td>\n",
       "      <td>22.244896</td>\n",
       "      <td>25.697399</td>\n",
       "      <td>30.706722</td>\n",
       "      <td>29.184013</td>\n",
       "      <td>30.292667</td>\n",
       "      <td>39.333092</td>\n",
       "      <td>39.423084</td>\n",
       "      <td>42.326111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.066695</td>\n",
       "      <td>19.182344</td>\n",
       "      <td>23.245707</td>\n",
       "      <td>28.614256</td>\n",
       "      <td>26.160431</td>\n",
       "      <td>26.501652</td>\n",
       "      <td>34.652939</td>\n",
       "      <td>35.096764</td>\n",
       "      <td>37.877037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.621830</td>\n",
       "      <td>14.512002</td>\n",
       "      <td>20.495499</td>\n",
       "      <td>27.096788</td>\n",
       "      <td>25.522949</td>\n",
       "      <td>24.113199</td>\n",
       "      <td>33.200935</td>\n",
       "      <td>33.946140</td>\n",
       "      <td>42.417927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.364833</td>\n",
       "      <td>15.456990</td>\n",
       "      <td>19.887440</td>\n",
       "      <td>25.163893</td>\n",
       "      <td>21.756081</td>\n",
       "      <td>21.611567</td>\n",
       "      <td>27.556894</td>\n",
       "      <td>28.349117</td>\n",
       "      <td>30.159492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.919316</td>\n",
       "      <td>16.356092</td>\n",
       "      <td>20.091255</td>\n",
       "      <td>24.842615</td>\n",
       "      <td>21.822651</td>\n",
       "      <td>22.467485</td>\n",
       "      <td>28.873140</td>\n",
       "      <td>29.784460</td>\n",
       "      <td>31.264435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.305933</td>\n",
       "      <td>14.342173</td>\n",
       "      <td>16.965315</td>\n",
       "      <td>20.581347</td>\n",
       "      <td>18.054493</td>\n",
       "      <td>19.777199</td>\n",
       "      <td>25.084467</td>\n",
       "      <td>26.006908</td>\n",
       "      <td>26.629461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.503308</td>\n",
       "      <td>10.973215</td>\n",
       "      <td>15.272486</td>\n",
       "      <td>20.004910</td>\n",
       "      <td>15.951016</td>\n",
       "      <td>15.694623</td>\n",
       "      <td>19.287167</td>\n",
       "      <td>20.345507</td>\n",
       "      <td>21.038740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.624987</td>\n",
       "      <td>5.569398</td>\n",
       "      <td>9.298272</td>\n",
       "      <td>13.697021</td>\n",
       "      <td>10.722804</td>\n",
       "      <td>10.207670</td>\n",
       "      <td>11.319527</td>\n",
       "      <td>11.952787</td>\n",
       "      <td>12.671544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.319868</td>\n",
       "      <td>0.871121</td>\n",
       "      <td>1.711004</td>\n",
       "      <td>3.198543</td>\n",
       "      <td>5.575622</td>\n",
       "      <td>7.700372</td>\n",
       "      <td>10.453104</td>\n",
       "      <td>13.157294</td>\n",
       "      <td>17.465687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.005308</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>0.021461</td>\n",
       "      <td>0.039664</td>\n",
       "      <td>0.047273</td>\n",
       "      <td>-0.074295</td>\n",
       "      <td>0.027296</td>\n",
       "      <td>0.197710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.005422</td>\n",
       "      <td>0.008836</td>\n",
       "      <td>-0.004951</td>\n",
       "      <td>0.020388</td>\n",
       "      <td>0.040092</td>\n",
       "      <td>0.046583</td>\n",
       "      <td>-0.072800</td>\n",
       "      <td>0.028004</td>\n",
       "      <td>0.195592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.005499</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>-0.004609</td>\n",
       "      <td>0.019282</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>0.044214</td>\n",
       "      <td>-0.077383</td>\n",
       "      <td>0.013301</td>\n",
       "      <td>0.144659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.005593</td>\n",
       "      <td>0.007339</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>0.018222</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>0.043812</td>\n",
       "      <td>-0.075743</td>\n",
       "      <td>0.014221</td>\n",
       "      <td>0.144194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.005612</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>-0.005219</td>\n",
       "      <td>0.016850</td>\n",
       "      <td>0.034976</td>\n",
       "      <td>0.039661</td>\n",
       "      <td>-0.072365</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.119468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.005716</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>0.015538</td>\n",
       "      <td>0.035279</td>\n",
       "      <td>0.039408</td>\n",
       "      <td>-0.070370</td>\n",
       "      <td>0.015380</td>\n",
       "      <td>0.119884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.005685</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>-0.006816</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>0.030187</td>\n",
       "      <td>0.033065</td>\n",
       "      <td>-0.057821</td>\n",
       "      <td>0.025182</td>\n",
       "      <td>0.113186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.005776</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>-0.007692</td>\n",
       "      <td>0.012139</td>\n",
       "      <td>0.030951</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>-0.055496</td>\n",
       "      <td>0.026256</td>\n",
       "      <td>0.114139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.005683</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.024892</td>\n",
       "      <td>-0.034070</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>0.118273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.005826</td>\n",
       "      <td>-0.000524</td>\n",
       "      <td>-0.010703</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>0.024766</td>\n",
       "      <td>0.025764</td>\n",
       "      <td>-0.030698</td>\n",
       "      <td>0.044806</td>\n",
       "      <td>0.120863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.005720</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>-0.013604</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.016590</td>\n",
       "      <td>0.015626</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.063778</td>\n",
       "      <td>0.129839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.005739</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-0.013752</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.017154</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.064027</td>\n",
       "      <td>0.130222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.005523</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>-0.016108</td>\n",
       "      <td>-0.000979</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.033582</td>\n",
       "      <td>0.076415</td>\n",
       "      <td>0.131625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.005523</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>-0.016108</td>\n",
       "      <td>-0.000979</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.033582</td>\n",
       "      <td>0.076415</td>\n",
       "      <td>0.131625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.008475  -0.003593  -0.010970  -0.009766  -0.001435   0.009317   \n",
       "1   -0.008172  -0.002848  -0.011345  -0.008944  -0.001234   0.009965   \n",
       "2   -0.007855  -0.002624  -0.011701  -0.007621   0.000501   0.008573   \n",
       "3   -0.008168  -0.003216  -0.011477  -0.008326   0.000420   0.007978   \n",
       "4   -0.008183  -0.003836  -0.011515  -0.008049   0.002103   0.006263   \n",
       "5   -0.007627  -0.002345  -0.011969  -0.006306   0.002835   0.007906   \n",
       "6   -0.007451  -0.002027  -0.012095  -0.004981   0.005641   0.007945   \n",
       "7   -0.006254   0.000044  -0.013250  -0.001024   0.006912   0.011593   \n",
       "8   -0.005532   0.000878  -0.014068   0.002965   0.010953   0.014594   \n",
       "9   -0.005155   0.000487  -0.015745   0.004819   0.011386   0.015880   \n",
       "10  -0.004814   0.000509  -0.016484   0.007909   0.015319   0.018398   \n",
       "11  -0.004856   0.000302  -0.017231   0.007975   0.015328   0.018115   \n",
       "12  -0.004640   0.001504  -0.015750   0.010525   0.020229   0.021559   \n",
       "13  -0.004673   0.001286  -0.016534   0.010466   0.020188   0.021086   \n",
       "14  -0.004304   0.003010  -0.013418   0.014647   0.026919   0.028038   \n",
       "15   0.224555   0.493217   0.769333   1.291518   2.884746   4.748418   \n",
       "16   1.064352   3.094429   5.176425   8.452856  11.382863  13.115429   \n",
       "17   2.085540   6.851115  11.054815  16.175102  18.505974  19.281204   \n",
       "18   3.243448  10.825759  16.592876  22.638235  24.353527  24.096861   \n",
       "19   4.668878  14.533233  20.736805  26.693420  26.814178  24.969471   \n",
       "20   8.403633  21.809557  25.825407  29.473806  27.048195  27.283840   \n",
       "21  10.052383  23.739573  25.210020  27.425840  28.081005  30.603060   \n",
       "22   8.884293  23.614380  28.908573  35.237713  33.645550  32.784695   \n",
       "23   7.170529  20.149168  26.042404  32.277306  30.443672  28.663486   \n",
       "24   7.006989  19.508602  24.621035  30.483465  28.035336  27.140644   \n",
       "25   8.630649  22.244896  25.697399  30.706722  29.184013  30.292667   \n",
       "26   7.066695  19.182344  23.245707  28.614256  26.160431  26.501652   \n",
       "27   4.621830  14.512002  20.495499  27.096788  25.522949  24.113199   \n",
       "28   5.364833  15.456990  19.887440  25.163893  21.756081  21.611567   \n",
       "29   5.919316  16.356092  20.091255  24.842615  21.822651  22.467485   \n",
       "30   5.305933  14.342173  16.965315  20.581347  18.054493  19.777199   \n",
       "31   3.503308  10.973215  15.272486  20.004910  15.951016  15.694623   \n",
       "32   1.624987   5.569398   9.298272  13.697021  10.722804  10.207670   \n",
       "33   0.319868   0.871121   1.711004   3.198543   5.575622   7.700372   \n",
       "34  -0.005308   0.009435  -0.004564   0.021461   0.039664   0.047273   \n",
       "35  -0.005422   0.008836  -0.004951   0.020388   0.040092   0.046583   \n",
       "36  -0.005499   0.007899  -0.004609   0.019282   0.038123   0.044214   \n",
       "37  -0.005593   0.007339  -0.005167   0.018222   0.038447   0.043812   \n",
       "38  -0.005612   0.005890  -0.005219   0.016850   0.034976   0.039661   \n",
       "39  -0.005716   0.005202  -0.005998   0.015538   0.035279   0.039408   \n",
       "40  -0.005685   0.003210  -0.006816   0.013436   0.030187   0.033065   \n",
       "41  -0.005776   0.002790  -0.007692   0.012139   0.030951   0.033642   \n",
       "42  -0.005683   0.000347  -0.009229   0.009308   0.023968   0.024892   \n",
       "43  -0.005826  -0.000524  -0.010703   0.007118   0.024766   0.025764   \n",
       "44  -0.005720  -0.003356  -0.013604   0.002662   0.016590   0.015626   \n",
       "45  -0.005739  -0.003346  -0.013752   0.002435   0.017154   0.016413   \n",
       "46  -0.005523  -0.005404  -0.016108  -0.000979   0.008393   0.004975   \n",
       "47  -0.005523  -0.005404  -0.016108  -0.000979   0.008393   0.004975   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.007745  -0.001998   0.013644  \n",
       "1    0.008533  -0.001692   0.013986  \n",
       "2    0.007745   0.001441   0.018128  \n",
       "3    0.007057   0.001221   0.017873  \n",
       "4    0.001220   0.000509   0.020448  \n",
       "5    0.002986   0.000615   0.020618  \n",
       "6   -0.003716  -0.004533   0.022629  \n",
       "7   -0.000667  -0.004474   0.023953  \n",
       "8   -0.005948  -0.011721   0.032198  \n",
       "9   -0.004026  -0.010507   0.036050  \n",
       "10  -0.007913  -0.012019   0.061850  \n",
       "11  -0.007909  -0.010908   0.065133  \n",
       "12  -0.007937   0.004939   0.126479  \n",
       "13  -0.008539   0.005620   0.129978  \n",
       "14   0.000494   0.065109   0.273918  \n",
       "15   8.300235  13.855488  19.790730  \n",
       "16  20.796421  27.747461  33.285465  \n",
       "17  27.036049  30.534048  35.522842  \n",
       "18  34.109833  36.764412  43.473362  \n",
       "19  31.520792  31.285065  36.117672  \n",
       "20  34.561665  34.875008  37.050900  \n",
       "21  37.294102  37.275146  39.570808  \n",
       "22  42.524303  41.881824  46.127567  \n",
       "23  37.694725  37.923538  43.073772  \n",
       "24  35.836781  36.353233  40.748795  \n",
       "25  39.333092  39.423084  42.326111  \n",
       "26  34.652939  35.096764  37.877037  \n",
       "27  33.200935  33.946140  42.417927  \n",
       "28  27.556894  28.349117  30.159492  \n",
       "29  28.873140  29.784460  31.264435  \n",
       "30  25.084467  26.006908  26.629461  \n",
       "31  19.287167  20.345507  21.038740  \n",
       "32  11.319527  11.952787  12.671544  \n",
       "33  10.453104  13.157294  17.465687  \n",
       "34  -0.074295   0.027296   0.197710  \n",
       "35  -0.072800   0.028004   0.195592  \n",
       "36  -0.077383   0.013301   0.144659  \n",
       "37  -0.075743   0.014221   0.144194  \n",
       "38  -0.072365   0.014074   0.119468  \n",
       "39  -0.070370   0.015380   0.119884  \n",
       "40  -0.057821   0.025182   0.113186  \n",
       "41  -0.055496   0.026256   0.114139  \n",
       "42  -0.034070   0.042322   0.118273  \n",
       "43  -0.030698   0.044806   0.120863  \n",
       "44  -0.000029   0.063778   0.129839  \n",
       "45   0.000601   0.064027   0.130222  \n",
       "46   0.033582   0.076415   0.131625  \n",
       "47   0.033582   0.076415   0.131625  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_M8 = tf.keras.Sequential([\n",
    "    layers.LSTM(units=64, return_sequences=True, input_shape=[52464, 9]),\n",
    "    layers.LSTM(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_M8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_M8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_M8.fit(np.array(Day0).reshape(52464, 1, 9), np.array(Day8).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_M8 = np.squeeze(model_M8.predict(np.array(df_test0).reshape(3888, 1, 9)))\n",
    "    pred_M8 = pd.DataFrame(pred_M8)\n",
    "    result_M8 = pd.concat([result_M8, pred_M8], axis=1)\n",
    "    \n",
    "result_M8[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_L0 = pd.DataFrame(results_1.sort_index())\n",
    "res_L0.columns = ['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']\n",
    "res_L1 = pd.DataFrame(results_2.sort_index())\n",
    "res_L1.columns = ['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']\n",
    "\n",
    "res_D0 = pd.DataFrame(results[0].sort_index())\n",
    "res_D0.columns = ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9']\n",
    "res_D1 = pd.DataFrame(results[1].sort_index())\n",
    "res_D1.columns = ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9']\n",
    "\n",
    "res_C0 = pd.DataFrame(result7.sort_index())\n",
    "res_C0.columns = ['C00.1','C00.2','C00.3','C00.4','C00.5','C00.6','C00.7','C00.8','C00.9']\n",
    "res_C1 = pd.DataFrame(result8.sort_index())\n",
    "res_C1.columns = ['C10.1','C10.2','C10.3','C10.4','C10.5','C10.6','C10.7','C10.8','C10.9']\n",
    "\n",
    "res_G0 = pd.DataFrame(result_G7.sort_index())\n",
    "res_G0.columns = ['G00.1','G00.2','G00.3','G00.4','G00.5','G00.6','G00.7','G00.8','G00.9']\n",
    "res_G1 = pd.DataFrame(result_G8.sort_index())\n",
    "res_G1.columns = ['G10.1','G10.2','G10.3','G10.4','G10.5','G10.6','G10.7','G10.8','G10.9']\n",
    "\n",
    "res_M0 = pd.DataFrame(result_M7.sort_index())\n",
    "res_M0.columns = ['M00.1','M00.2','M00.3','M00.4','M00.5','M00.6','M00.7','M00.8','M00.9']\n",
    "res_M1 = pd.DataFrame(result_M8.sort_index())\n",
    "res_M1.columns = ['M10.1','M10.2','M10.3','M10.4','M10.5','M10.6','M10.7','M10.8','M10.9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0 = pd.DataFrame()\n",
    "res_1= pd.DataFrame()\n",
    "res_0 = pd.concat([res_L0, res_D0, res_C0, res_G0, res_M0], axis=1)\n",
    "res_1 = pd.concat([res_L1, res_D1, res_C1, res_G1, res_M1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L00.1</th>\n",
       "      <th>L00.2</th>\n",
       "      <th>L00.3</th>\n",
       "      <th>L00.4</th>\n",
       "      <th>L00.5</th>\n",
       "      <th>L00.6</th>\n",
       "      <th>L00.7</th>\n",
       "      <th>L00.8</th>\n",
       "      <th>L00.9</th>\n",
       "      <th>D00.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G00.9</th>\n",
       "      <th>M00.1</th>\n",
       "      <th>M00.2</th>\n",
       "      <th>M00.3</th>\n",
       "      <th>M00.4</th>\n",
       "      <th>M00.5</th>\n",
       "      <th>M00.6</th>\n",
       "      <th>M00.7</th>\n",
       "      <th>M00.8</th>\n",
       "      <th>M00.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.173280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>-0.025156</td>\n",
       "      <td>-0.013253</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>-0.010397</td>\n",
       "      <td>-0.009584</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.023365</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>0.061637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.175137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>-0.025134</td>\n",
       "      <td>-0.013022</td>\n",
       "      <td>-0.002140</td>\n",
       "      <td>-0.010277</td>\n",
       "      <td>-0.009148</td>\n",
       "      <td>0.011234</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.060183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.168723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>-0.024511</td>\n",
       "      <td>-0.012978</td>\n",
       "      <td>-0.002520</td>\n",
       "      <td>-0.011473</td>\n",
       "      <td>-0.011140</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.058211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.167419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>-0.024558</td>\n",
       "      <td>-0.013143</td>\n",
       "      <td>-0.002306</td>\n",
       "      <td>-0.011226</td>\n",
       "      <td>-0.011087</td>\n",
       "      <td>0.006331</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.059596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.140366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008652</td>\n",
       "      <td>-0.023976</td>\n",
       "      <td>-0.013308</td>\n",
       "      <td>-0.002698</td>\n",
       "      <td>-0.012568</td>\n",
       "      <td>-0.013493</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>-0.006729</td>\n",
       "      <td>0.058327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.143237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>-0.023961</td>\n",
       "      <td>-0.012929</td>\n",
       "      <td>-0.002689</td>\n",
       "      <td>-0.012271</td>\n",
       "      <td>-0.012604</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.005386</td>\n",
       "      <td>0.057103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.136401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>-0.023437</td>\n",
       "      <td>-0.012877</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>-0.012813</td>\n",
       "      <td>-0.013736</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.004479</td>\n",
       "      <td>-0.011112</td>\n",
       "      <td>0.056343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.143101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012211</td>\n",
       "      <td>-0.023514</td>\n",
       "      <td>-0.011895</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>-0.012304</td>\n",
       "      <td>-0.012060</td>\n",
       "      <td>-0.003623</td>\n",
       "      <td>-0.004381</td>\n",
       "      <td>-0.008891</td>\n",
       "      <td>0.055771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.141119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015793</td>\n",
       "      <td>-0.023176</td>\n",
       "      <td>-0.011118</td>\n",
       "      <td>-0.000895</td>\n",
       "      <td>-0.011551</td>\n",
       "      <td>-0.011026</td>\n",
       "      <td>-0.005771</td>\n",
       "      <td>-0.009467</td>\n",
       "      <td>-0.010279</td>\n",
       "      <td>0.059837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.146155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>-0.023502</td>\n",
       "      <td>-0.010486</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.010635</td>\n",
       "      <td>-0.009500</td>\n",
       "      <td>-0.006829</td>\n",
       "      <td>-0.009509</td>\n",
       "      <td>-0.009076</td>\n",
       "      <td>0.060506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.142657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020646</td>\n",
       "      <td>-0.023278</td>\n",
       "      <td>-0.009931</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>-0.009511</td>\n",
       "      <td>-0.008121</td>\n",
       "      <td>-0.006645</td>\n",
       "      <td>-0.011970</td>\n",
       "      <td>-0.007594</td>\n",
       "      <td>0.069285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.145219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>-0.023365</td>\n",
       "      <td>-0.009879</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>-0.009500</td>\n",
       "      <td>-0.008156</td>\n",
       "      <td>-0.006855</td>\n",
       "      <td>-0.011897</td>\n",
       "      <td>-0.007724</td>\n",
       "      <td>0.067764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.136755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024040</td>\n",
       "      <td>-0.023074</td>\n",
       "      <td>-0.009769</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>-0.008633</td>\n",
       "      <td>-0.007188</td>\n",
       "      <td>-0.005436</td>\n",
       "      <td>-0.012513</td>\n",
       "      <td>-0.004907</td>\n",
       "      <td>0.077859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.139141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023545</td>\n",
       "      <td>-0.023172</td>\n",
       "      <td>-0.009738</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>-0.008737</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-0.005816</td>\n",
       "      <td>-0.012639</td>\n",
       "      <td>-0.005269</td>\n",
       "      <td>0.076412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.125433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028736</td>\n",
       "      <td>-0.022994</td>\n",
       "      <td>-0.009570</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>-0.006896</td>\n",
       "      <td>-0.004854</td>\n",
       "      <td>-0.003355</td>\n",
       "      <td>-0.011098</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.089936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "      <td>0.438392</td>\n",
       "      <td>...</td>\n",
       "      <td>10.337276</td>\n",
       "      <td>0.278376</td>\n",
       "      <td>0.533850</td>\n",
       "      <td>0.851767</td>\n",
       "      <td>1.135873</td>\n",
       "      <td>1.901088</td>\n",
       "      <td>3.958309</td>\n",
       "      <td>5.576889</td>\n",
       "      <td>8.032295</td>\n",
       "      <td>11.671991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "      <td>0.898831</td>\n",
       "      <td>...</td>\n",
       "      <td>29.208799</td>\n",
       "      <td>1.382670</td>\n",
       "      <td>2.785345</td>\n",
       "      <td>4.602665</td>\n",
       "      <td>6.151273</td>\n",
       "      <td>9.155155</td>\n",
       "      <td>12.433868</td>\n",
       "      <td>14.405657</td>\n",
       "      <td>20.825100</td>\n",
       "      <td>29.005898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "      <td>2.243876</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273144</td>\n",
       "      <td>2.625676</td>\n",
       "      <td>5.551159</td>\n",
       "      <td>9.029684</td>\n",
       "      <td>11.174472</td>\n",
       "      <td>14.575258</td>\n",
       "      <td>18.475082</td>\n",
       "      <td>19.700142</td>\n",
       "      <td>27.711407</td>\n",
       "      <td>36.345173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "      <td>7.654971</td>\n",
       "      <td>...</td>\n",
       "      <td>44.152420</td>\n",
       "      <td>3.949140</td>\n",
       "      <td>8.700056</td>\n",
       "      <td>13.569352</td>\n",
       "      <td>15.761546</td>\n",
       "      <td>19.214750</td>\n",
       "      <td>24.219654</td>\n",
       "      <td>24.601713</td>\n",
       "      <td>33.837654</td>\n",
       "      <td>42.674992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "      <td>8.701793</td>\n",
       "      <td>...</td>\n",
       "      <td>43.151070</td>\n",
       "      <td>5.421922</td>\n",
       "      <td>12.015749</td>\n",
       "      <td>17.605280</td>\n",
       "      <td>19.513790</td>\n",
       "      <td>22.503637</td>\n",
       "      <td>27.247337</td>\n",
       "      <td>25.901115</td>\n",
       "      <td>34.847942</td>\n",
       "      <td>41.016430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "      <td>8.442618</td>\n",
       "      <td>...</td>\n",
       "      <td>39.635620</td>\n",
       "      <td>8.717727</td>\n",
       "      <td>18.720308</td>\n",
       "      <td>24.695587</td>\n",
       "      <td>24.716814</td>\n",
       "      <td>26.385412</td>\n",
       "      <td>31.913935</td>\n",
       "      <td>29.011997</td>\n",
       "      <td>34.564465</td>\n",
       "      <td>34.320408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "      <td>8.227545</td>\n",
       "      <td>...</td>\n",
       "      <td>38.851074</td>\n",
       "      <td>9.944949</td>\n",
       "      <td>20.628939</td>\n",
       "      <td>26.114195</td>\n",
       "      <td>25.301535</td>\n",
       "      <td>27.607227</td>\n",
       "      <td>34.258957</td>\n",
       "      <td>32.246815</td>\n",
       "      <td>36.913647</td>\n",
       "      <td>36.626324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "      <td>10.879261</td>\n",
       "      <td>...</td>\n",
       "      <td>54.107361</td>\n",
       "      <td>9.114343</td>\n",
       "      <td>20.005499</td>\n",
       "      <td>27.539772</td>\n",
       "      <td>28.713305</td>\n",
       "      <td>30.506847</td>\n",
       "      <td>37.443073</td>\n",
       "      <td>34.428154</td>\n",
       "      <td>42.832264</td>\n",
       "      <td>45.281658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "      <td>11.177592</td>\n",
       "      <td>...</td>\n",
       "      <td>52.995789</td>\n",
       "      <td>7.772636</td>\n",
       "      <td>17.395922</td>\n",
       "      <td>24.138544</td>\n",
       "      <td>25.423502</td>\n",
       "      <td>27.575884</td>\n",
       "      <td>33.603493</td>\n",
       "      <td>30.786173</td>\n",
       "      <td>39.545959</td>\n",
       "      <td>43.069870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "      <td>10.822333</td>\n",
       "      <td>...</td>\n",
       "      <td>49.077896</td>\n",
       "      <td>7.626348</td>\n",
       "      <td>17.201427</td>\n",
       "      <td>23.625643</td>\n",
       "      <td>24.337145</td>\n",
       "      <td>26.427290</td>\n",
       "      <td>32.402966</td>\n",
       "      <td>29.736628</td>\n",
       "      <td>37.601875</td>\n",
       "      <td>40.233265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "      <td>9.649780</td>\n",
       "      <td>...</td>\n",
       "      <td>46.703388</td>\n",
       "      <td>8.852440</td>\n",
       "      <td>19.440535</td>\n",
       "      <td>26.092402</td>\n",
       "      <td>26.135069</td>\n",
       "      <td>28.116590</td>\n",
       "      <td>34.983261</td>\n",
       "      <td>32.498928</td>\n",
       "      <td>39.111118</td>\n",
       "      <td>40.255474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "      <td>9.131831</td>\n",
       "      <td>...</td>\n",
       "      <td>43.204292</td>\n",
       "      <td>7.633096</td>\n",
       "      <td>17.258287</td>\n",
       "      <td>23.392521</td>\n",
       "      <td>23.490526</td>\n",
       "      <td>25.597803</td>\n",
       "      <td>31.786926</td>\n",
       "      <td>29.368408</td>\n",
       "      <td>36.334438</td>\n",
       "      <td>38.169403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "      <td>10.216905</td>\n",
       "      <td>...</td>\n",
       "      <td>53.884148</td>\n",
       "      <td>5.426360</td>\n",
       "      <td>12.573776</td>\n",
       "      <td>18.299803</td>\n",
       "      <td>19.517847</td>\n",
       "      <td>22.470036</td>\n",
       "      <td>28.079905</td>\n",
       "      <td>26.910032</td>\n",
       "      <td>35.878826</td>\n",
       "      <td>42.546635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "      <td>7.601222</td>\n",
       "      <td>...</td>\n",
       "      <td>37.988602</td>\n",
       "      <td>6.129554</td>\n",
       "      <td>14.129081</td>\n",
       "      <td>19.424168</td>\n",
       "      <td>19.508886</td>\n",
       "      <td>21.832975</td>\n",
       "      <td>26.938946</td>\n",
       "      <td>24.951799</td>\n",
       "      <td>31.463928</td>\n",
       "      <td>33.699268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "      <td>7.567501</td>\n",
       "      <td>...</td>\n",
       "      <td>36.058998</td>\n",
       "      <td>6.561932</td>\n",
       "      <td>14.943178</td>\n",
       "      <td>20.139486</td>\n",
       "      <td>19.879492</td>\n",
       "      <td>22.147516</td>\n",
       "      <td>27.619825</td>\n",
       "      <td>25.590836</td>\n",
       "      <td>31.595079</td>\n",
       "      <td>33.061008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "      <td>5.413460</td>\n",
       "      <td>...</td>\n",
       "      <td>28.707012</td>\n",
       "      <td>6.047947</td>\n",
       "      <td>13.872070</td>\n",
       "      <td>18.218056</td>\n",
       "      <td>17.434427</td>\n",
       "      <td>19.661671</td>\n",
       "      <td>24.608288</td>\n",
       "      <td>22.910669</td>\n",
       "      <td>27.609596</td>\n",
       "      <td>28.168516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "      <td>4.938184</td>\n",
       "      <td>...</td>\n",
       "      <td>30.306799</td>\n",
       "      <td>4.250319</td>\n",
       "      <td>9.801942</td>\n",
       "      <td>13.945416</td>\n",
       "      <td>14.109744</td>\n",
       "      <td>16.555128</td>\n",
       "      <td>20.302433</td>\n",
       "      <td>19.118517</td>\n",
       "      <td>24.372580</td>\n",
       "      <td>26.528730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "      <td>2.910076</td>\n",
       "      <td>...</td>\n",
       "      <td>21.081171</td>\n",
       "      <td>2.166086</td>\n",
       "      <td>4.919737</td>\n",
       "      <td>7.904614</td>\n",
       "      <td>8.732598</td>\n",
       "      <td>11.176006</td>\n",
       "      <td>13.758587</td>\n",
       "      <td>13.877783</td>\n",
       "      <td>18.237036</td>\n",
       "      <td>21.240238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "      <td>1.529654</td>\n",
       "      <td>...</td>\n",
       "      <td>20.914684</td>\n",
       "      <td>0.447172</td>\n",
       "      <td>0.955420</td>\n",
       "      <td>1.710934</td>\n",
       "      <td>2.428085</td>\n",
       "      <td>4.241768</td>\n",
       "      <td>7.769482</td>\n",
       "      <td>9.957658</td>\n",
       "      <td>13.421550</td>\n",
       "      <td>18.445862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.111503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016110</td>\n",
       "      <td>-0.019087</td>\n",
       "      <td>-0.010179</td>\n",
       "      <td>0.008234</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.016144</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>0.042048</td>\n",
       "      <td>0.127486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.107732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>-0.019074</td>\n",
       "      <td>-0.010748</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>-0.001864</td>\n",
       "      <td>0.014825</td>\n",
       "      <td>0.018191</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.121346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.094565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>-0.018880</td>\n",
       "      <td>-0.011613</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>-0.003298</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.016217</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>0.111468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.093457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>-0.018895</td>\n",
       "      <td>-0.012059</td>\n",
       "      <td>0.005537</td>\n",
       "      <td>-0.000337</td>\n",
       "      <td>-0.004792</td>\n",
       "      <td>0.010570</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>0.030661</td>\n",
       "      <td>0.107538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.097099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009986</td>\n",
       "      <td>-0.018718</td>\n",
       "      <td>-0.012907</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>-0.000382</td>\n",
       "      <td>-0.005830</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.012276</td>\n",
       "      <td>0.025599</td>\n",
       "      <td>0.097012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.095971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009819</td>\n",
       "      <td>-0.018743</td>\n",
       "      <td>-0.013371</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.023280</td>\n",
       "      <td>0.093670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.079436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008106</td>\n",
       "      <td>-0.018607</td>\n",
       "      <td>-0.014290</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>-0.008551</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>0.082149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.082888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008024</td>\n",
       "      <td>-0.018672</td>\n",
       "      <td>-0.014690</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>-0.002696</td>\n",
       "      <td>-0.010221</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>0.015344</td>\n",
       "      <td>0.080675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.035586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>-0.018561</td>\n",
       "      <td>-0.015644</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>-0.002864</td>\n",
       "      <td>-0.011361</td>\n",
       "      <td>-0.005251</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>0.068372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.037804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007029</td>\n",
       "      <td>-0.018628</td>\n",
       "      <td>-0.016210</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.004417</td>\n",
       "      <td>-0.013515</td>\n",
       "      <td>-0.005692</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.066969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.014040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>-0.018579</td>\n",
       "      <td>-0.017376</td>\n",
       "      <td>-0.001560</td>\n",
       "      <td>-0.005358</td>\n",
       "      <td>-0.015743</td>\n",
       "      <td>-0.012552</td>\n",
       "      <td>-0.004824</td>\n",
       "      <td>-0.001577</td>\n",
       "      <td>0.054842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.013162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>-0.018607</td>\n",
       "      <td>-0.017457</td>\n",
       "      <td>-0.001805</td>\n",
       "      <td>-0.005639</td>\n",
       "      <td>-0.016169</td>\n",
       "      <td>-0.012681</td>\n",
       "      <td>-0.004998</td>\n",
       "      <td>-0.001723</td>\n",
       "      <td>0.055233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.042704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>-0.018571</td>\n",
       "      <td>-0.018396</td>\n",
       "      <td>-0.002754</td>\n",
       "      <td>-0.006238</td>\n",
       "      <td>-0.018174</td>\n",
       "      <td>-0.021020</td>\n",
       "      <td>-0.012369</td>\n",
       "      <td>-0.011444</td>\n",
       "      <td>0.043390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.042704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>-0.018571</td>\n",
       "      <td>-0.018396</td>\n",
       "      <td>-0.002754</td>\n",
       "      <td>-0.006238</td>\n",
       "      <td>-0.018174</td>\n",
       "      <td>-0.021020</td>\n",
       "      <td>-0.012369</td>\n",
       "      <td>-0.011444</td>\n",
       "      <td>0.043390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L00.1  L00.2  L00.3  L00.4  L00.5  L00.6  L00.7  L00.8  L00.9      D00.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.173280   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.175137   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.168723   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.167419   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.140366   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.143237   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.136401   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.143101   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.141119   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.146155   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.142657   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.145219   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.136755   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.139141   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.125433   \n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45   0.438392   \n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51   0.898831   \n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11   2.243876   \n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34   7.654971   \n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51   8.701793   \n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17   8.442618   \n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03   8.227545   \n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17  10.879261   \n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82  11.177592   \n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14  10.822333   \n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61   9.649780   \n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92   9.131831   \n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20  10.216905   \n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73   7.601222   \n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94   7.567501   \n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20   5.413460   \n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59   4.938184   \n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49   2.910076   \n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13   1.529654   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.111503   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.107732   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.094565   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.093457   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.097099   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.095971   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.079436   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.082888   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.035586   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.037804   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.014040   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.013162   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.042704   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.042704   \n",
       "\n",
       "    ...      G00.9     M00.1      M00.2      M00.3      M00.4      M00.5  \\\n",
       "0   ...   0.008211 -0.025156  -0.013253  -0.002063  -0.010397  -0.009584   \n",
       "1   ...   0.008475 -0.025134  -0.013022  -0.002140  -0.010277  -0.009148   \n",
       "2   ...   0.008420 -0.024511  -0.012978  -0.002520  -0.011473  -0.011140   \n",
       "3   ...   0.008217 -0.024558  -0.013143  -0.002306  -0.011226  -0.011087   \n",
       "4   ...   0.008652 -0.023976  -0.013308  -0.002698  -0.012568  -0.013493   \n",
       "5   ...   0.009237 -0.023961  -0.012929  -0.002689  -0.012271  -0.012604   \n",
       "6   ...   0.010883 -0.023437  -0.012877  -0.002717  -0.012813  -0.013736   \n",
       "7   ...   0.012211 -0.023514  -0.011895  -0.002166  -0.012304  -0.012060   \n",
       "8   ...   0.015793 -0.023176  -0.011118  -0.000895  -0.011551  -0.011026   \n",
       "9   ...   0.016471 -0.023502  -0.010486  -0.000027  -0.010635  -0.009500   \n",
       "10  ...   0.020646 -0.023278  -0.009931   0.001432  -0.009511  -0.008121   \n",
       "11  ...   0.020226 -0.023365  -0.009879   0.001393  -0.009500  -0.008156   \n",
       "12  ...   0.024040 -0.023074  -0.009769   0.002691  -0.008633  -0.007188   \n",
       "13  ...   0.023545 -0.023172  -0.009738   0.002620  -0.008737  -0.007413   \n",
       "14  ...   0.028736 -0.022994  -0.009570   0.004730  -0.006896  -0.004854   \n",
       "15  ...  10.337276  0.278376   0.533850   0.851767   1.135873   1.901088   \n",
       "16  ...  29.208799  1.382670   2.785345   4.602665   6.151273   9.155155   \n",
       "17  ...  32.273144  2.625676   5.551159   9.029684  11.174472  14.575258   \n",
       "18  ...  44.152420  3.949140   8.700056  13.569352  15.761546  19.214750   \n",
       "19  ...  43.151070  5.421922  12.015749  17.605280  19.513790  22.503637   \n",
       "20  ...  39.635620  8.717727  18.720308  24.695587  24.716814  26.385412   \n",
       "21  ...  38.851074  9.944949  20.628939  26.114195  25.301535  27.607227   \n",
       "22  ...  54.107361  9.114343  20.005499  27.539772  28.713305  30.506847   \n",
       "23  ...  52.995789  7.772636  17.395922  24.138544  25.423502  27.575884   \n",
       "24  ...  49.077896  7.626348  17.201427  23.625643  24.337145  26.427290   \n",
       "25  ...  46.703388  8.852440  19.440535  26.092402  26.135069  28.116590   \n",
       "26  ...  43.204292  7.633096  17.258287  23.392521  23.490526  25.597803   \n",
       "27  ...  53.884148  5.426360  12.573776  18.299803  19.517847  22.470036   \n",
       "28  ...  37.988602  6.129554  14.129081  19.424168  19.508886  21.832975   \n",
       "29  ...  36.058998  6.561932  14.943178  20.139486  19.879492  22.147516   \n",
       "30  ...  28.707012  6.047947  13.872070  18.218056  17.434427  19.661671   \n",
       "31  ...  30.306799  4.250319   9.801942  13.945416  14.109744  16.555128   \n",
       "32  ...  21.081171  2.166086   4.919737   7.904614   8.732598  11.176006   \n",
       "33  ...  20.914684  0.447172   0.955420   1.710934   2.428085   4.241768   \n",
       "34  ...   0.016110 -0.019087  -0.010179   0.008234   0.001928  -0.000037   \n",
       "35  ...   0.016097 -0.019074  -0.010748   0.007256   0.000840  -0.001864   \n",
       "36  ...   0.012737 -0.018880  -0.011613   0.006416   0.000609  -0.003298   \n",
       "37  ...   0.012634 -0.018895  -0.012059   0.005537  -0.000337  -0.004792   \n",
       "38  ...   0.009986 -0.018718  -0.012907   0.004820  -0.000382  -0.005830   \n",
       "39  ...   0.009819 -0.018743  -0.013371   0.003830  -0.001418  -0.007380   \n",
       "40  ...   0.008106 -0.018607  -0.014290   0.003038  -0.001584  -0.008551   \n",
       "41  ...   0.008024 -0.018672  -0.014690   0.002030  -0.002696  -0.010221   \n",
       "42  ...   0.007233 -0.018561  -0.015644   0.001293  -0.002864  -0.011361   \n",
       "43  ...   0.007029 -0.018628  -0.016210  -0.000190  -0.004417  -0.013515   \n",
       "44  ...   0.007028 -0.018579  -0.017376  -0.001560  -0.005358  -0.015743   \n",
       "45  ...   0.007072 -0.018607  -0.017457  -0.001805  -0.005639  -0.016169   \n",
       "46  ...   0.007955 -0.018571  -0.018396  -0.002754  -0.006238  -0.018174   \n",
       "47  ...   0.007955 -0.018571  -0.018396  -0.002754  -0.006238  -0.018174   \n",
       "\n",
       "        M00.6      M00.7      M00.8      M00.9  \n",
       "0    0.011747   0.023365   0.010626   0.061637  \n",
       "1    0.011234   0.023427   0.010931   0.060183  \n",
       "2    0.005508   0.012225   0.001916   0.058211  \n",
       "3    0.006331   0.012539   0.001968   0.059596  \n",
       "4    0.001924   0.002373  -0.006729   0.058327  \n",
       "5    0.001386   0.002988  -0.005386   0.057103  \n",
       "6   -0.001402  -0.004479  -0.011112   0.056343  \n",
       "7   -0.003623  -0.004381  -0.008891   0.055771  \n",
       "8   -0.005771  -0.009467  -0.010279   0.059837  \n",
       "9   -0.006829  -0.009509  -0.009076   0.060506  \n",
       "10  -0.006645  -0.011970  -0.007594   0.069285  \n",
       "11  -0.006855  -0.011897  -0.007724   0.067764  \n",
       "12  -0.005436  -0.012513  -0.004907   0.077859  \n",
       "13  -0.005816  -0.012639  -0.005269   0.076412  \n",
       "14  -0.003355  -0.011098   0.000618   0.089936  \n",
       "15   3.958309   5.576889   8.032295  11.671991  \n",
       "16  12.433868  14.405657  20.825100  29.005898  \n",
       "17  18.475082  19.700142  27.711407  36.345173  \n",
       "18  24.219654  24.601713  33.837654  42.674992  \n",
       "19  27.247337  25.901115  34.847942  41.016430  \n",
       "20  31.913935  29.011997  34.564465  34.320408  \n",
       "21  34.258957  32.246815  36.913647  36.626324  \n",
       "22  37.443073  34.428154  42.832264  45.281658  \n",
       "23  33.603493  30.786173  39.545959  43.069870  \n",
       "24  32.402966  29.736628  37.601875  40.233265  \n",
       "25  34.983261  32.498928  39.111118  40.255474  \n",
       "26  31.786926  29.368408  36.334438  38.169403  \n",
       "27  28.079905  26.910032  35.878826  42.546635  \n",
       "28  26.938946  24.951799  31.463928  33.699268  \n",
       "29  27.619825  25.590836  31.595079  33.061008  \n",
       "30  24.608288  22.910669  27.609596  28.168516  \n",
       "31  20.302433  19.118517  24.372580  26.528730  \n",
       "32  13.758587  13.877783  18.237036  21.240238  \n",
       "33   7.769482   9.957658  13.421550  18.445862  \n",
       "34   0.016144   0.020384   0.042048   0.127486  \n",
       "35   0.014825   0.018191   0.038080   0.121346  \n",
       "36   0.011465   0.016217   0.033354   0.111468  \n",
       "37   0.010570   0.014691   0.030661   0.107538  \n",
       "38   0.006707   0.012276   0.025599   0.097012  \n",
       "39   0.005987   0.010971   0.023280   0.093670  \n",
       "40   0.001186   0.007525   0.017151   0.082149  \n",
       "41   0.000430   0.006318   0.015344   0.080675  \n",
       "42  -0.005251   0.001912   0.008329   0.068372  \n",
       "43  -0.005692   0.000982   0.006755   0.066969  \n",
       "44  -0.012552  -0.004824  -0.001577   0.054842  \n",
       "45  -0.012681  -0.004998  -0.001723   0.055233  \n",
       "46  -0.021020  -0.012369  -0.011444   0.043390  \n",
       "47  -0.021020  -0.012369  -0.011444   0.043390  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_0[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L10.1</th>\n",
       "      <th>L10.2</th>\n",
       "      <th>L10.3</th>\n",
       "      <th>L10.4</th>\n",
       "      <th>L10.5</th>\n",
       "      <th>L10.6</th>\n",
       "      <th>L10.7</th>\n",
       "      <th>L10.8</th>\n",
       "      <th>L10.9</th>\n",
       "      <th>D10.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G10.9</th>\n",
       "      <th>M10.1</th>\n",
       "      <th>M10.2</th>\n",
       "      <th>M10.3</th>\n",
       "      <th>M10.4</th>\n",
       "      <th>M10.5</th>\n",
       "      <th>M10.6</th>\n",
       "      <th>M10.7</th>\n",
       "      <th>M10.8</th>\n",
       "      <th>M10.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.196271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>-0.008475</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>-0.010970</td>\n",
       "      <td>-0.009766</td>\n",
       "      <td>-0.001435</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.007745</td>\n",
       "      <td>-0.001998</td>\n",
       "      <td>0.013644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.198651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>-0.008172</td>\n",
       "      <td>-0.002848</td>\n",
       "      <td>-0.011345</td>\n",
       "      <td>-0.008944</td>\n",
       "      <td>-0.001234</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>-0.001692</td>\n",
       "      <td>0.013986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.202375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>-0.007855</td>\n",
       "      <td>-0.002624</td>\n",
       "      <td>-0.011701</td>\n",
       "      <td>-0.007621</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.007745</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.018128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.200499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>-0.008168</td>\n",
       "      <td>-0.003216</td>\n",
       "      <td>-0.011477</td>\n",
       "      <td>-0.008326</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.007057</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.017873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.202891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.003836</td>\n",
       "      <td>-0.011515</td>\n",
       "      <td>-0.008049</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.020448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.206415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>-0.007627</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>-0.011969</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.020618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.208896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007543</td>\n",
       "      <td>-0.007451</td>\n",
       "      <td>-0.002027</td>\n",
       "      <td>-0.012095</td>\n",
       "      <td>-0.004981</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>-0.003716</td>\n",
       "      <td>-0.004533</td>\n",
       "      <td>0.022629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.218464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>-0.006254</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.013250</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.011593</td>\n",
       "      <td>-0.000667</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>0.023953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.228049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>-0.005532</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>-0.014068</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>-0.005948</td>\n",
       "      <td>-0.011721</td>\n",
       "      <td>0.032198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.234109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011016</td>\n",
       "      <td>-0.005155</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.015745</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>0.011386</td>\n",
       "      <td>0.015880</td>\n",
       "      <td>-0.004026</td>\n",
       "      <td>-0.010507</td>\n",
       "      <td>0.036050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.241534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>-0.004814</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>-0.016484</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.015319</td>\n",
       "      <td>0.018398</td>\n",
       "      <td>-0.007913</td>\n",
       "      <td>-0.012019</td>\n",
       "      <td>0.061850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.243413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>-0.004856</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>-0.017231</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>0.015328</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>-0.007909</td>\n",
       "      <td>-0.010908</td>\n",
       "      <td>0.065133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.244977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013974</td>\n",
       "      <td>-0.004640</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>-0.015750</td>\n",
       "      <td>0.010525</td>\n",
       "      <td>0.020229</td>\n",
       "      <td>0.021559</td>\n",
       "      <td>-0.007937</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.126479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.246698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>-0.016534</td>\n",
       "      <td>0.010466</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.021086</td>\n",
       "      <td>-0.008539</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.129978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.241465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017818</td>\n",
       "      <td>-0.004304</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>-0.013418</td>\n",
       "      <td>0.014647</td>\n",
       "      <td>0.026919</td>\n",
       "      <td>0.028038</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.065109</td>\n",
       "      <td>0.273918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.87</td>\n",
       "      <td>-0.106694</td>\n",
       "      <td>...</td>\n",
       "      <td>8.146265</td>\n",
       "      <td>0.224555</td>\n",
       "      <td>0.493217</td>\n",
       "      <td>0.769333</td>\n",
       "      <td>1.291518</td>\n",
       "      <td>2.884746</td>\n",
       "      <td>4.748418</td>\n",
       "      <td>8.300235</td>\n",
       "      <td>13.855488</td>\n",
       "      <td>19.790730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.92</td>\n",
       "      <td>0.164094</td>\n",
       "      <td>...</td>\n",
       "      <td>23.949827</td>\n",
       "      <td>1.064352</td>\n",
       "      <td>3.094429</td>\n",
       "      <td>5.176425</td>\n",
       "      <td>8.452856</td>\n",
       "      <td>11.382863</td>\n",
       "      <td>13.115429</td>\n",
       "      <td>20.796421</td>\n",
       "      <td>27.747461</td>\n",
       "      <td>33.285465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>18.24</td>\n",
       "      <td>25.56</td>\n",
       "      <td>1.581352</td>\n",
       "      <td>...</td>\n",
       "      <td>29.552259</td>\n",
       "      <td>2.085540</td>\n",
       "      <td>6.851115</td>\n",
       "      <td>11.054815</td>\n",
       "      <td>16.175102</td>\n",
       "      <td>18.505974</td>\n",
       "      <td>19.281204</td>\n",
       "      <td>27.036049</td>\n",
       "      <td>30.534048</td>\n",
       "      <td>35.522842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.74</td>\n",
       "      <td>14.78</td>\n",
       "      <td>19.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>22.60</td>\n",
       "      <td>20.57</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.66</td>\n",
       "      <td>32.96</td>\n",
       "      <td>7.584140</td>\n",
       "      <td>...</td>\n",
       "      <td>40.693188</td>\n",
       "      <td>3.243448</td>\n",
       "      <td>10.825759</td>\n",
       "      <td>16.592876</td>\n",
       "      <td>22.638235</td>\n",
       "      <td>24.353527</td>\n",
       "      <td>24.096861</td>\n",
       "      <td>34.109833</td>\n",
       "      <td>36.764412</td>\n",
       "      <td>43.473362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.47</td>\n",
       "      <td>19.06</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.62</td>\n",
       "      <td>20.20</td>\n",
       "      <td>17.15</td>\n",
       "      <td>22.85</td>\n",
       "      <td>33.97</td>\n",
       "      <td>8.712058</td>\n",
       "      <td>...</td>\n",
       "      <td>38.026020</td>\n",
       "      <td>4.668878</td>\n",
       "      <td>14.533233</td>\n",
       "      <td>20.736805</td>\n",
       "      <td>26.693420</td>\n",
       "      <td>26.814178</td>\n",
       "      <td>24.969471</td>\n",
       "      <td>31.520792</td>\n",
       "      <td>31.285065</td>\n",
       "      <td>36.117672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.33</td>\n",
       "      <td>24.78</td>\n",
       "      <td>24.52</td>\n",
       "      <td>23.02</td>\n",
       "      <td>28.58</td>\n",
       "      <td>28.55</td>\n",
       "      <td>25.06</td>\n",
       "      <td>26.39</td>\n",
       "      <td>27.24</td>\n",
       "      <td>8.296864</td>\n",
       "      <td>...</td>\n",
       "      <td>33.450459</td>\n",
       "      <td>8.403633</td>\n",
       "      <td>21.809557</td>\n",
       "      <td>25.825407</td>\n",
       "      <td>29.473806</td>\n",
       "      <td>27.048195</td>\n",
       "      <td>27.283840</td>\n",
       "      <td>34.561665</td>\n",
       "      <td>34.875008</td>\n",
       "      <td>37.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.26</td>\n",
       "      <td>26.20</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.70</td>\n",
       "      <td>29.22</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.05</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.45</td>\n",
       "      <td>8.116883</td>\n",
       "      <td>...</td>\n",
       "      <td>36.857723</td>\n",
       "      <td>10.052383</td>\n",
       "      <td>23.739573</td>\n",
       "      <td>25.210020</td>\n",
       "      <td>27.425840</td>\n",
       "      <td>28.081005</td>\n",
       "      <td>30.603060</td>\n",
       "      <td>37.294102</td>\n",
       "      <td>37.275146</td>\n",
       "      <td>39.570808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.96</td>\n",
       "      <td>29.89</td>\n",
       "      <td>33.58</td>\n",
       "      <td>33.28</td>\n",
       "      <td>38.13</td>\n",
       "      <td>35.95</td>\n",
       "      <td>34.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.78</td>\n",
       "      <td>10.945988</td>\n",
       "      <td>...</td>\n",
       "      <td>43.487072</td>\n",
       "      <td>8.884293</td>\n",
       "      <td>23.614380</td>\n",
       "      <td>28.908573</td>\n",
       "      <td>35.237713</td>\n",
       "      <td>33.645550</td>\n",
       "      <td>32.784695</td>\n",
       "      <td>42.524303</td>\n",
       "      <td>41.881824</td>\n",
       "      <td>46.127567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.24</td>\n",
       "      <td>24.45</td>\n",
       "      <td>29.77</td>\n",
       "      <td>29.36</td>\n",
       "      <td>34.56</td>\n",
       "      <td>30.81</td>\n",
       "      <td>26.23</td>\n",
       "      <td>27.65</td>\n",
       "      <td>34.53</td>\n",
       "      <td>11.328506</td>\n",
       "      <td>...</td>\n",
       "      <td>40.628624</td>\n",
       "      <td>7.170529</td>\n",
       "      <td>20.149168</td>\n",
       "      <td>26.042404</td>\n",
       "      <td>32.277306</td>\n",
       "      <td>30.443672</td>\n",
       "      <td>28.663486</td>\n",
       "      <td>37.694725</td>\n",
       "      <td>37.923538</td>\n",
       "      <td>43.073772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.30</td>\n",
       "      <td>25.22</td>\n",
       "      <td>31.45</td>\n",
       "      <td>29.95</td>\n",
       "      <td>34.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>29.10</td>\n",
       "      <td>27.16</td>\n",
       "      <td>31.12</td>\n",
       "      <td>10.933179</td>\n",
       "      <td>...</td>\n",
       "      <td>37.329742</td>\n",
       "      <td>7.006989</td>\n",
       "      <td>19.508602</td>\n",
       "      <td>24.621035</td>\n",
       "      <td>30.483465</td>\n",
       "      <td>28.035336</td>\n",
       "      <td>27.140644</td>\n",
       "      <td>35.836781</td>\n",
       "      <td>36.353233</td>\n",
       "      <td>40.748795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.03</td>\n",
       "      <td>30.02</td>\n",
       "      <td>34.72</td>\n",
       "      <td>32.49</td>\n",
       "      <td>35.12</td>\n",
       "      <td>35.82</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "      <td>9.595832</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761215</td>\n",
       "      <td>8.630649</td>\n",
       "      <td>22.244896</td>\n",
       "      <td>25.697399</td>\n",
       "      <td>30.706722</td>\n",
       "      <td>29.184013</td>\n",
       "      <td>30.292667</td>\n",
       "      <td>39.333092</td>\n",
       "      <td>39.423084</td>\n",
       "      <td>42.326111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.21</td>\n",
       "      <td>27.84</td>\n",
       "      <td>33.51</td>\n",
       "      <td>32.70</td>\n",
       "      <td>37.10</td>\n",
       "      <td>34.75</td>\n",
       "      <td>29.17</td>\n",
       "      <td>27.61</td>\n",
       "      <td>29.70</td>\n",
       "      <td>9.052801</td>\n",
       "      <td>...</td>\n",
       "      <td>36.849087</td>\n",
       "      <td>7.066695</td>\n",
       "      <td>19.182344</td>\n",
       "      <td>23.245707</td>\n",
       "      <td>28.614256</td>\n",
       "      <td>26.160431</td>\n",
       "      <td>26.501652</td>\n",
       "      <td>34.652939</td>\n",
       "      <td>35.096764</td>\n",
       "      <td>37.877037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.50</td>\n",
       "      <td>22.99</td>\n",
       "      <td>30.11</td>\n",
       "      <td>31.82</td>\n",
       "      <td>32.80</td>\n",
       "      <td>33.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>36.02</td>\n",
       "      <td>44.45</td>\n",
       "      <td>10.307910</td>\n",
       "      <td>...</td>\n",
       "      <td>43.257210</td>\n",
       "      <td>4.621830</td>\n",
       "      <td>14.512002</td>\n",
       "      <td>20.495499</td>\n",
       "      <td>27.096788</td>\n",
       "      <td>25.522949</td>\n",
       "      <td>24.113199</td>\n",
       "      <td>33.200935</td>\n",
       "      <td>33.946140</td>\n",
       "      <td>42.417927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.97</td>\n",
       "      <td>21.03</td>\n",
       "      <td>25.88</td>\n",
       "      <td>24.37</td>\n",
       "      <td>28.26</td>\n",
       "      <td>26.10</td>\n",
       "      <td>21.49</td>\n",
       "      <td>20.61</td>\n",
       "      <td>26.21</td>\n",
       "      <td>7.386799</td>\n",
       "      <td>...</td>\n",
       "      <td>33.641750</td>\n",
       "      <td>5.364833</td>\n",
       "      <td>15.456990</td>\n",
       "      <td>19.887440</td>\n",
       "      <td>25.163893</td>\n",
       "      <td>21.756081</td>\n",
       "      <td>21.611567</td>\n",
       "      <td>27.556894</td>\n",
       "      <td>28.349117</td>\n",
       "      <td>30.159492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>22.61</td>\n",
       "      <td>27.12</td>\n",
       "      <td>27.53</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.98</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.92</td>\n",
       "      <td>25.82</td>\n",
       "      <td>7.341166</td>\n",
       "      <td>...</td>\n",
       "      <td>33.069721</td>\n",
       "      <td>5.919316</td>\n",
       "      <td>16.356092</td>\n",
       "      <td>20.091255</td>\n",
       "      <td>24.842615</td>\n",
       "      <td>21.822651</td>\n",
       "      <td>22.467485</td>\n",
       "      <td>28.873140</td>\n",
       "      <td>29.784460</td>\n",
       "      <td>31.264435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.57</td>\n",
       "      <td>17.69</td>\n",
       "      <td>21.43</td>\n",
       "      <td>22.13</td>\n",
       "      <td>22.79</td>\n",
       "      <td>24.23</td>\n",
       "      <td>23.32</td>\n",
       "      <td>21.47</td>\n",
       "      <td>23.67</td>\n",
       "      <td>4.992326</td>\n",
       "      <td>...</td>\n",
       "      <td>29.133614</td>\n",
       "      <td>5.305933</td>\n",
       "      <td>14.342173</td>\n",
       "      <td>16.965315</td>\n",
       "      <td>20.581347</td>\n",
       "      <td>18.054493</td>\n",
       "      <td>19.777199</td>\n",
       "      <td>25.084467</td>\n",
       "      <td>26.006908</td>\n",
       "      <td>26.629461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.19</td>\n",
       "      <td>15.84</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.86</td>\n",
       "      <td>15.93</td>\n",
       "      <td>16.22</td>\n",
       "      <td>15.55</td>\n",
       "      <td>15.47</td>\n",
       "      <td>24.20</td>\n",
       "      <td>4.490984</td>\n",
       "      <td>...</td>\n",
       "      <td>27.370882</td>\n",
       "      <td>3.503308</td>\n",
       "      <td>10.973215</td>\n",
       "      <td>15.272486</td>\n",
       "      <td>20.004910</td>\n",
       "      <td>15.951016</td>\n",
       "      <td>15.694623</td>\n",
       "      <td>19.287167</td>\n",
       "      <td>20.345507</td>\n",
       "      <td>21.038740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.95</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>15.83</td>\n",
       "      <td>2.306039</td>\n",
       "      <td>...</td>\n",
       "      <td>18.221132</td>\n",
       "      <td>1.624987</td>\n",
       "      <td>5.569398</td>\n",
       "      <td>9.298272</td>\n",
       "      <td>13.697021</td>\n",
       "      <td>10.722804</td>\n",
       "      <td>10.207670</td>\n",
       "      <td>11.319527</td>\n",
       "      <td>11.952787</td>\n",
       "      <td>12.671544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>13.80</td>\n",
       "      <td>0.968652</td>\n",
       "      <td>...</td>\n",
       "      <td>16.878769</td>\n",
       "      <td>0.319868</td>\n",
       "      <td>0.871121</td>\n",
       "      <td>1.711004</td>\n",
       "      <td>3.198543</td>\n",
       "      <td>5.575622</td>\n",
       "      <td>7.700372</td>\n",
       "      <td>10.453104</td>\n",
       "      <td>13.157294</td>\n",
       "      <td>17.465687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.179626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026779</td>\n",
       "      <td>-0.005308</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>0.021461</td>\n",
       "      <td>0.039664</td>\n",
       "      <td>0.047273</td>\n",
       "      <td>-0.074295</td>\n",
       "      <td>0.027296</td>\n",
       "      <td>0.197710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.175473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025720</td>\n",
       "      <td>-0.005422</td>\n",
       "      <td>0.008836</td>\n",
       "      <td>-0.004951</td>\n",
       "      <td>0.020388</td>\n",
       "      <td>0.040092</td>\n",
       "      <td>0.046583</td>\n",
       "      <td>-0.072800</td>\n",
       "      <td>0.028004</td>\n",
       "      <td>0.195592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.158388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.005499</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>-0.004609</td>\n",
       "      <td>0.019282</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>0.044214</td>\n",
       "      <td>-0.077383</td>\n",
       "      <td>0.013301</td>\n",
       "      <td>0.144659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.156579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017574</td>\n",
       "      <td>-0.005593</td>\n",
       "      <td>0.007339</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>0.018222</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>0.043812</td>\n",
       "      <td>-0.075743</td>\n",
       "      <td>0.014221</td>\n",
       "      <td>0.144194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.154233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>-0.005612</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>-0.005219</td>\n",
       "      <td>0.016850</td>\n",
       "      <td>0.034976</td>\n",
       "      <td>0.039661</td>\n",
       "      <td>-0.072365</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.119468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.152328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014816</td>\n",
       "      <td>-0.005716</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>0.015538</td>\n",
       "      <td>0.035279</td>\n",
       "      <td>0.039408</td>\n",
       "      <td>-0.070370</td>\n",
       "      <td>0.015380</td>\n",
       "      <td>0.119884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.133629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013565</td>\n",
       "      <td>-0.005685</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>-0.006816</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>0.030187</td>\n",
       "      <td>0.033065</td>\n",
       "      <td>-0.057821</td>\n",
       "      <td>0.025182</td>\n",
       "      <td>0.113186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.136170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013723</td>\n",
       "      <td>-0.005776</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>-0.007692</td>\n",
       "      <td>0.012139</td>\n",
       "      <td>0.030951</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>-0.055496</td>\n",
       "      <td>0.026256</td>\n",
       "      <td>0.114139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.092927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>-0.005683</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.024892</td>\n",
       "      <td>-0.034070</td>\n",
       "      <td>0.042322</td>\n",
       "      <td>0.118273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.094239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012180</td>\n",
       "      <td>-0.005826</td>\n",
       "      <td>-0.000524</td>\n",
       "      <td>-0.010703</td>\n",
       "      <td>0.007118</td>\n",
       "      <td>0.024766</td>\n",
       "      <td>0.025764</td>\n",
       "      <td>-0.030698</td>\n",
       "      <td>0.044806</td>\n",
       "      <td>0.120863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.068762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>-0.005720</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>-0.013604</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.016590</td>\n",
       "      <td>0.015626</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.063778</td>\n",
       "      <td>0.129839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.068997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010136</td>\n",
       "      <td>-0.005739</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-0.013752</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.017154</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.064027</td>\n",
       "      <td>0.130222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.081831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>-0.005523</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>-0.016108</td>\n",
       "      <td>-0.000979</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.033582</td>\n",
       "      <td>0.076415</td>\n",
       "      <td>0.131625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.081831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>-0.005523</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>-0.016108</td>\n",
       "      <td>-0.000979</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.033582</td>\n",
       "      <td>0.076415</td>\n",
       "      <td>0.131625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L10.1  L10.2  L10.3  L10.4  L10.5  L10.6  L10.7  L10.8  L10.9      D10.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.196271   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.198651   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.202375   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.200499   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.202891   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.206415   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.208896   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.218464   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.228049   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.234109   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.241534   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.243413   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.244977   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.246698   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.241465   \n",
       "15   0.48   0.70   1.80   2.18   0.98   3.56   2.99   6.80   9.87  -0.106694   \n",
       "16   2.67   7.45   8.62  10.08  10.06  11.52  17.00  19.79  20.92   0.164094   \n",
       "17   3.80   8.55   8.15  10.49  10.84  12.03  15.05  18.24  25.56   1.581352   \n",
       "18   7.74  14.78  19.70  18.93  22.60  20.57  16.19  15.66  32.96   7.584140   \n",
       "19   9.47  19.06  21.52  21.87  23.62  20.20  17.15  22.85  33.97   8.712058   \n",
       "20  14.33  24.78  24.52  23.02  28.58  28.55  25.06  26.39  27.24   8.296864   \n",
       "21  15.26  26.20  24.57  24.70  29.22  31.11  31.05  31.59  32.45   8.116883   \n",
       "22  15.96  29.89  33.58  33.28  38.13  35.95  34.24  33.24  33.78  10.945988   \n",
       "23  14.24  24.45  29.77  29.36  34.56  30.81  26.23  27.65  34.53  11.328506   \n",
       "24  14.30  25.22  31.45  29.95  34.70  31.31  29.10  27.16  31.12  10.933179   \n",
       "25  16.03  30.02  34.72  32.49  35.12  35.82  33.60  32.73  34.11   9.595832   \n",
       "26  15.21  27.84  33.51  32.70  37.10  34.75  29.17  27.61  29.70   9.052801   \n",
       "27   9.50  22.99  30.11  31.82  32.80  33.88  19.43  36.02  44.45  10.307910   \n",
       "28  10.97  21.03  25.88  24.37  28.26  26.10  21.49  20.61  26.21   7.386799   \n",
       "29  12.63  22.61  27.12  27.53  31.02  29.98  27.37  22.92  25.82   7.341166   \n",
       "30   8.57  17.69  21.43  22.13  22.79  24.23  23.32  21.47  23.67   4.992326   \n",
       "31   7.19  15.84  18.83  17.86  15.93  16.22  15.55  15.47  24.20   4.490984   \n",
       "32   2.95   8.16   8.37   7.24   6.39   6.81   6.33   6.51  15.83   2.306039   \n",
       "33   1.71   4.94   4.62   3.24   3.42   1.19   3.41  -0.50  13.80   0.968652   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17  -0.179626   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17  -0.175473   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.158388   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.156579   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.154233   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.152328   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.133629   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.136170   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.092927   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.094239   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.068762   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.068997   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.081831   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.081831   \n",
       "\n",
       "    ...      G10.9      M10.1      M10.2      M10.3      M10.4      M10.5  \\\n",
       "0   ...   0.005168  -0.008475  -0.003593  -0.010970  -0.009766  -0.001435   \n",
       "1   ...   0.005025  -0.008172  -0.002848  -0.011345  -0.008944  -0.001234   \n",
       "2   ...   0.003990  -0.007855  -0.002624  -0.011701  -0.007621   0.000501   \n",
       "3   ...   0.003994  -0.008168  -0.003216  -0.011477  -0.008326   0.000420   \n",
       "4   ...   0.004420  -0.008183  -0.003836  -0.011515  -0.008049   0.002103   \n",
       "5   ...   0.004595  -0.007627  -0.002345  -0.011969  -0.006306   0.002835   \n",
       "6   ...   0.007543  -0.007451  -0.002027  -0.012095  -0.004981   0.005641   \n",
       "7   ...   0.007584  -0.006254   0.000044  -0.013250  -0.001024   0.006912   \n",
       "8   ...   0.010557  -0.005532   0.000878  -0.014068   0.002965   0.010953   \n",
       "9   ...   0.011016  -0.005155   0.000487  -0.015745   0.004819   0.011386   \n",
       "10  ...   0.012708  -0.004814   0.000509  -0.016484   0.007909   0.015319   \n",
       "11  ...   0.013036  -0.004856   0.000302  -0.017231   0.007975   0.015328   \n",
       "12  ...   0.013974  -0.004640   0.001504  -0.015750   0.010525   0.020229   \n",
       "13  ...   0.014256  -0.004673   0.001286  -0.016534   0.010466   0.020188   \n",
       "14  ...   0.017818  -0.004304   0.003010  -0.013418   0.014647   0.026919   \n",
       "15  ...   8.146265   0.224555   0.493217   0.769333   1.291518   2.884746   \n",
       "16  ...  23.949827   1.064352   3.094429   5.176425   8.452856  11.382863   \n",
       "17  ...  29.552259   2.085540   6.851115  11.054815  16.175102  18.505974   \n",
       "18  ...  40.693188   3.243448  10.825759  16.592876  22.638235  24.353527   \n",
       "19  ...  38.026020   4.668878  14.533233  20.736805  26.693420  26.814178   \n",
       "20  ...  33.450459   8.403633  21.809557  25.825407  29.473806  27.048195   \n",
       "21  ...  36.857723  10.052383  23.739573  25.210020  27.425840  28.081005   \n",
       "22  ...  43.487072   8.884293  23.614380  28.908573  35.237713  33.645550   \n",
       "23  ...  40.628624   7.170529  20.149168  26.042404  32.277306  30.443672   \n",
       "24  ...  37.329742   7.006989  19.508602  24.621035  30.483465  28.035336   \n",
       "25  ...  38.761215   8.630649  22.244896  25.697399  30.706722  29.184013   \n",
       "26  ...  36.849087   7.066695  19.182344  23.245707  28.614256  26.160431   \n",
       "27  ...  43.257210   4.621830  14.512002  20.495499  27.096788  25.522949   \n",
       "28  ...  33.641750   5.364833  15.456990  19.887440  25.163893  21.756081   \n",
       "29  ...  33.069721   5.919316  16.356092  20.091255  24.842615  21.822651   \n",
       "30  ...  29.133614   5.305933  14.342173  16.965315  20.581347  18.054493   \n",
       "31  ...  27.370882   3.503308  10.973215  15.272486  20.004910  15.951016   \n",
       "32  ...  18.221132   1.624987   5.569398   9.298272  13.697021  10.722804   \n",
       "33  ...  16.878769   0.319868   0.871121   1.711004   3.198543   5.575622   \n",
       "34  ...   0.026779  -0.005308   0.009435  -0.004564   0.021461   0.039664   \n",
       "35  ...   0.025720  -0.005422   0.008836  -0.004951   0.020388   0.040092   \n",
       "36  ...   0.017769  -0.005499   0.007899  -0.004609   0.019282   0.038123   \n",
       "37  ...   0.017574  -0.005593   0.007339  -0.005167   0.018222   0.038447   \n",
       "38  ...   0.014865  -0.005612   0.005890  -0.005219   0.016850   0.034976   \n",
       "39  ...   0.014816  -0.005716   0.005202  -0.005998   0.015538   0.035279   \n",
       "40  ...   0.013565  -0.005685   0.003210  -0.006816   0.013436   0.030187   \n",
       "41  ...   0.013723  -0.005776   0.002790  -0.007692   0.012139   0.030951   \n",
       "42  ...   0.012314  -0.005683   0.000347  -0.009229   0.009308   0.023968   \n",
       "43  ...   0.012180  -0.005826  -0.000524  -0.010703   0.007118   0.024766   \n",
       "44  ...   0.010161  -0.005720  -0.003356  -0.013604   0.002662   0.016590   \n",
       "45  ...   0.010136  -0.005739  -0.003346  -0.013752   0.002435   0.017154   \n",
       "46  ...   0.008082  -0.005523  -0.005404  -0.016108  -0.000979   0.008393   \n",
       "47  ...   0.008082  -0.005523  -0.005404  -0.016108  -0.000979   0.008393   \n",
       "\n",
       "        M10.6      M10.7      M10.8      M10.9  \n",
       "0    0.009317   0.007745  -0.001998   0.013644  \n",
       "1    0.009965   0.008533  -0.001692   0.013986  \n",
       "2    0.008573   0.007745   0.001441   0.018128  \n",
       "3    0.007978   0.007057   0.001221   0.017873  \n",
       "4    0.006263   0.001220   0.000509   0.020448  \n",
       "5    0.007906   0.002986   0.000615   0.020618  \n",
       "6    0.007945  -0.003716  -0.004533   0.022629  \n",
       "7    0.011593  -0.000667  -0.004474   0.023953  \n",
       "8    0.014594  -0.005948  -0.011721   0.032198  \n",
       "9    0.015880  -0.004026  -0.010507   0.036050  \n",
       "10   0.018398  -0.007913  -0.012019   0.061850  \n",
       "11   0.018115  -0.007909  -0.010908   0.065133  \n",
       "12   0.021559  -0.007937   0.004939   0.126479  \n",
       "13   0.021086  -0.008539   0.005620   0.129978  \n",
       "14   0.028038   0.000494   0.065109   0.273918  \n",
       "15   4.748418   8.300235  13.855488  19.790730  \n",
       "16  13.115429  20.796421  27.747461  33.285465  \n",
       "17  19.281204  27.036049  30.534048  35.522842  \n",
       "18  24.096861  34.109833  36.764412  43.473362  \n",
       "19  24.969471  31.520792  31.285065  36.117672  \n",
       "20  27.283840  34.561665  34.875008  37.050900  \n",
       "21  30.603060  37.294102  37.275146  39.570808  \n",
       "22  32.784695  42.524303  41.881824  46.127567  \n",
       "23  28.663486  37.694725  37.923538  43.073772  \n",
       "24  27.140644  35.836781  36.353233  40.748795  \n",
       "25  30.292667  39.333092  39.423084  42.326111  \n",
       "26  26.501652  34.652939  35.096764  37.877037  \n",
       "27  24.113199  33.200935  33.946140  42.417927  \n",
       "28  21.611567  27.556894  28.349117  30.159492  \n",
       "29  22.467485  28.873140  29.784460  31.264435  \n",
       "30  19.777199  25.084467  26.006908  26.629461  \n",
       "31  15.694623  19.287167  20.345507  21.038740  \n",
       "32  10.207670  11.319527  11.952787  12.671544  \n",
       "33   7.700372  10.453104  13.157294  17.465687  \n",
       "34   0.047273  -0.074295   0.027296   0.197710  \n",
       "35   0.046583  -0.072800   0.028004   0.195592  \n",
       "36   0.044214  -0.077383   0.013301   0.144659  \n",
       "37   0.043812  -0.075743   0.014221   0.144194  \n",
       "38   0.039661  -0.072365   0.014074   0.119468  \n",
       "39   0.039408  -0.070370   0.015380   0.119884  \n",
       "40   0.033065  -0.057821   0.025182   0.113186  \n",
       "41   0.033642  -0.055496   0.026256   0.114139  \n",
       "42   0.024892  -0.034070   0.042322   0.118273  \n",
       "43   0.025764  -0.030698   0.044806   0.120863  \n",
       "44   0.015626  -0.000029   0.063778   0.129839  \n",
       "45   0.016413   0.000601   0.064027   0.130222  \n",
       "46   0.004975   0.033582   0.076415   0.131625  \n",
       "47   0.004975   0.033582   0.076415   0.131625  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0.loc[res_0[res_0['L00.1'] == 0].index, ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9'\n",
    "                                            ,'C00.1','C00.2','C00.3','C00.4','C00.5','C00.6','C00.7','C00.8','C00.9'\n",
    "                                            ,'G00.1','G00.2','G00.3','G00.4','G00.5','G00.6','G00.7','G00.8','G00.9'\n",
    "                                            ,'M00.1','M00.2','M00.3','M00.4','M00.5','M00.6','M00.7','M00.8','M00.9'\n",
    "                                            ]] = 0\n",
    "res_1.loc[res_1[res_1['L10.1'] == 0].index, ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9'\n",
    "                                            ,'C10.1','C10.2','C10.3','C10.4','C10.5','C10.6','C10.7','C10.8','C10.9'\n",
    "                                            ,'G10.1','G10.2','G10.3','G10.4','G10.5','G10.6','G10.7','G10.8','G10.9'\n",
    "                                            ,'M10.1','M10.2','M10.3','M10.4','M10.5','M10.6','M10.7','M10.8','M10.9'\n",
    "                                            ]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L00.1</th>\n",
       "      <th>L00.2</th>\n",
       "      <th>L00.3</th>\n",
       "      <th>L00.4</th>\n",
       "      <th>L00.5</th>\n",
       "      <th>L00.6</th>\n",
       "      <th>L00.7</th>\n",
       "      <th>L00.8</th>\n",
       "      <th>L00.9</th>\n",
       "      <th>D00.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G00.9</th>\n",
       "      <th>M00.1</th>\n",
       "      <th>M00.2</th>\n",
       "      <th>M00.3</th>\n",
       "      <th>M00.4</th>\n",
       "      <th>M00.5</th>\n",
       "      <th>M00.6</th>\n",
       "      <th>M00.7</th>\n",
       "      <th>M00.8</th>\n",
       "      <th>M00.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "      <td>0.438392</td>\n",
       "      <td>...</td>\n",
       "      <td>10.337276</td>\n",
       "      <td>0.278376</td>\n",
       "      <td>0.533850</td>\n",
       "      <td>0.851767</td>\n",
       "      <td>1.135873</td>\n",
       "      <td>1.901088</td>\n",
       "      <td>3.958309</td>\n",
       "      <td>5.576889</td>\n",
       "      <td>8.032295</td>\n",
       "      <td>11.671991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "      <td>0.898831</td>\n",
       "      <td>...</td>\n",
       "      <td>29.208799</td>\n",
       "      <td>1.382670</td>\n",
       "      <td>2.785345</td>\n",
       "      <td>4.602665</td>\n",
       "      <td>6.151273</td>\n",
       "      <td>9.155155</td>\n",
       "      <td>12.433868</td>\n",
       "      <td>14.405657</td>\n",
       "      <td>20.825100</td>\n",
       "      <td>29.005898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "      <td>2.243876</td>\n",
       "      <td>...</td>\n",
       "      <td>32.273144</td>\n",
       "      <td>2.625676</td>\n",
       "      <td>5.551159</td>\n",
       "      <td>9.029684</td>\n",
       "      <td>11.174472</td>\n",
       "      <td>14.575258</td>\n",
       "      <td>18.475082</td>\n",
       "      <td>19.700142</td>\n",
       "      <td>27.711407</td>\n",
       "      <td>36.345173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "      <td>7.654971</td>\n",
       "      <td>...</td>\n",
       "      <td>44.152420</td>\n",
       "      <td>3.949140</td>\n",
       "      <td>8.700056</td>\n",
       "      <td>13.569352</td>\n",
       "      <td>15.761546</td>\n",
       "      <td>19.214750</td>\n",
       "      <td>24.219654</td>\n",
       "      <td>24.601713</td>\n",
       "      <td>33.837654</td>\n",
       "      <td>42.674992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "      <td>8.701793</td>\n",
       "      <td>...</td>\n",
       "      <td>43.151070</td>\n",
       "      <td>5.421922</td>\n",
       "      <td>12.015749</td>\n",
       "      <td>17.605280</td>\n",
       "      <td>19.513790</td>\n",
       "      <td>22.503637</td>\n",
       "      <td>27.247337</td>\n",
       "      <td>25.901115</td>\n",
       "      <td>34.847942</td>\n",
       "      <td>41.016430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "      <td>8.442618</td>\n",
       "      <td>...</td>\n",
       "      <td>39.635620</td>\n",
       "      <td>8.717727</td>\n",
       "      <td>18.720308</td>\n",
       "      <td>24.695587</td>\n",
       "      <td>24.716814</td>\n",
       "      <td>26.385412</td>\n",
       "      <td>31.913935</td>\n",
       "      <td>29.011997</td>\n",
       "      <td>34.564465</td>\n",
       "      <td>34.320408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "      <td>8.227545</td>\n",
       "      <td>...</td>\n",
       "      <td>38.851074</td>\n",
       "      <td>9.944949</td>\n",
       "      <td>20.628939</td>\n",
       "      <td>26.114195</td>\n",
       "      <td>25.301535</td>\n",
       "      <td>27.607227</td>\n",
       "      <td>34.258957</td>\n",
       "      <td>32.246815</td>\n",
       "      <td>36.913647</td>\n",
       "      <td>36.626324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "      <td>10.879261</td>\n",
       "      <td>...</td>\n",
       "      <td>54.107361</td>\n",
       "      <td>9.114343</td>\n",
       "      <td>20.005499</td>\n",
       "      <td>27.539772</td>\n",
       "      <td>28.713305</td>\n",
       "      <td>30.506847</td>\n",
       "      <td>37.443073</td>\n",
       "      <td>34.428154</td>\n",
       "      <td>42.832264</td>\n",
       "      <td>45.281658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "      <td>11.177592</td>\n",
       "      <td>...</td>\n",
       "      <td>52.995789</td>\n",
       "      <td>7.772636</td>\n",
       "      <td>17.395922</td>\n",
       "      <td>24.138544</td>\n",
       "      <td>25.423502</td>\n",
       "      <td>27.575884</td>\n",
       "      <td>33.603493</td>\n",
       "      <td>30.786173</td>\n",
       "      <td>39.545959</td>\n",
       "      <td>43.069870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "      <td>10.822333</td>\n",
       "      <td>...</td>\n",
       "      <td>49.077896</td>\n",
       "      <td>7.626348</td>\n",
       "      <td>17.201427</td>\n",
       "      <td>23.625643</td>\n",
       "      <td>24.337145</td>\n",
       "      <td>26.427290</td>\n",
       "      <td>32.402966</td>\n",
       "      <td>29.736628</td>\n",
       "      <td>37.601875</td>\n",
       "      <td>40.233265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "      <td>9.649780</td>\n",
       "      <td>...</td>\n",
       "      <td>46.703388</td>\n",
       "      <td>8.852440</td>\n",
       "      <td>19.440535</td>\n",
       "      <td>26.092402</td>\n",
       "      <td>26.135069</td>\n",
       "      <td>28.116590</td>\n",
       "      <td>34.983261</td>\n",
       "      <td>32.498928</td>\n",
       "      <td>39.111118</td>\n",
       "      <td>40.255474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "      <td>9.131831</td>\n",
       "      <td>...</td>\n",
       "      <td>43.204292</td>\n",
       "      <td>7.633096</td>\n",
       "      <td>17.258287</td>\n",
       "      <td>23.392521</td>\n",
       "      <td>23.490526</td>\n",
       "      <td>25.597803</td>\n",
       "      <td>31.786926</td>\n",
       "      <td>29.368408</td>\n",
       "      <td>36.334438</td>\n",
       "      <td>38.169403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "      <td>10.216905</td>\n",
       "      <td>...</td>\n",
       "      <td>53.884148</td>\n",
       "      <td>5.426360</td>\n",
       "      <td>12.573776</td>\n",
       "      <td>18.299803</td>\n",
       "      <td>19.517847</td>\n",
       "      <td>22.470036</td>\n",
       "      <td>28.079905</td>\n",
       "      <td>26.910032</td>\n",
       "      <td>35.878826</td>\n",
       "      <td>42.546635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "      <td>7.601222</td>\n",
       "      <td>...</td>\n",
       "      <td>37.988602</td>\n",
       "      <td>6.129554</td>\n",
       "      <td>14.129081</td>\n",
       "      <td>19.424168</td>\n",
       "      <td>19.508886</td>\n",
       "      <td>21.832975</td>\n",
       "      <td>26.938946</td>\n",
       "      <td>24.951799</td>\n",
       "      <td>31.463928</td>\n",
       "      <td>33.699268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "      <td>7.567501</td>\n",
       "      <td>...</td>\n",
       "      <td>36.058998</td>\n",
       "      <td>6.561932</td>\n",
       "      <td>14.943178</td>\n",
       "      <td>20.139486</td>\n",
       "      <td>19.879492</td>\n",
       "      <td>22.147516</td>\n",
       "      <td>27.619825</td>\n",
       "      <td>25.590836</td>\n",
       "      <td>31.595079</td>\n",
       "      <td>33.061008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "      <td>5.413460</td>\n",
       "      <td>...</td>\n",
       "      <td>28.707012</td>\n",
       "      <td>6.047947</td>\n",
       "      <td>13.872070</td>\n",
       "      <td>18.218056</td>\n",
       "      <td>17.434427</td>\n",
       "      <td>19.661671</td>\n",
       "      <td>24.608288</td>\n",
       "      <td>22.910669</td>\n",
       "      <td>27.609596</td>\n",
       "      <td>28.168516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "      <td>4.938184</td>\n",
       "      <td>...</td>\n",
       "      <td>30.306799</td>\n",
       "      <td>4.250319</td>\n",
       "      <td>9.801942</td>\n",
       "      <td>13.945416</td>\n",
       "      <td>14.109744</td>\n",
       "      <td>16.555128</td>\n",
       "      <td>20.302433</td>\n",
       "      <td>19.118517</td>\n",
       "      <td>24.372580</td>\n",
       "      <td>26.528730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "      <td>2.910076</td>\n",
       "      <td>...</td>\n",
       "      <td>21.081171</td>\n",
       "      <td>2.166086</td>\n",
       "      <td>4.919737</td>\n",
       "      <td>7.904614</td>\n",
       "      <td>8.732598</td>\n",
       "      <td>11.176006</td>\n",
       "      <td>13.758587</td>\n",
       "      <td>13.877783</td>\n",
       "      <td>18.237036</td>\n",
       "      <td>21.240238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "      <td>1.529654</td>\n",
       "      <td>...</td>\n",
       "      <td>20.914684</td>\n",
       "      <td>0.447172</td>\n",
       "      <td>0.955420</td>\n",
       "      <td>1.710934</td>\n",
       "      <td>2.428085</td>\n",
       "      <td>4.241768</td>\n",
       "      <td>7.769482</td>\n",
       "      <td>9.957658</td>\n",
       "      <td>13.421550</td>\n",
       "      <td>18.445862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L00.1  L00.2  L00.3  L00.4  L00.5  L00.6  L00.7  L00.8  L00.9      D00.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45   0.438392   \n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51   0.898831   \n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11   2.243876   \n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34   7.654971   \n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51   8.701793   \n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17   8.442618   \n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03   8.227545   \n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17  10.879261   \n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82  11.177592   \n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14  10.822333   \n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61   9.649780   \n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92   9.131831   \n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20  10.216905   \n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73   7.601222   \n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94   7.567501   \n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20   5.413460   \n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59   4.938184   \n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49   2.910076   \n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13   1.529654   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "\n",
       "    ...      G00.9     M00.1      M00.2      M00.3      M00.4      M00.5  \\\n",
       "0   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15  ...  10.337276  0.278376   0.533850   0.851767   1.135873   1.901088   \n",
       "16  ...  29.208799  1.382670   2.785345   4.602665   6.151273   9.155155   \n",
       "17  ...  32.273144  2.625676   5.551159   9.029684  11.174472  14.575258   \n",
       "18  ...  44.152420  3.949140   8.700056  13.569352  15.761546  19.214750   \n",
       "19  ...  43.151070  5.421922  12.015749  17.605280  19.513790  22.503637   \n",
       "20  ...  39.635620  8.717727  18.720308  24.695587  24.716814  26.385412   \n",
       "21  ...  38.851074  9.944949  20.628939  26.114195  25.301535  27.607227   \n",
       "22  ...  54.107361  9.114343  20.005499  27.539772  28.713305  30.506847   \n",
       "23  ...  52.995789  7.772636  17.395922  24.138544  25.423502  27.575884   \n",
       "24  ...  49.077896  7.626348  17.201427  23.625643  24.337145  26.427290   \n",
       "25  ...  46.703388  8.852440  19.440535  26.092402  26.135069  28.116590   \n",
       "26  ...  43.204292  7.633096  17.258287  23.392521  23.490526  25.597803   \n",
       "27  ...  53.884148  5.426360  12.573776  18.299803  19.517847  22.470036   \n",
       "28  ...  37.988602  6.129554  14.129081  19.424168  19.508886  21.832975   \n",
       "29  ...  36.058998  6.561932  14.943178  20.139486  19.879492  22.147516   \n",
       "30  ...  28.707012  6.047947  13.872070  18.218056  17.434427  19.661671   \n",
       "31  ...  30.306799  4.250319   9.801942  13.945416  14.109744  16.555128   \n",
       "32  ...  21.081171  2.166086   4.919737   7.904614   8.732598  11.176006   \n",
       "33  ...  20.914684  0.447172   0.955420   1.710934   2.428085   4.241768   \n",
       "34  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        M00.6      M00.7      M00.8      M00.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   3.958309   5.576889   8.032295  11.671991  \n",
       "16  12.433868  14.405657  20.825100  29.005898  \n",
       "17  18.475082  19.700142  27.711407  36.345173  \n",
       "18  24.219654  24.601713  33.837654  42.674992  \n",
       "19  27.247337  25.901115  34.847942  41.016430  \n",
       "20  31.913935  29.011997  34.564465  34.320408  \n",
       "21  34.258957  32.246815  36.913647  36.626324  \n",
       "22  37.443073  34.428154  42.832264  45.281658  \n",
       "23  33.603493  30.786173  39.545959  43.069870  \n",
       "24  32.402966  29.736628  37.601875  40.233265  \n",
       "25  34.983261  32.498928  39.111118  40.255474  \n",
       "26  31.786926  29.368408  36.334438  38.169403  \n",
       "27  28.079905  26.910032  35.878826  42.546635  \n",
       "28  26.938946  24.951799  31.463928  33.699268  \n",
       "29  27.619825  25.590836  31.595079  33.061008  \n",
       "30  24.608288  22.910669  27.609596  28.168516  \n",
       "31  20.302433  19.118517  24.372580  26.528730  \n",
       "32  13.758587  13.877783  18.237036  21.240238  \n",
       "33   7.769482   9.957658  13.421550  18.445862  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_0[:48]#.to_csv('0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L10.1</th>\n",
       "      <th>L10.2</th>\n",
       "      <th>L10.3</th>\n",
       "      <th>L10.4</th>\n",
       "      <th>L10.5</th>\n",
       "      <th>L10.6</th>\n",
       "      <th>L10.7</th>\n",
       "      <th>L10.8</th>\n",
       "      <th>L10.9</th>\n",
       "      <th>D10.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G10.9</th>\n",
       "      <th>M10.1</th>\n",
       "      <th>M10.2</th>\n",
       "      <th>M10.3</th>\n",
       "      <th>M10.4</th>\n",
       "      <th>M10.5</th>\n",
       "      <th>M10.6</th>\n",
       "      <th>M10.7</th>\n",
       "      <th>M10.8</th>\n",
       "      <th>M10.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.87</td>\n",
       "      <td>-0.106694</td>\n",
       "      <td>...</td>\n",
       "      <td>8.146265</td>\n",
       "      <td>0.224555</td>\n",
       "      <td>0.493217</td>\n",
       "      <td>0.769333</td>\n",
       "      <td>1.291518</td>\n",
       "      <td>2.884746</td>\n",
       "      <td>4.748418</td>\n",
       "      <td>8.300235</td>\n",
       "      <td>13.855488</td>\n",
       "      <td>19.790730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.92</td>\n",
       "      <td>0.164094</td>\n",
       "      <td>...</td>\n",
       "      <td>23.949827</td>\n",
       "      <td>1.064352</td>\n",
       "      <td>3.094429</td>\n",
       "      <td>5.176425</td>\n",
       "      <td>8.452856</td>\n",
       "      <td>11.382863</td>\n",
       "      <td>13.115429</td>\n",
       "      <td>20.796421</td>\n",
       "      <td>27.747461</td>\n",
       "      <td>33.285465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>18.24</td>\n",
       "      <td>25.56</td>\n",
       "      <td>1.581352</td>\n",
       "      <td>...</td>\n",
       "      <td>29.552259</td>\n",
       "      <td>2.085540</td>\n",
       "      <td>6.851115</td>\n",
       "      <td>11.054815</td>\n",
       "      <td>16.175102</td>\n",
       "      <td>18.505974</td>\n",
       "      <td>19.281204</td>\n",
       "      <td>27.036049</td>\n",
       "      <td>30.534048</td>\n",
       "      <td>35.522842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.74</td>\n",
       "      <td>14.78</td>\n",
       "      <td>19.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>22.60</td>\n",
       "      <td>20.57</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.66</td>\n",
       "      <td>32.96</td>\n",
       "      <td>7.584140</td>\n",
       "      <td>...</td>\n",
       "      <td>40.693188</td>\n",
       "      <td>3.243448</td>\n",
       "      <td>10.825759</td>\n",
       "      <td>16.592876</td>\n",
       "      <td>22.638235</td>\n",
       "      <td>24.353527</td>\n",
       "      <td>24.096861</td>\n",
       "      <td>34.109833</td>\n",
       "      <td>36.764412</td>\n",
       "      <td>43.473362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.47</td>\n",
       "      <td>19.06</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.62</td>\n",
       "      <td>20.20</td>\n",
       "      <td>17.15</td>\n",
       "      <td>22.85</td>\n",
       "      <td>33.97</td>\n",
       "      <td>8.712058</td>\n",
       "      <td>...</td>\n",
       "      <td>38.026020</td>\n",
       "      <td>4.668878</td>\n",
       "      <td>14.533233</td>\n",
       "      <td>20.736805</td>\n",
       "      <td>26.693420</td>\n",
       "      <td>26.814178</td>\n",
       "      <td>24.969471</td>\n",
       "      <td>31.520792</td>\n",
       "      <td>31.285065</td>\n",
       "      <td>36.117672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.33</td>\n",
       "      <td>24.78</td>\n",
       "      <td>24.52</td>\n",
       "      <td>23.02</td>\n",
       "      <td>28.58</td>\n",
       "      <td>28.55</td>\n",
       "      <td>25.06</td>\n",
       "      <td>26.39</td>\n",
       "      <td>27.24</td>\n",
       "      <td>8.296864</td>\n",
       "      <td>...</td>\n",
       "      <td>33.450459</td>\n",
       "      <td>8.403633</td>\n",
       "      <td>21.809557</td>\n",
       "      <td>25.825407</td>\n",
       "      <td>29.473806</td>\n",
       "      <td>27.048195</td>\n",
       "      <td>27.283840</td>\n",
       "      <td>34.561665</td>\n",
       "      <td>34.875008</td>\n",
       "      <td>37.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.26</td>\n",
       "      <td>26.20</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.70</td>\n",
       "      <td>29.22</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.05</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.45</td>\n",
       "      <td>8.116883</td>\n",
       "      <td>...</td>\n",
       "      <td>36.857723</td>\n",
       "      <td>10.052383</td>\n",
       "      <td>23.739573</td>\n",
       "      <td>25.210020</td>\n",
       "      <td>27.425840</td>\n",
       "      <td>28.081005</td>\n",
       "      <td>30.603060</td>\n",
       "      <td>37.294102</td>\n",
       "      <td>37.275146</td>\n",
       "      <td>39.570808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.96</td>\n",
       "      <td>29.89</td>\n",
       "      <td>33.58</td>\n",
       "      <td>33.28</td>\n",
       "      <td>38.13</td>\n",
       "      <td>35.95</td>\n",
       "      <td>34.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.78</td>\n",
       "      <td>10.945988</td>\n",
       "      <td>...</td>\n",
       "      <td>43.487072</td>\n",
       "      <td>8.884293</td>\n",
       "      <td>23.614380</td>\n",
       "      <td>28.908573</td>\n",
       "      <td>35.237713</td>\n",
       "      <td>33.645550</td>\n",
       "      <td>32.784695</td>\n",
       "      <td>42.524303</td>\n",
       "      <td>41.881824</td>\n",
       "      <td>46.127567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.24</td>\n",
       "      <td>24.45</td>\n",
       "      <td>29.77</td>\n",
       "      <td>29.36</td>\n",
       "      <td>34.56</td>\n",
       "      <td>30.81</td>\n",
       "      <td>26.23</td>\n",
       "      <td>27.65</td>\n",
       "      <td>34.53</td>\n",
       "      <td>11.328506</td>\n",
       "      <td>...</td>\n",
       "      <td>40.628624</td>\n",
       "      <td>7.170529</td>\n",
       "      <td>20.149168</td>\n",
       "      <td>26.042404</td>\n",
       "      <td>32.277306</td>\n",
       "      <td>30.443672</td>\n",
       "      <td>28.663486</td>\n",
       "      <td>37.694725</td>\n",
       "      <td>37.923538</td>\n",
       "      <td>43.073772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.30</td>\n",
       "      <td>25.22</td>\n",
       "      <td>31.45</td>\n",
       "      <td>29.95</td>\n",
       "      <td>34.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>29.10</td>\n",
       "      <td>27.16</td>\n",
       "      <td>31.12</td>\n",
       "      <td>10.933179</td>\n",
       "      <td>...</td>\n",
       "      <td>37.329742</td>\n",
       "      <td>7.006989</td>\n",
       "      <td>19.508602</td>\n",
       "      <td>24.621035</td>\n",
       "      <td>30.483465</td>\n",
       "      <td>28.035336</td>\n",
       "      <td>27.140644</td>\n",
       "      <td>35.836781</td>\n",
       "      <td>36.353233</td>\n",
       "      <td>40.748795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.03</td>\n",
       "      <td>30.02</td>\n",
       "      <td>34.72</td>\n",
       "      <td>32.49</td>\n",
       "      <td>35.12</td>\n",
       "      <td>35.82</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "      <td>9.595832</td>\n",
       "      <td>...</td>\n",
       "      <td>38.761215</td>\n",
       "      <td>8.630649</td>\n",
       "      <td>22.244896</td>\n",
       "      <td>25.697399</td>\n",
       "      <td>30.706722</td>\n",
       "      <td>29.184013</td>\n",
       "      <td>30.292667</td>\n",
       "      <td>39.333092</td>\n",
       "      <td>39.423084</td>\n",
       "      <td>42.326111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.21</td>\n",
       "      <td>27.84</td>\n",
       "      <td>33.51</td>\n",
       "      <td>32.70</td>\n",
       "      <td>37.10</td>\n",
       "      <td>34.75</td>\n",
       "      <td>29.17</td>\n",
       "      <td>27.61</td>\n",
       "      <td>29.70</td>\n",
       "      <td>9.052801</td>\n",
       "      <td>...</td>\n",
       "      <td>36.849087</td>\n",
       "      <td>7.066695</td>\n",
       "      <td>19.182344</td>\n",
       "      <td>23.245707</td>\n",
       "      <td>28.614256</td>\n",
       "      <td>26.160431</td>\n",
       "      <td>26.501652</td>\n",
       "      <td>34.652939</td>\n",
       "      <td>35.096764</td>\n",
       "      <td>37.877037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.50</td>\n",
       "      <td>22.99</td>\n",
       "      <td>30.11</td>\n",
       "      <td>31.82</td>\n",
       "      <td>32.80</td>\n",
       "      <td>33.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>36.02</td>\n",
       "      <td>44.45</td>\n",
       "      <td>10.307910</td>\n",
       "      <td>...</td>\n",
       "      <td>43.257210</td>\n",
       "      <td>4.621830</td>\n",
       "      <td>14.512002</td>\n",
       "      <td>20.495499</td>\n",
       "      <td>27.096788</td>\n",
       "      <td>25.522949</td>\n",
       "      <td>24.113199</td>\n",
       "      <td>33.200935</td>\n",
       "      <td>33.946140</td>\n",
       "      <td>42.417927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.97</td>\n",
       "      <td>21.03</td>\n",
       "      <td>25.88</td>\n",
       "      <td>24.37</td>\n",
       "      <td>28.26</td>\n",
       "      <td>26.10</td>\n",
       "      <td>21.49</td>\n",
       "      <td>20.61</td>\n",
       "      <td>26.21</td>\n",
       "      <td>7.386799</td>\n",
       "      <td>...</td>\n",
       "      <td>33.641750</td>\n",
       "      <td>5.364833</td>\n",
       "      <td>15.456990</td>\n",
       "      <td>19.887440</td>\n",
       "      <td>25.163893</td>\n",
       "      <td>21.756081</td>\n",
       "      <td>21.611567</td>\n",
       "      <td>27.556894</td>\n",
       "      <td>28.349117</td>\n",
       "      <td>30.159492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>22.61</td>\n",
       "      <td>27.12</td>\n",
       "      <td>27.53</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.98</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.92</td>\n",
       "      <td>25.82</td>\n",
       "      <td>7.341166</td>\n",
       "      <td>...</td>\n",
       "      <td>33.069721</td>\n",
       "      <td>5.919316</td>\n",
       "      <td>16.356092</td>\n",
       "      <td>20.091255</td>\n",
       "      <td>24.842615</td>\n",
       "      <td>21.822651</td>\n",
       "      <td>22.467485</td>\n",
       "      <td>28.873140</td>\n",
       "      <td>29.784460</td>\n",
       "      <td>31.264435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.57</td>\n",
       "      <td>17.69</td>\n",
       "      <td>21.43</td>\n",
       "      <td>22.13</td>\n",
       "      <td>22.79</td>\n",
       "      <td>24.23</td>\n",
       "      <td>23.32</td>\n",
       "      <td>21.47</td>\n",
       "      <td>23.67</td>\n",
       "      <td>4.992326</td>\n",
       "      <td>...</td>\n",
       "      <td>29.133614</td>\n",
       "      <td>5.305933</td>\n",
       "      <td>14.342173</td>\n",
       "      <td>16.965315</td>\n",
       "      <td>20.581347</td>\n",
       "      <td>18.054493</td>\n",
       "      <td>19.777199</td>\n",
       "      <td>25.084467</td>\n",
       "      <td>26.006908</td>\n",
       "      <td>26.629461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.19</td>\n",
       "      <td>15.84</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.86</td>\n",
       "      <td>15.93</td>\n",
       "      <td>16.22</td>\n",
       "      <td>15.55</td>\n",
       "      <td>15.47</td>\n",
       "      <td>24.20</td>\n",
       "      <td>4.490984</td>\n",
       "      <td>...</td>\n",
       "      <td>27.370882</td>\n",
       "      <td>3.503308</td>\n",
       "      <td>10.973215</td>\n",
       "      <td>15.272486</td>\n",
       "      <td>20.004910</td>\n",
       "      <td>15.951016</td>\n",
       "      <td>15.694623</td>\n",
       "      <td>19.287167</td>\n",
       "      <td>20.345507</td>\n",
       "      <td>21.038740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.95</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>15.83</td>\n",
       "      <td>2.306039</td>\n",
       "      <td>...</td>\n",
       "      <td>18.221132</td>\n",
       "      <td>1.624987</td>\n",
       "      <td>5.569398</td>\n",
       "      <td>9.298272</td>\n",
       "      <td>13.697021</td>\n",
       "      <td>10.722804</td>\n",
       "      <td>10.207670</td>\n",
       "      <td>11.319527</td>\n",
       "      <td>11.952787</td>\n",
       "      <td>12.671544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>13.80</td>\n",
       "      <td>0.968652</td>\n",
       "      <td>...</td>\n",
       "      <td>16.878769</td>\n",
       "      <td>0.319868</td>\n",
       "      <td>0.871121</td>\n",
       "      <td>1.711004</td>\n",
       "      <td>3.198543</td>\n",
       "      <td>5.575622</td>\n",
       "      <td>7.700372</td>\n",
       "      <td>10.453104</td>\n",
       "      <td>13.157294</td>\n",
       "      <td>17.465687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L10.1  L10.2  L10.3  L10.4  L10.5  L10.6  L10.7  L10.8  L10.9      D10.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "15   0.48   0.70   1.80   2.18   0.98   3.56   2.99   6.80   9.87  -0.106694   \n",
       "16   2.67   7.45   8.62  10.08  10.06  11.52  17.00  19.79  20.92   0.164094   \n",
       "17   3.80   8.55   8.15  10.49  10.84  12.03  15.05  18.24  25.56   1.581352   \n",
       "18   7.74  14.78  19.70  18.93  22.60  20.57  16.19  15.66  32.96   7.584140   \n",
       "19   9.47  19.06  21.52  21.87  23.62  20.20  17.15  22.85  33.97   8.712058   \n",
       "20  14.33  24.78  24.52  23.02  28.58  28.55  25.06  26.39  27.24   8.296864   \n",
       "21  15.26  26.20  24.57  24.70  29.22  31.11  31.05  31.59  32.45   8.116883   \n",
       "22  15.96  29.89  33.58  33.28  38.13  35.95  34.24  33.24  33.78  10.945988   \n",
       "23  14.24  24.45  29.77  29.36  34.56  30.81  26.23  27.65  34.53  11.328506   \n",
       "24  14.30  25.22  31.45  29.95  34.70  31.31  29.10  27.16  31.12  10.933179   \n",
       "25  16.03  30.02  34.72  32.49  35.12  35.82  33.60  32.73  34.11   9.595832   \n",
       "26  15.21  27.84  33.51  32.70  37.10  34.75  29.17  27.61  29.70   9.052801   \n",
       "27   9.50  22.99  30.11  31.82  32.80  33.88  19.43  36.02  44.45  10.307910   \n",
       "28  10.97  21.03  25.88  24.37  28.26  26.10  21.49  20.61  26.21   7.386799   \n",
       "29  12.63  22.61  27.12  27.53  31.02  29.98  27.37  22.92  25.82   7.341166   \n",
       "30   8.57  17.69  21.43  22.13  22.79  24.23  23.32  21.47  23.67   4.992326   \n",
       "31   7.19  15.84  18.83  17.86  15.93  16.22  15.55  15.47  24.20   4.490984   \n",
       "32   2.95   8.16   8.37   7.24   6.39   6.81   6.33   6.51  15.83   2.306039   \n",
       "33   1.71   4.94   4.62   3.24   3.42   1.19   3.41  -0.50  13.80   0.968652   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17   0.000000   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17   0.000000   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "\n",
       "    ...      G10.9      M10.1      M10.2      M10.3      M10.4      M10.5  \\\n",
       "0   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9   ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15  ...   8.146265   0.224555   0.493217   0.769333   1.291518   2.884746   \n",
       "16  ...  23.949827   1.064352   3.094429   5.176425   8.452856  11.382863   \n",
       "17  ...  29.552259   2.085540   6.851115  11.054815  16.175102  18.505974   \n",
       "18  ...  40.693188   3.243448  10.825759  16.592876  22.638235  24.353527   \n",
       "19  ...  38.026020   4.668878  14.533233  20.736805  26.693420  26.814178   \n",
       "20  ...  33.450459   8.403633  21.809557  25.825407  29.473806  27.048195   \n",
       "21  ...  36.857723  10.052383  23.739573  25.210020  27.425840  28.081005   \n",
       "22  ...  43.487072   8.884293  23.614380  28.908573  35.237713  33.645550   \n",
       "23  ...  40.628624   7.170529  20.149168  26.042404  32.277306  30.443672   \n",
       "24  ...  37.329742   7.006989  19.508602  24.621035  30.483465  28.035336   \n",
       "25  ...  38.761215   8.630649  22.244896  25.697399  30.706722  29.184013   \n",
       "26  ...  36.849087   7.066695  19.182344  23.245707  28.614256  26.160431   \n",
       "27  ...  43.257210   4.621830  14.512002  20.495499  27.096788  25.522949   \n",
       "28  ...  33.641750   5.364833  15.456990  19.887440  25.163893  21.756081   \n",
       "29  ...  33.069721   5.919316  16.356092  20.091255  24.842615  21.822651   \n",
       "30  ...  29.133614   5.305933  14.342173  16.965315  20.581347  18.054493   \n",
       "31  ...  27.370882   3.503308  10.973215  15.272486  20.004910  15.951016   \n",
       "32  ...  18.221132   1.624987   5.569398   9.298272  13.697021  10.722804   \n",
       "33  ...  16.878769   0.319868   0.871121   1.711004   3.198543   5.575622   \n",
       "34  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  ...   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        M10.6      M10.7      M10.8      M10.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   4.748418   8.300235  13.855488  19.790730  \n",
       "16  13.115429  20.796421  27.747461  33.285465  \n",
       "17  19.281204  27.036049  30.534048  35.522842  \n",
       "18  24.096861  34.109833  36.764412  43.473362  \n",
       "19  24.969471  31.520792  31.285065  36.117672  \n",
       "20  27.283840  34.561665  34.875008  37.050900  \n",
       "21  30.603060  37.294102  37.275146  39.570808  \n",
       "22  32.784695  42.524303  41.881824  46.127567  \n",
       "23  28.663486  37.694725  37.923538  43.073772  \n",
       "24  27.140644  35.836781  36.353233  40.748795  \n",
       "25  30.292667  39.333092  39.423084  42.326111  \n",
       "26  26.501652  34.652939  35.096764  37.877037  \n",
       "27  24.113199  33.200935  33.946140  42.417927  \n",
       "28  21.611567  27.556894  28.349117  30.159492  \n",
       "29  22.467485  28.873140  29.784460  31.264435  \n",
       "30  19.777199  25.084467  26.006908  26.629461  \n",
       "31  15.694623  19.287167  20.345507  21.038740  \n",
       "32  10.207670  11.319527  11.952787  12.671544  \n",
       "33   7.700372  10.453104  13.157294  17.465687  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1[:48]#.to_csv('0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    res_0[\"L00.\"+str(i)] = (res_0[\"L00.\"+str(i)] + res_0[\"D00.\"+str(i)] + res_0[\"C00.\"+str(i)] + res_0[\"G00.\"+str(i)] + res_0[\"M00.\"+str(i)])/5\n",
    "    res_1[\"L10.\"+str(i)] = (res_1[\"L10.\"+str(i)] + res_1[\"D10.\"+str(i)] + res_1[\"C10.\"+str(i)] + res_1[\"G10.\"+str(i)] + res_1[\"M10.\"+str(i)])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.csv_Day7_2h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.csv_Day7_3h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.csv_Day7_3h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.csv_Day7_4h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.csv_Day7_4h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.csv_Day7_5h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.csv_Day7_5h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.csv_Day7_6h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.csv_Day7_6h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.csv_Day7_7h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.csv_Day7_7h30m</td>\n",
       "      <td>0.426221</td>\n",
       "      <td>1.127112</td>\n",
       "      <td>1.353650</td>\n",
       "      <td>1.760052</td>\n",
       "      <td>2.798468</td>\n",
       "      <td>3.243232</td>\n",
       "      <td>5.121620</td>\n",
       "      <td>6.840937</td>\n",
       "      <td>10.169849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.csv_Day7_8h00m</td>\n",
       "      <td>1.412598</td>\n",
       "      <td>3.448413</td>\n",
       "      <td>4.031707</td>\n",
       "      <td>5.529490</td>\n",
       "      <td>8.766799</td>\n",
       "      <td>9.535867</td>\n",
       "      <td>13.302792</td>\n",
       "      <td>17.219922</td>\n",
       "      <td>23.979612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.csv_Day7_8h30m</td>\n",
       "      <td>2.392764</td>\n",
       "      <td>5.475439</td>\n",
       "      <td>7.280822</td>\n",
       "      <td>8.650428</td>\n",
       "      <td>12.028300</td>\n",
       "      <td>12.339961</td>\n",
       "      <td>16.164327</td>\n",
       "      <td>20.952932</td>\n",
       "      <td>28.372677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.csv_Day7_9h00m</td>\n",
       "      <td>5.315377</td>\n",
       "      <td>11.287718</td>\n",
       "      <td>14.885520</td>\n",
       "      <td>16.734259</td>\n",
       "      <td>21.663280</td>\n",
       "      <td>21.744131</td>\n",
       "      <td>24.534333</td>\n",
       "      <td>30.745782</td>\n",
       "      <td>38.159073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.csv_Day7_9h30m</td>\n",
       "      <td>7.084581</td>\n",
       "      <td>14.612878</td>\n",
       "      <td>17.850100</td>\n",
       "      <td>19.303315</td>\n",
       "      <td>24.213320</td>\n",
       "      <td>24.102734</td>\n",
       "      <td>26.050150</td>\n",
       "      <td>30.784463</td>\n",
       "      <td>37.361167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.csv_Day7_10h00m</td>\n",
       "      <td>11.189669</td>\n",
       "      <td>21.495192</td>\n",
       "      <td>26.143913</td>\n",
       "      <td>26.463844</td>\n",
       "      <td>31.853098</td>\n",
       "      <td>33.928379</td>\n",
       "      <td>33.145222</td>\n",
       "      <td>35.212770</td>\n",
       "      <td>36.025326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.csv_Day7_10h30m</td>\n",
       "      <td>11.913903</td>\n",
       "      <td>22.672099</td>\n",
       "      <td>27.698052</td>\n",
       "      <td>27.454219</td>\n",
       "      <td>33.337310</td>\n",
       "      <td>35.214494</td>\n",
       "      <td>35.871771</td>\n",
       "      <td>37.176989</td>\n",
       "      <td>36.640791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.csv_Day7_11h00m</td>\n",
       "      <td>13.242655</td>\n",
       "      <td>24.961140</td>\n",
       "      <td>31.361335</td>\n",
       "      <td>33.187043</td>\n",
       "      <td>38.363400</td>\n",
       "      <td>40.683708</td>\n",
       "      <td>41.548713</td>\n",
       "      <td>44.722892</td>\n",
       "      <td>45.934574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.csv_Day7_11h30m</td>\n",
       "      <td>11.350333</td>\n",
       "      <td>21.758560</td>\n",
       "      <td>27.175168</td>\n",
       "      <td>29.779874</td>\n",
       "      <td>34.996662</td>\n",
       "      <td>37.103974</td>\n",
       "      <td>39.567736</td>\n",
       "      <td>40.892962</td>\n",
       "      <td>45.341636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.csv_Day7_12h00m</td>\n",
       "      <td>11.487437</td>\n",
       "      <td>21.876347</td>\n",
       "      <td>27.264342</td>\n",
       "      <td>30.543665</td>\n",
       "      <td>35.559703</td>\n",
       "      <td>36.697036</td>\n",
       "      <td>37.431070</td>\n",
       "      <td>39.188410</td>\n",
       "      <td>42.074119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.csv_Day7_12h30m</td>\n",
       "      <td>12.395511</td>\n",
       "      <td>23.923861</td>\n",
       "      <td>29.778749</td>\n",
       "      <td>32.065279</td>\n",
       "      <td>36.025760</td>\n",
       "      <td>38.789688</td>\n",
       "      <td>39.644492</td>\n",
       "      <td>41.253779</td>\n",
       "      <td>41.750092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.csv_Day7_13h00m</td>\n",
       "      <td>11.052992</td>\n",
       "      <td>20.686880</td>\n",
       "      <td>26.180580</td>\n",
       "      <td>29.909811</td>\n",
       "      <td>35.011124</td>\n",
       "      <td>35.997262</td>\n",
       "      <td>36.421431</td>\n",
       "      <td>37.666756</td>\n",
       "      <td>38.505065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.csv_Day7_13h30m</td>\n",
       "      <td>8.351934</td>\n",
       "      <td>17.909914</td>\n",
       "      <td>22.747501</td>\n",
       "      <td>27.678503</td>\n",
       "      <td>32.399477</td>\n",
       "      <td>33.604752</td>\n",
       "      <td>34.587871</td>\n",
       "      <td>37.299380</td>\n",
       "      <td>44.067701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.csv_Day7_14h00m</td>\n",
       "      <td>8.131455</td>\n",
       "      <td>16.966848</td>\n",
       "      <td>21.290832</td>\n",
       "      <td>24.670542</td>\n",
       "      <td>28.884885</td>\n",
       "      <td>30.191413</td>\n",
       "      <td>29.580989</td>\n",
       "      <td>31.984193</td>\n",
       "      <td>34.141220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.csv_Day7_14h30m</td>\n",
       "      <td>8.581903</td>\n",
       "      <td>17.767285</td>\n",
       "      <td>21.964618</td>\n",
       "      <td>24.739631</td>\n",
       "      <td>28.804332</td>\n",
       "      <td>30.018678</td>\n",
       "      <td>30.475887</td>\n",
       "      <td>32.348644</td>\n",
       "      <td>33.189851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.csv_Day7_15h00m</td>\n",
       "      <td>6.779718</td>\n",
       "      <td>14.753762</td>\n",
       "      <td>18.562784</td>\n",
       "      <td>19.976966</td>\n",
       "      <td>24.198969</td>\n",
       "      <td>24.983778</td>\n",
       "      <td>25.824765</td>\n",
       "      <td>28.066616</td>\n",
       "      <td>28.031769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.csv_Day7_15h30m</td>\n",
       "      <td>4.991319</td>\n",
       "      <td>12.440321</td>\n",
       "      <td>14.841937</td>\n",
       "      <td>16.920089</td>\n",
       "      <td>19.838469</td>\n",
       "      <td>21.191439</td>\n",
       "      <td>21.782409</td>\n",
       "      <td>24.500248</td>\n",
       "      <td>26.769142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.csv_Day7_16h00m</td>\n",
       "      <td>2.863604</td>\n",
       "      <td>6.906722</td>\n",
       "      <td>8.487927</td>\n",
       "      <td>9.755982</td>\n",
       "      <td>11.789374</td>\n",
       "      <td>11.899101</td>\n",
       "      <td>13.112647</td>\n",
       "      <td>15.988545</td>\n",
       "      <td>19.157742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.csv_Day7_16h30m</td>\n",
       "      <td>0.945959</td>\n",
       "      <td>2.596904</td>\n",
       "      <td>2.999257</td>\n",
       "      <td>3.823203</td>\n",
       "      <td>5.276477</td>\n",
       "      <td>6.668670</td>\n",
       "      <td>8.238943</td>\n",
       "      <td>10.097812</td>\n",
       "      <td>18.046997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.csv_Day7_17h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.csv_Day7_17h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.csv_Day7_18h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.csv_Day7_18h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.csv_Day7_19h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.csv_Day7_19h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.csv_Day7_20h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.csv_Day7_20h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.csv_Day7_21h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.csv_Day7_21h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.csv_Day7_22h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.csv_Day7_22h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.csv_Day7_23h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.csv_Day7_23h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id      q_0.1      q_0.2      q_0.3      q_0.4      q_0.5  \\\n",
       "0    0.csv_Day7_0h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1    0.csv_Day7_0h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2    0.csv_Day7_1h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3    0.csv_Day7_1h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4    0.csv_Day7_2h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5    0.csv_Day7_2h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6    0.csv_Day7_3h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7    0.csv_Day7_3h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8    0.csv_Day7_4h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9    0.csv_Day7_4h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10   0.csv_Day7_5h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11   0.csv_Day7_5h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12   0.csv_Day7_6h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13   0.csv_Day7_6h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14   0.csv_Day7_7h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15   0.csv_Day7_7h30m   0.426221   1.127112   1.353650   1.760052   2.798468   \n",
       "16   0.csv_Day7_8h00m   1.412598   3.448413   4.031707   5.529490   8.766799   \n",
       "17   0.csv_Day7_8h30m   2.392764   5.475439   7.280822   8.650428  12.028300   \n",
       "18   0.csv_Day7_9h00m   5.315377  11.287718  14.885520  16.734259  21.663280   \n",
       "19   0.csv_Day7_9h30m   7.084581  14.612878  17.850100  19.303315  24.213320   \n",
       "20  0.csv_Day7_10h00m  11.189669  21.495192  26.143913  26.463844  31.853098   \n",
       "21  0.csv_Day7_10h30m  11.913903  22.672099  27.698052  27.454219  33.337310   \n",
       "22  0.csv_Day7_11h00m  13.242655  24.961140  31.361335  33.187043  38.363400   \n",
       "23  0.csv_Day7_11h30m  11.350333  21.758560  27.175168  29.779874  34.996662   \n",
       "24  0.csv_Day7_12h00m  11.487437  21.876347  27.264342  30.543665  35.559703   \n",
       "25  0.csv_Day7_12h30m  12.395511  23.923861  29.778749  32.065279  36.025760   \n",
       "26  0.csv_Day7_13h00m  11.052992  20.686880  26.180580  29.909811  35.011124   \n",
       "27  0.csv_Day7_13h30m   8.351934  17.909914  22.747501  27.678503  32.399477   \n",
       "28  0.csv_Day7_14h00m   8.131455  16.966848  21.290832  24.670542  28.884885   \n",
       "29  0.csv_Day7_14h30m   8.581903  17.767285  21.964618  24.739631  28.804332   \n",
       "30  0.csv_Day7_15h00m   6.779718  14.753762  18.562784  19.976966  24.198969   \n",
       "31  0.csv_Day7_15h30m   4.991319  12.440321  14.841937  16.920089  19.838469   \n",
       "32  0.csv_Day7_16h00m   2.863604   6.906722   8.487927   9.755982  11.789374   \n",
       "33  0.csv_Day7_16h30m   0.945959   2.596904   2.999257   3.823203   5.276477   \n",
       "34  0.csv_Day7_17h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  0.csv_Day7_17h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  0.csv_Day7_18h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  0.csv_Day7_18h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  0.csv_Day7_19h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  0.csv_Day7_19h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  0.csv_Day7_20h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  0.csv_Day7_20h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  0.csv_Day7_21h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  0.csv_Day7_21h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  0.csv_Day7_22h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  0.csv_Day7_22h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  0.csv_Day7_23h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  0.csv_Day7_23h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        q_0.6      q_0.7      q_0.8      q_0.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   3.243232   5.121620   6.840937  10.169849  \n",
       "16   9.535867  13.302792  17.219922  23.979612  \n",
       "17  12.339961  16.164327  20.952932  28.372677  \n",
       "18  21.744131  24.534333  30.745782  38.159073  \n",
       "19  24.102734  26.050150  30.784463  37.361167  \n",
       "20  33.928379  33.145222  35.212770  36.025326  \n",
       "21  35.214494  35.871771  37.176989  36.640791  \n",
       "22  40.683708  41.548713  44.722892  45.934574  \n",
       "23  37.103974  39.567736  40.892962  45.341636  \n",
       "24  36.697036  37.431070  39.188410  42.074119  \n",
       "25  38.789688  39.644492  41.253779  41.750092  \n",
       "26  35.997262  36.421431  37.666756  38.505065  \n",
       "27  33.604752  34.587871  37.299380  44.067701  \n",
       "28  30.191413  29.580989  31.984193  34.141220  \n",
       "29  30.018678  30.475887  32.348644  33.189851  \n",
       "30  24.983778  25.824765  28.066616  28.031769  \n",
       "31  21.191439  21.782409  24.500248  26.769142  \n",
       "32  11.899101  13.112647  15.988545  19.157742  \n",
       "33   6.668670   8.238943  10.097812  18.046997  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = res_0[['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']].values\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = res_1[['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']].values\n",
    "submission[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission/submission_20210123-3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
