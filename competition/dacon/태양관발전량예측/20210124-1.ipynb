{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['Day','Hour', 'TARGET', 'DHI', 'DNI', 'NET']\n",
    "\n",
    "def preprocess_data(data, is_train=True):\n",
    "    \n",
    "    temp = data.copy()\n",
    "    \n",
    "    temp['NET'] = 37-(37-temp['T'])/(0.68-0.0014*temp.RH+1/(1.76+1.4*temp.WS**0.75))-0.29*temp['T']*(1-0.001*temp.RH)\n",
    "    \n",
    "    temp = temp[col]\n",
    "\n",
    "    if is_train==True:          \n",
    "    \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill')\n",
    "        temp = temp.dropna()\n",
    "        \n",
    "        return temp.iloc[:-96]\n",
    "\n",
    "    elif is_train==False:\n",
    "        \n",
    "        temp = temp[col]\n",
    "                              \n",
    "        return temp.iloc[-48:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Day', 'Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['Day', 'Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']\n",
    "\n",
    "def preprocess_data(data, is_train=True):\n",
    "    \n",
    "    temp = data.copy()\n",
    "    temp = temp[['Day','Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "\n",
    "    if is_train==True:          \n",
    "    \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill')\n",
    "        temp = temp.dropna()\n",
    "        \n",
    "        return temp.iloc[:-96]\n",
    "\n",
    "    elif is_train==False:\n",
    "        \n",
    "        temp = temp[['Day','Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "                              \n",
    "        return temp.iloc[-48:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train/train.csv')\n",
    "\n",
    "test = []\n",
    "\n",
    "for i in range(81):\n",
    "    file_path = './data/test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp['Day'] = i\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    test.append(temp)\n",
    "\n",
    "df_test = pd.concat(test)\n",
    "\n",
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52464, 10), (3888, 8))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = preprocess_data(train)\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.WS = np.log1p(df_train.WS)\n",
    "df_test.WS = np.log1p(df_test.WS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = df_train[col].min()\n",
    "max  = df_train[col].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(col):\n",
    "    df_train[col] = (df_train[col] - min[i]) / (max[i] - min[i])\n",
    "    df_test[col] = (df_test[col] - min[i]) / (max[i] - min[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Day0 = df_train.iloc[:, :-2]\n",
    "Day  = df_train.iloc[:, 1:-2]\n",
    "Day7 = df_train.iloc[:, -2]\n",
    "Day8 = df_train.iloc[:, -1]\n",
    "Day78 = df_train.iloc[:, -2:]\n",
    "\n",
    "df_test0 = df_test.copy()\n",
    "df_test = df_test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(q, y, pred):\n",
    "    err = (y-pred)\n",
    "    return mean(maximum(q*err, (q-1)*err), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_lst = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39348, 7), (13116, 7), (39348, 2), (13116, 2))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(Day, Day78, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train_1, X_valid_1, Y_train_1, Y_valid_1 = train_test_split(Day, Day7, test_size=0.25, random_state=42)\n",
    "X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(Day, Day8, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import mean, maximum\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "611/615 [============================>.] - ETA: 0s - loss: 474.8367WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 473.2456 - val_loss: 167.4784\n",
      "Epoch 2/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 164.7995 - val_loss: 152.2804\n",
      "Epoch 3/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 148.9799 - val_loss: 143.0434\n",
      "Epoch 4/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 146.6482 - val_loss: 141.6473\n",
      "Epoch 5/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 139.9642 - val_loss: 137.6312\n",
      "Epoch 6/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 140.5633 - val_loss: 138.0333\n",
      "Epoch 7/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 138.6093 - val_loss: 135.1270\n",
      "Epoch 8/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 132.6183 - val_loss: 135.1921\n",
      "Epoch 9/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 136.0814 - val_loss: 133.8816\n",
      "Epoch 10/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 136.7880 - val_loss: 132.3870\n",
      "Epoch 11/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 141.1960 - val_loss: 133.5115\n",
      "Epoch 12/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 134.1921 - val_loss: 130.9550\n",
      "Epoch 13/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 133.9211 - val_loss: 133.1596\n",
      "Epoch 14/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 136.0108 - val_loss: 130.7125\n",
      "Epoch 15/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 130.1885 - val_loss: 132.5227\n",
      "Epoch 16/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 136.6459 - val_loss: 131.6945\n",
      "Epoch 17/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 137.4845 - val_loss: 132.5084\n",
      "Epoch 00017: early stopping\n",
      "410/410 [==============================] - 0s 598us/step - loss: 131.2333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131.2332763671875"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(Day.shape)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(2)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "hist = model.fit(X_train, Y_train, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "model.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'], 'b-', label='loss')\n",
    "plt.plot(hist.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "793/820 [============================>.] - ETA: 0s - loss: 1.4452WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4435 - val_loss: 1.5553\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3840 - val_loss: 1.5420\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3711 - val_loss: 1.5563\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3592 - val_loss: 1.5389\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3696 - val_loss: 1.5477\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3578 - val_loss: 1.5690\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3646 - val_loss: 1.5435\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "806/820 [============================>.] - ETA: 0s - loss: 2.2282WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.2280 - val_loss: 2.5484\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2057 - val_loss: 2.5236\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2024 - val_loss: 2.5317\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.1827 - val_loss: 2.5153\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2096 - val_loss: 2.5540\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.1958 - val_loss: 2.5207\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2084 - val_loss: 2.5193\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "787/820 [===========================>..] - ETA: 0s - loss: 2.6475WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6470 - val_loss: 3.0059\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6130 - val_loss: 3.0252\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6242 - val_loss: 3.0199\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6091 - val_loss: 2.9914\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6348 - val_loss: 3.0555\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6203 - val_loss: 2.9855\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6308 - val_loss: 3.0076\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6519 - val_loss: 3.0271\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6243 - val_loss: 2.9905\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 2.7551WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7551 - val_loss: 3.1292\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7190 - val_loss: 3.1233\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7298 - val_loss: 3.1547\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7207 - val_loss: 3.1253\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7374 - val_loss: 3.1528\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "776/820 [===========================>..] - ETA: 0s - loss: 2.6397WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.6390 - val_loss: 2.9884\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6025 - val_loss: 2.9911\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6135 - val_loss: 2.9995\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6054 - val_loss: 3.0282\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "802/820 [============================>.] - ETA: 0s - loss: 2.3317WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.3318 - val_loss: 2.6847\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3126 - val_loss: 2.6687\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3198 - val_loss: 2.7079\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3138 - val_loss: 2.6835\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3249 - val_loss: 2.6941\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "764/820 [==========================>...] - ETA: 0s - loss: 1.9220WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9218 - val_loss: 2.2065\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.8987 - val_loss: 2.1824\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9062 - val_loss: 2.2016\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.8982 - val_loss: 2.1990\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9091 - val_loss: 2.1970\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "787/820 [===========================>..] - ETA: 0s - loss: 1.3960WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 1.3961 - val_loss: 1.6085\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3843 - val_loss: 1.5867\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3935 - val_loss: 1.6231\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3875 - val_loss: 1.5811\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3883 - val_loss: 1.5803\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3745 - val_loss: 1.5644\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3948 - val_loss: 1.5888\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4009 - val_loss: 1.5882\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3732 - val_loss: 1.5717\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812/820 [============================>.] - ETA: 0s - loss: 0.7810WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 0.7810 - val_loss: 0.8715\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7632 - val_loss: 0.8912\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7746 - val_loss: 0.8905\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7694 - val_loss: 0.9020\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_12_input'), name='dense_12_input', description=\"created by layer 'dense_12_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3888, 18)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    model.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model.fit(Day, Day78, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred = pd.DataFrame(model.predict(df_test))\n",
    "    results = pd.concat([results, pred], axis=1)\n",
    "\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.106918</td>\n",
       "      <td>-0.120948</td>\n",
       "      <td>-0.079273</td>\n",
       "      <td>0.062259</td>\n",
       "      <td>-0.191842</td>\n",
       "      <td>0.025832</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>-0.002217</td>\n",
       "      <td>0.114272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.104244</td>\n",
       "      <td>-0.121900</td>\n",
       "      <td>-0.079003</td>\n",
       "      <td>0.058536</td>\n",
       "      <td>-0.192401</td>\n",
       "      <td>0.022430</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>0.112941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.100999</td>\n",
       "      <td>-0.124836</td>\n",
       "      <td>-0.070515</td>\n",
       "      <td>-0.008604</td>\n",
       "      <td>-0.180071</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>-0.009600</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.114387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.104531</td>\n",
       "      <td>-0.125177</td>\n",
       "      <td>-0.070703</td>\n",
       "      <td>-0.006476</td>\n",
       "      <td>-0.180108</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.008582</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.113966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.105793</td>\n",
       "      <td>-0.126267</td>\n",
       "      <td>-0.062580</td>\n",
       "      <td>-0.005565</td>\n",
       "      <td>-0.165916</td>\n",
       "      <td>-0.003966</td>\n",
       "      <td>-0.008398</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.114762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.108143</td>\n",
       "      <td>-0.128241</td>\n",
       "      <td>-0.062087</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.165554</td>\n",
       "      <td>-0.005609</td>\n",
       "      <td>-0.009637</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.116329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.115231</td>\n",
       "      <td>-0.131903</td>\n",
       "      <td>-0.053621</td>\n",
       "      <td>-0.001259</td>\n",
       "      <td>-0.150381</td>\n",
       "      <td>-0.006834</td>\n",
       "      <td>-0.009636</td>\n",
       "      <td>0.007550</td>\n",
       "      <td>0.117830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.117750</td>\n",
       "      <td>-0.133064</td>\n",
       "      <td>-0.051847</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>-0.148185</td>\n",
       "      <td>-0.003416</td>\n",
       "      <td>-0.011120</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.127272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.117665</td>\n",
       "      <td>-0.135536</td>\n",
       "      <td>-0.043223</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>-0.129848</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>-0.005668</td>\n",
       "      <td>0.008908</td>\n",
       "      <td>0.133470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.118352</td>\n",
       "      <td>-0.136167</td>\n",
       "      <td>-0.045217</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>-0.130463</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>-0.004389</td>\n",
       "      <td>0.008692</td>\n",
       "      <td>0.137462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.129696</td>\n",
       "      <td>-0.130071</td>\n",
       "      <td>-0.043495</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>-0.117996</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>-0.001811</td>\n",
       "      <td>0.013214</td>\n",
       "      <td>0.146869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.129130</td>\n",
       "      <td>-0.132022</td>\n",
       "      <td>-0.042745</td>\n",
       "      <td>0.010673</td>\n",
       "      <td>-0.115026</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>-0.002923</td>\n",
       "      <td>0.012430</td>\n",
       "      <td>0.143102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.129863</td>\n",
       "      <td>-0.123927</td>\n",
       "      <td>-0.033425</td>\n",
       "      <td>0.022814</td>\n",
       "      <td>-0.067753</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.020935</td>\n",
       "      <td>0.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.128609</td>\n",
       "      <td>-0.126129</td>\n",
       "      <td>-0.033552</td>\n",
       "      <td>0.024995</td>\n",
       "      <td>-0.049833</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.020223</td>\n",
       "      <td>1.075521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.217106</td>\n",
       "      <td>-0.117790</td>\n",
       "      <td>-0.055895</td>\n",
       "      <td>0.034709</td>\n",
       "      <td>0.070270</td>\n",
       "      <td>0.113831</td>\n",
       "      <td>0.041357</td>\n",
       "      <td>0.815903</td>\n",
       "      <td>3.303328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.112919</td>\n",
       "      <td>0.026585</td>\n",
       "      <td>0.286901</td>\n",
       "      <td>0.357709</td>\n",
       "      <td>0.883365</td>\n",
       "      <td>0.669870</td>\n",
       "      <td>0.901192</td>\n",
       "      <td>2.357839</td>\n",
       "      <td>5.864777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.116548</td>\n",
       "      <td>1.328232</td>\n",
       "      <td>4.582942</td>\n",
       "      <td>3.268538</td>\n",
       "      <td>7.970702</td>\n",
       "      <td>6.126076</td>\n",
       "      <td>7.882350</td>\n",
       "      <td>15.125023</td>\n",
       "      <td>22.036236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.603195</td>\n",
       "      <td>3.783231</td>\n",
       "      <td>9.277839</td>\n",
       "      <td>7.791111</td>\n",
       "      <td>12.961576</td>\n",
       "      <td>11.342305</td>\n",
       "      <td>14.436786</td>\n",
       "      <td>20.432245</td>\n",
       "      <td>24.944843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.278840</td>\n",
       "      <td>12.680812</td>\n",
       "      <td>21.836594</td>\n",
       "      <td>17.797110</td>\n",
       "      <td>26.550579</td>\n",
       "      <td>22.528593</td>\n",
       "      <td>26.770861</td>\n",
       "      <td>32.828815</td>\n",
       "      <td>37.085972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.936314</td>\n",
       "      <td>16.540617</td>\n",
       "      <td>25.009468</td>\n",
       "      <td>20.677988</td>\n",
       "      <td>26.115402</td>\n",
       "      <td>24.399092</td>\n",
       "      <td>26.594545</td>\n",
       "      <td>31.007238</td>\n",
       "      <td>34.573780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.450748</td>\n",
       "      <td>23.603296</td>\n",
       "      <td>32.581451</td>\n",
       "      <td>28.047585</td>\n",
       "      <td>33.257725</td>\n",
       "      <td>32.399391</td>\n",
       "      <td>33.169567</td>\n",
       "      <td>33.733704</td>\n",
       "      <td>35.826694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.226276</td>\n",
       "      <td>22.739199</td>\n",
       "      <td>30.937910</td>\n",
       "      <td>29.532001</td>\n",
       "      <td>33.898979</td>\n",
       "      <td>33.959206</td>\n",
       "      <td>35.282021</td>\n",
       "      <td>35.631695</td>\n",
       "      <td>36.794380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17.926546</td>\n",
       "      <td>31.176088</td>\n",
       "      <td>44.288765</td>\n",
       "      <td>36.627312</td>\n",
       "      <td>45.188179</td>\n",
       "      <td>40.069000</td>\n",
       "      <td>42.912334</td>\n",
       "      <td>45.900799</td>\n",
       "      <td>47.940739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17.436552</td>\n",
       "      <td>29.696701</td>\n",
       "      <td>40.972023</td>\n",
       "      <td>34.438324</td>\n",
       "      <td>42.371269</td>\n",
       "      <td>36.539970</td>\n",
       "      <td>39.510647</td>\n",
       "      <td>43.110065</td>\n",
       "      <td>45.953808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>18.304255</td>\n",
       "      <td>27.609091</td>\n",
       "      <td>40.694618</td>\n",
       "      <td>33.615944</td>\n",
       "      <td>40.838333</td>\n",
       "      <td>35.975792</td>\n",
       "      <td>38.437199</td>\n",
       "      <td>41.488632</td>\n",
       "      <td>42.786869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18.084896</td>\n",
       "      <td>27.466993</td>\n",
       "      <td>39.801838</td>\n",
       "      <td>33.376640</td>\n",
       "      <td>39.555378</td>\n",
       "      <td>37.717224</td>\n",
       "      <td>39.623631</td>\n",
       "      <td>42.056126</td>\n",
       "      <td>42.824215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.875334</td>\n",
       "      <td>25.002893</td>\n",
       "      <td>36.440166</td>\n",
       "      <td>30.225227</td>\n",
       "      <td>35.754326</td>\n",
       "      <td>33.944706</td>\n",
       "      <td>35.453167</td>\n",
       "      <td>37.966335</td>\n",
       "      <td>39.017914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.224295</td>\n",
       "      <td>25.754990</td>\n",
       "      <td>37.665642</td>\n",
       "      <td>30.528259</td>\n",
       "      <td>37.632496</td>\n",
       "      <td>30.993601</td>\n",
       "      <td>33.269405</td>\n",
       "      <td>38.151154</td>\n",
       "      <td>38.697197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.174002</td>\n",
       "      <td>22.151552</td>\n",
       "      <td>32.156979</td>\n",
       "      <td>26.273987</td>\n",
       "      <td>30.904240</td>\n",
       "      <td>29.075773</td>\n",
       "      <td>29.483982</td>\n",
       "      <td>31.287039</td>\n",
       "      <td>32.783516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14.869037</td>\n",
       "      <td>21.593700</td>\n",
       "      <td>31.222584</td>\n",
       "      <td>26.607685</td>\n",
       "      <td>31.252970</td>\n",
       "      <td>29.745306</td>\n",
       "      <td>30.106936</td>\n",
       "      <td>31.719355</td>\n",
       "      <td>33.507172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9.054276</td>\n",
       "      <td>18.449099</td>\n",
       "      <td>25.647400</td>\n",
       "      <td>23.549423</td>\n",
       "      <td>27.832392</td>\n",
       "      <td>27.133284</td>\n",
       "      <td>27.453039</td>\n",
       "      <td>27.510105</td>\n",
       "      <td>28.347534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10.094210</td>\n",
       "      <td>16.234509</td>\n",
       "      <td>26.750645</td>\n",
       "      <td>20.977509</td>\n",
       "      <td>24.012642</td>\n",
       "      <td>22.170401</td>\n",
       "      <td>22.478609</td>\n",
       "      <td>22.928741</td>\n",
       "      <td>25.129354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.551653</td>\n",
       "      <td>3.376426</td>\n",
       "      <td>10.736365</td>\n",
       "      <td>8.271430</td>\n",
       "      <td>11.474004</td>\n",
       "      <td>10.300305</td>\n",
       "      <td>12.245663</td>\n",
       "      <td>12.807835</td>\n",
       "      <td>14.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.176253</td>\n",
       "      <td>0.149484</td>\n",
       "      <td>1.604793</td>\n",
       "      <td>0.875711</td>\n",
       "      <td>1.454310</td>\n",
       "      <td>0.735043</td>\n",
       "      <td>1.703559</td>\n",
       "      <td>4.874080</td>\n",
       "      <td>8.311294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.337851</td>\n",
       "      <td>-0.296168</td>\n",
       "      <td>0.121475</td>\n",
       "      <td>0.062491</td>\n",
       "      <td>0.147693</td>\n",
       "      <td>0.008105</td>\n",
       "      <td>0.087982</td>\n",
       "      <td>0.079593</td>\n",
       "      <td>0.948957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.356946</td>\n",
       "      <td>-0.290678</td>\n",
       "      <td>0.110224</td>\n",
       "      <td>0.056473</td>\n",
       "      <td>0.132559</td>\n",
       "      <td>0.018768</td>\n",
       "      <td>0.109764</td>\n",
       "      <td>0.080352</td>\n",
       "      <td>0.892884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.032899</td>\n",
       "      <td>-0.013330</td>\n",
       "      <td>0.094393</td>\n",
       "      <td>-0.006054</td>\n",
       "      <td>0.102937</td>\n",
       "      <td>-0.000559</td>\n",
       "      <td>0.023919</td>\n",
       "      <td>0.099939</td>\n",
       "      <td>0.076749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.032862</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>0.082467</td>\n",
       "      <td>-0.006797</td>\n",
       "      <td>0.098292</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>0.023895</td>\n",
       "      <td>0.083218</td>\n",
       "      <td>0.057606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.049657</td>\n",
       "      <td>-0.056134</td>\n",
       "      <td>0.044839</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.010801</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.026097</td>\n",
       "      <td>0.015196</td>\n",
       "      <td>0.028572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.046683</td>\n",
       "      <td>-0.056738</td>\n",
       "      <td>0.035830</td>\n",
       "      <td>-0.009474</td>\n",
       "      <td>0.011318</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.026120</td>\n",
       "      <td>0.013972</td>\n",
       "      <td>0.027685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.034375</td>\n",
       "      <td>-0.062112</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>-0.012243</td>\n",
       "      <td>-0.002513</td>\n",
       "      <td>0.006232</td>\n",
       "      <td>0.029394</td>\n",
       "      <td>0.013708</td>\n",
       "      <td>0.022872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.033595</td>\n",
       "      <td>-0.061995</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>-0.012015</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>0.029706</td>\n",
       "      <td>0.013399</td>\n",
       "      <td>0.022580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.029068</td>\n",
       "      <td>-0.065443</td>\n",
       "      <td>0.008658</td>\n",
       "      <td>-0.012576</td>\n",
       "      <td>-0.009332</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>0.030706</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.019430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.027590</td>\n",
       "      <td>-0.065072</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>-0.012140</td>\n",
       "      <td>-0.007296</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.018571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.022062</td>\n",
       "      <td>-0.069171</td>\n",
       "      <td>0.012057</td>\n",
       "      <td>-0.010160</td>\n",
       "      <td>-0.012629</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>0.032154</td>\n",
       "      <td>0.009115</td>\n",
       "      <td>0.014422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.021784</td>\n",
       "      <td>-0.068969</td>\n",
       "      <td>0.012120</td>\n",
       "      <td>-0.010020</td>\n",
       "      <td>-0.012057</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.032476</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>0.014316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.013247</td>\n",
       "      <td>-0.069471</td>\n",
       "      <td>0.024385</td>\n",
       "      <td>-0.003161</td>\n",
       "      <td>-0.012864</td>\n",
       "      <td>-0.000959</td>\n",
       "      <td>0.033676</td>\n",
       "      <td>0.010896</td>\n",
       "      <td>0.011743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.013247</td>\n",
       "      <td>-0.069471</td>\n",
       "      <td>0.024385</td>\n",
       "      <td>-0.003161</td>\n",
       "      <td>-0.012864</td>\n",
       "      <td>-0.000959</td>\n",
       "      <td>0.033676</td>\n",
       "      <td>0.010896</td>\n",
       "      <td>0.011743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1          1          1          1          1          1  \\\n",
       "0   -0.106918  -0.120948  -0.079273   0.062259  -0.191842   0.025832   \n",
       "1   -0.104244  -0.121900  -0.079003   0.058536  -0.192401   0.022430   \n",
       "2   -0.100999  -0.124836  -0.070515  -0.008604  -0.180071  -0.001472   \n",
       "3   -0.104531  -0.125177  -0.070703  -0.006476  -0.180108  -0.000270   \n",
       "4   -0.105793  -0.126267  -0.062580  -0.005565  -0.165916  -0.003966   \n",
       "5   -0.108143  -0.128241  -0.062087  -0.004919  -0.165554  -0.005609   \n",
       "6   -0.115231  -0.131903  -0.053621  -0.001259  -0.150381  -0.006834   \n",
       "7   -0.117750  -0.133064  -0.051847   0.000477  -0.148185  -0.003416   \n",
       "8   -0.117665  -0.135536  -0.043223   0.005278  -0.129848  -0.000313   \n",
       "9   -0.118352  -0.136167  -0.045217   0.004708  -0.130463  -0.000676   \n",
       "10  -0.129696  -0.130071  -0.043495   0.008679  -0.117996   0.002496   \n",
       "11  -0.129130  -0.132022  -0.042745   0.010673  -0.115026   0.003572   \n",
       "12  -0.129863  -0.123927  -0.033425   0.022814  -0.067753   0.001268   \n",
       "13  -0.128609  -0.126129  -0.033552   0.024995  -0.049833   0.001445   \n",
       "14  -0.217106  -0.117790  -0.055895   0.034709   0.070270   0.113831   \n",
       "15  -0.112919   0.026585   0.286901   0.357709   0.883365   0.669870   \n",
       "16   1.116548   1.328232   4.582942   3.268538   7.970702   6.126076   \n",
       "17   2.603195   3.783231   9.277839   7.791111  12.961576  11.342305   \n",
       "18   8.278840  12.680812  21.836594  17.797110  26.550579  22.528593   \n",
       "19  10.936314  16.540617  25.009468  20.677988  26.115402  24.399092   \n",
       "20  14.450748  23.603296  32.581451  28.047585  33.257725  32.399391   \n",
       "21  13.226276  22.739199  30.937910  29.532001  33.898979  33.959206   \n",
       "22  17.926546  31.176088  44.288765  36.627312  45.188179  40.069000   \n",
       "23  17.436552  29.696701  40.972023  34.438324  42.371269  36.539970   \n",
       "24  18.304255  27.609091  40.694618  33.615944  40.838333  35.975792   \n",
       "25  18.084896  27.466993  39.801838  33.376640  39.555378  37.717224   \n",
       "26  16.875334  25.002893  36.440166  30.225227  35.754326  33.944706   \n",
       "27  18.224295  25.754990  37.665642  30.528259  37.632496  30.993601   \n",
       "28  15.174002  22.151552  32.156979  26.273987  30.904240  29.075773   \n",
       "29  14.869037  21.593700  31.222584  26.607685  31.252970  29.745306   \n",
       "30   9.054276  18.449099  25.647400  23.549423  27.832392  27.133284   \n",
       "31  10.094210  16.234509  26.750645  20.977509  24.012642  22.170401   \n",
       "32   2.551653   3.376426  10.736365   8.271430  11.474004  10.300305   \n",
       "33   0.176253   0.149484   1.604793   0.875711   1.454310   0.735043   \n",
       "34  -0.337851  -0.296168   0.121475   0.062491   0.147693   0.008105   \n",
       "35  -0.356946  -0.290678   0.110224   0.056473   0.132559   0.018768   \n",
       "36  -0.032899  -0.013330   0.094393  -0.006054   0.102937  -0.000559   \n",
       "37  -0.032862  -0.017885   0.082467  -0.006797   0.098292  -0.001151   \n",
       "38  -0.049657  -0.056134   0.044839  -0.008325   0.010801   0.004934   \n",
       "39  -0.046683  -0.056738   0.035830  -0.009474   0.011318   0.004606   \n",
       "40  -0.034375  -0.062112   0.004768  -0.012243  -0.002513   0.006232   \n",
       "41  -0.033595  -0.061995   0.004844  -0.012015  -0.000224   0.006405   \n",
       "42  -0.029068  -0.065443   0.008658  -0.012576  -0.009332   0.007053   \n",
       "43  -0.027590  -0.065072   0.008275  -0.012140  -0.007296   0.006894   \n",
       "44  -0.022062  -0.069171   0.012057  -0.010160  -0.012629   0.002437   \n",
       "45  -0.021784  -0.068969   0.012120  -0.010020  -0.012057   0.002803   \n",
       "46  -0.013247  -0.069471   0.024385  -0.003161  -0.012864  -0.000959   \n",
       "47  -0.013247  -0.069471   0.024385  -0.003161  -0.012864  -0.000959   \n",
       "\n",
       "            1          1          1  \n",
       "0    0.002594  -0.002217   0.114272  \n",
       "1   -0.000514  -0.002309   0.112941  \n",
       "2   -0.009600   0.001669   0.114387  \n",
       "3   -0.008582   0.001346   0.113966  \n",
       "4   -0.008398   0.003413   0.114762  \n",
       "5   -0.009637   0.003766   0.116329  \n",
       "6   -0.009636   0.007550   0.117830  \n",
       "7   -0.011120   0.006938   0.127272  \n",
       "8   -0.005668   0.008908   0.133470  \n",
       "9   -0.004389   0.008692   0.137462  \n",
       "10  -0.001811   0.013214   0.146869  \n",
       "11  -0.002923   0.012430   0.143102  \n",
       "12   0.004051   0.020935   0.935300  \n",
       "13   0.003663   0.020223   1.075521  \n",
       "14   0.041357   0.815903   3.303328  \n",
       "15   0.901192   2.357839   5.864777  \n",
       "16   7.882350  15.125023  22.036236  \n",
       "17  14.436786  20.432245  24.944843  \n",
       "18  26.770861  32.828815  37.085972  \n",
       "19  26.594545  31.007238  34.573780  \n",
       "20  33.169567  33.733704  35.826694  \n",
       "21  35.282021  35.631695  36.794380  \n",
       "22  42.912334  45.900799  47.940739  \n",
       "23  39.510647  43.110065  45.953808  \n",
       "24  38.437199  41.488632  42.786869  \n",
       "25  39.623631  42.056126  42.824215  \n",
       "26  35.453167  37.966335  39.017914  \n",
       "27  33.269405  38.151154  38.697197  \n",
       "28  29.483982  31.287039  32.783516  \n",
       "29  30.106936  31.719355  33.507172  \n",
       "30  27.453039  27.510105  28.347534  \n",
       "31  22.478609  22.928741  25.129354  \n",
       "32  12.245663  12.807835  14.290800  \n",
       "33   1.703559   4.874080   8.311294  \n",
       "34   0.087982   0.079593   0.948957  \n",
       "35   0.109764   0.080352   0.892884  \n",
       "36   0.023919   0.099939   0.076749  \n",
       "37   0.023895   0.083218   0.057606  \n",
       "38   0.026097   0.015196   0.028572  \n",
       "39   0.026120   0.013972   0.027685  \n",
       "40   0.029394   0.013708   0.022872  \n",
       "41   0.029706   0.013399   0.022580  \n",
       "42   0.030706   0.009576   0.019430  \n",
       "43   0.030612   0.008436   0.018571  \n",
       "44   0.032154   0.009115   0.014422  \n",
       "45   0.032476   0.009312   0.014316  \n",
       "46   0.033676   0.010896   0.011743  \n",
       "47   0.033676   0.010896   0.011743  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1][:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34889\n",
      "Early stopping, best iteration is:\n",
      "[418]\tvalid_0's quantile: 1.34812\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.14466\n",
      "[1000]\tvalid_0's quantile: 2.13764\n",
      "[1500]\tvalid_0's quantile: 2.13582\n",
      "[2000]\tvalid_0's quantile: 2.1334\n",
      "Early stopping, best iteration is:\n",
      "[1749]\tvalid_0's quantile: 2.13312\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.53565\n",
      "[1000]\tvalid_0's quantile: 2.50726\n",
      "[1500]\tvalid_0's quantile: 2.49216\n",
      "Early stopping, best iteration is:\n",
      "[1604]\tvalid_0's quantile: 2.48959\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.66191\n",
      "[1000]\tvalid_0's quantile: 2.62846\n",
      "[1500]\tvalid_0's quantile: 2.61266\n",
      "[2000]\tvalid_0's quantile: 2.6059\n",
      "[2500]\tvalid_0's quantile: 2.59923\n",
      "[3000]\tvalid_0's quantile: 2.59644\n",
      "Early stopping, best iteration is:\n",
      "[2707]\tvalid_0's quantile: 2.59598\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.55537\n",
      "[1000]\tvalid_0's quantile: 2.54183\n",
      "[1500]\tvalid_0's quantile: 2.52395\n",
      "[2000]\tvalid_0's quantile: 2.5191\n",
      "[2500]\tvalid_0's quantile: 2.51605\n",
      "[3000]\tvalid_0's quantile: 2.51386\n",
      "[3500]\tvalid_0's quantile: 2.50859\n",
      "[4000]\tvalid_0's quantile: 2.50447\n",
      "[4500]\tvalid_0's quantile: 2.50256\n",
      "[5000]\tvalid_0's quantile: 2.50036\n",
      "[5500]\tvalid_0's quantile: 2.498\n",
      "[6000]\tvalid_0's quantile: 2.49693\n",
      "[6500]\tvalid_0's quantile: 2.49595\n",
      "Early stopping, best iteration is:\n",
      "[6563]\tvalid_0's quantile: 2.49582\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.28838\n",
      "[1000]\tvalid_0's quantile: 2.26798\n",
      "[1500]\tvalid_0's quantile: 2.25775\n",
      "[2000]\tvalid_0's quantile: 2.25273\n",
      "[2500]\tvalid_0's quantile: 2.24936\n",
      "[3000]\tvalid_0's quantile: 2.24642\n",
      "[3500]\tvalid_0's quantile: 2.24451\n",
      "[4000]\tvalid_0's quantile: 2.24427\n",
      "Early stopping, best iteration is:\n",
      "[3735]\tvalid_0's quantile: 2.24391\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.87856\n",
      "[1000]\tvalid_0's quantile: 1.86236\n",
      "[1500]\tvalid_0's quantile: 1.85737\n",
      "[2000]\tvalid_0's quantile: 1.85491\n",
      "Early stopping, best iteration is:\n",
      "[1840]\tvalid_0's quantile: 1.85404\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34949\n",
      "[1000]\tvalid_0's quantile: 1.34333\n",
      "Early stopping, best iteration is:\n",
      "[898]\tvalid_0's quantile: 1.34271\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.748569\n",
      "[1000]\tvalid_0's quantile: 0.746232\n",
      "[1500]\tvalid_0's quantile: 0.745479\n",
      "Early stopping, best iteration is:\n",
      "[1284]\tvalid_0's quantile: 0.745153\n",
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.39392\n",
      "Early stopping, best iteration is:\n",
      "[331]\tvalid_0's quantile: 1.3933\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.19349\n",
      "Early stopping, best iteration is:\n",
      "[616]\tvalid_0's quantile: 2.18579\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.61892\n",
      "[1000]\tvalid_0's quantile: 2.57944\n",
      "[1500]\tvalid_0's quantile: 2.578\n",
      "Early stopping, best iteration is:\n",
      "[1380]\tvalid_0's quantile: 2.57786\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.74331\n",
      "[1000]\tvalid_0's quantile: 2.70696\n",
      "[1500]\tvalid_0's quantile: 2.70093\n",
      "[2000]\tvalid_0's quantile: 2.69465\n",
      "[2500]\tvalid_0's quantile: 2.68928\n",
      "[3000]\tvalid_0's quantile: 2.68205\n",
      "[3500]\tvalid_0's quantile: 2.67219\n",
      "[4000]\tvalid_0's quantile: 2.66516\n",
      "[4500]\tvalid_0's quantile: 2.6608\n",
      "Early stopping, best iteration is:\n",
      "[4475]\tvalid_0's quantile: 2.6608\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.6458\n",
      "[1000]\tvalid_0's quantile: 2.62814\n",
      "[1500]\tvalid_0's quantile: 2.61626\n",
      "[2000]\tvalid_0's quantile: 2.60023\n",
      "[2500]\tvalid_0's quantile: 2.58706\n",
      "[3000]\tvalid_0's quantile: 2.58403\n",
      "Early stopping, best iteration is:\n",
      "[2807]\tvalid_0's quantile: 2.58403\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.36786\n",
      "[1000]\tvalid_0's quantile: 2.35343\n",
      "[1500]\tvalid_0's quantile: 2.33476\n",
      "[2000]\tvalid_0's quantile: 2.32426\n",
      "[2500]\tvalid_0's quantile: 2.31984\n",
      "[3000]\tvalid_0's quantile: 2.31736\n",
      "Early stopping, best iteration is:\n",
      "[2871]\tvalid_0's quantile: 2.31729\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.96038\n",
      "[1000]\tvalid_0's quantile: 1.93778\n",
      "[1500]\tvalid_0's quantile: 1.92682\n",
      "[2000]\tvalid_0's quantile: 1.91583\n",
      "[2500]\tvalid_0's quantile: 1.91245\n",
      "Early stopping, best iteration is:\n",
      "[2560]\tvalid_0's quantile: 1.91213\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.40785\n",
      "[1000]\tvalid_0's quantile: 1.39944\n",
      "[1500]\tvalid_0's quantile: 1.39438\n",
      "[2000]\tvalid_0's quantile: 1.39224\n",
      "[2500]\tvalid_0's quantile: 1.39124\n",
      "Early stopping, best iteration is:\n",
      "[2308]\tvalid_0's quantile: 1.39107\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.780449\n",
      "[1000]\tvalid_0's quantile: 0.777177\n",
      "[1500]\tvalid_0's quantile: 0.775629\n",
      "[2000]\tvalid_0's quantile: 0.77524\n",
      "Early stopping, best iteration is:\n",
      "[1831]\tvalid_0's quantile: 0.774899\n"
     ]
    }
   ],
   "source": [
    "def LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "    \n",
    "    # (a) Modeling  \n",
    "    model = LGBMRegressor(objective='quantile', alpha=q,\n",
    "                         n_estimators=10000, bagging_fraction=0.7, learning_rate=0.027, subsample=0.7)                   \n",
    "                         \n",
    "                         \n",
    "    model.fit(X_train, Y_train, eval_metric = ['quantile'], \n",
    "          eval_set=[(X_valid, Y_valid)], early_stopping_rounds=300, verbose=500)\n",
    "\n",
    "    # (b) Predictions\n",
    "    pred = pd.Series(model.predict(X_test).round(2))\n",
    "    return pred, model\n",
    "\n",
    "def train_data(X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "\n",
    "    LGBM_models=[]\n",
    "    LGBM_actual_pred = pd.DataFrame()\n",
    "\n",
    "    for q in q_lst:\n",
    "        print(q)\n",
    "        pred , model = LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test)\n",
    "        LGBM_models.append(model)\n",
    "        LGBM_actual_pred = pd.concat([LGBM_actual_pred,pred],axis=1)\n",
    "\n",
    "    LGBM_actual_pred.columns=q_lst\n",
    "    \n",
    "    return LGBM_models, LGBM_actual_pred\n",
    "\n",
    "models_1, results_1 = train_data(X_train_1, Y_train_1, X_valid_1, Y_valid_1, df_test)\n",
    "models_2, results_2 = train_data(X_train_2, Y_train_2, X_valid_2, Y_valid_2, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45\n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51\n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11\n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34\n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51\n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17\n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03\n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17\n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82\n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14\n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61\n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92\n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20\n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73\n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94\n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20\n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59\n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49\n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13\n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_1[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>18.24</td>\n",
       "      <td>25.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.74</td>\n",
       "      <td>14.78</td>\n",
       "      <td>19.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>22.60</td>\n",
       "      <td>20.57</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.66</td>\n",
       "      <td>32.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.47</td>\n",
       "      <td>19.06</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.62</td>\n",
       "      <td>20.20</td>\n",
       "      <td>17.15</td>\n",
       "      <td>22.85</td>\n",
       "      <td>33.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.33</td>\n",
       "      <td>24.78</td>\n",
       "      <td>24.52</td>\n",
       "      <td>23.02</td>\n",
       "      <td>28.58</td>\n",
       "      <td>28.55</td>\n",
       "      <td>25.06</td>\n",
       "      <td>26.39</td>\n",
       "      <td>27.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.26</td>\n",
       "      <td>26.20</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.70</td>\n",
       "      <td>29.22</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.05</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.96</td>\n",
       "      <td>29.89</td>\n",
       "      <td>33.58</td>\n",
       "      <td>33.28</td>\n",
       "      <td>38.13</td>\n",
       "      <td>35.95</td>\n",
       "      <td>34.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.24</td>\n",
       "      <td>24.45</td>\n",
       "      <td>29.77</td>\n",
       "      <td>29.36</td>\n",
       "      <td>34.56</td>\n",
       "      <td>30.81</td>\n",
       "      <td>26.23</td>\n",
       "      <td>27.65</td>\n",
       "      <td>34.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.30</td>\n",
       "      <td>25.22</td>\n",
       "      <td>31.45</td>\n",
       "      <td>29.95</td>\n",
       "      <td>34.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>29.10</td>\n",
       "      <td>27.16</td>\n",
       "      <td>31.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.03</td>\n",
       "      <td>30.02</td>\n",
       "      <td>34.72</td>\n",
       "      <td>32.49</td>\n",
       "      <td>35.12</td>\n",
       "      <td>35.82</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.21</td>\n",
       "      <td>27.84</td>\n",
       "      <td>33.51</td>\n",
       "      <td>32.70</td>\n",
       "      <td>37.10</td>\n",
       "      <td>34.75</td>\n",
       "      <td>29.17</td>\n",
       "      <td>27.61</td>\n",
       "      <td>29.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.50</td>\n",
       "      <td>22.99</td>\n",
       "      <td>30.11</td>\n",
       "      <td>31.82</td>\n",
       "      <td>32.80</td>\n",
       "      <td>33.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>36.02</td>\n",
       "      <td>44.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.97</td>\n",
       "      <td>21.03</td>\n",
       "      <td>25.88</td>\n",
       "      <td>24.37</td>\n",
       "      <td>28.26</td>\n",
       "      <td>26.10</td>\n",
       "      <td>21.49</td>\n",
       "      <td>20.61</td>\n",
       "      <td>26.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>22.61</td>\n",
       "      <td>27.12</td>\n",
       "      <td>27.53</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.98</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.92</td>\n",
       "      <td>25.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.57</td>\n",
       "      <td>17.69</td>\n",
       "      <td>21.43</td>\n",
       "      <td>22.13</td>\n",
       "      <td>22.79</td>\n",
       "      <td>24.23</td>\n",
       "      <td>23.32</td>\n",
       "      <td>21.47</td>\n",
       "      <td>23.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.19</td>\n",
       "      <td>15.84</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.86</td>\n",
       "      <td>15.93</td>\n",
       "      <td>16.22</td>\n",
       "      <td>15.55</td>\n",
       "      <td>15.47</td>\n",
       "      <td>24.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.95</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>15.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>13.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "15   0.48   0.70   1.80   2.18   0.98   3.56   2.99   6.80   9.87\n",
       "16   2.67   7.45   8.62  10.08  10.06  11.52  17.00  19.79  20.92\n",
       "17   3.80   8.55   8.15  10.49  10.84  12.03  15.05  18.24  25.56\n",
       "18   7.74  14.78  19.70  18.93  22.60  20.57  16.19  15.66  32.96\n",
       "19   9.47  19.06  21.52  21.87  23.62  20.20  17.15  22.85  33.97\n",
       "20  14.33  24.78  24.52  23.02  28.58  28.55  25.06  26.39  27.24\n",
       "21  15.26  26.20  24.57  24.70  29.22  31.11  31.05  31.59  32.45\n",
       "22  15.96  29.89  33.58  33.28  38.13  35.95  34.24  33.24  33.78\n",
       "23  14.24  24.45  29.77  29.36  34.56  30.81  26.23  27.65  34.53\n",
       "24  14.30  25.22  31.45  29.95  34.70  31.31  29.10  27.16  31.12\n",
       "25  16.03  30.02  34.72  32.49  35.12  35.82  33.60  32.73  34.11\n",
       "26  15.21  27.84  33.51  32.70  37.10  34.75  29.17  27.61  29.70\n",
       "27   9.50  22.99  30.11  31.82  32.80  33.88  19.43  36.02  44.45\n",
       "28  10.97  21.03  25.88  24.37  28.26  26.10  21.49  20.61  26.21\n",
       "29  12.63  22.61  27.12  27.53  31.02  29.98  27.37  22.92  25.82\n",
       "30   8.57  17.69  21.43  22.13  22.79  24.23  23.32  21.47  23.67\n",
       "31   7.19  15.84  18.83  17.86  15.93  16.22  15.55  15.47  24.20\n",
       "32   2.95   8.16   8.37   7.24   6.39   6.81   6.33   6.51  15.83\n",
       "33   1.71   4.94   4.62   3.24   3.42   1.19   3.41  -0.50  13.80\n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17\n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17\n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.5208 - val_loss: 1.5986\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4160 - val_loss: 1.5863\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4031 - val_loss: 1.5770\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3982 - val_loss: 1.5685\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4022 - val_loss: 1.5659\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3864 - val_loss: 1.5852\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3860 - val_loss: 1.5699\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3978 - val_loss: 1.5535\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3806 - val_loss: 1.5508\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3874 - val_loss: 1.5373\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3538 - val_loss: 1.5471\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3507 - val_loss: 1.5731\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3673 - val_loss: 1.5435\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3460 - val_loss: 1.5473\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3715 - val_loss: 1.5450\n",
      "Epoch 00015: early stopping\n",
      "0.2\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1881 - val_loss: 2.4979\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1809 - val_loss: 2.4934\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1665 - val_loss: 2.4907\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1562 - val_loss: 2.4985\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1800 - val_loss: 2.4952\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1649 - val_loss: 2.5031\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1710 - val_loss: 2.5039\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2009 - val_loss: 2.5108\n",
      "Epoch 00008: early stopping\n",
      "0.3\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5898 - val_loss: 2.9506\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5606 - val_loss: 2.9457\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5648 - val_loss: 2.9735\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5503 - val_loss: 2.9476\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5774 - val_loss: 2.9535\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5739 - val_loss: 2.9530\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5789 - val_loss: 2.9573\n",
      "Epoch 00007: early stopping\n",
      "0.4\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7032 - val_loss: 3.0585\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6584 - val_loss: 3.0483\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6706 - val_loss: 3.1222\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6502 - val_loss: 3.0722\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6742 - val_loss: 3.1071\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6878 - val_loss: 3.0691\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6942 - val_loss: 3.0517\n",
      "Epoch 00007: early stopping\n",
      "0.5\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 2.5783 - val_loss: 2.9218\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5330 - val_loss: 2.9137\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5453 - val_loss: 2.9142\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5380 - val_loss: 2.9338\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5454 - val_loss: 2.9339\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5639 - val_loss: 3.0185\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5792 - val_loss: 2.9231\n",
      "Epoch 00007: early stopping\n",
      "0.6\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 2.2763 - val_loss: 2.6017\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2505 - val_loss: 2.6309\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2617 - val_loss: 2.7133\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2604 - val_loss: 2.6287\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2593 - val_loss: 2.6202\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2842 - val_loss: 2.6671\n",
      "Epoch 00006: early stopping\n",
      "0.7\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 1.8805 - val_loss: 2.1657\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8536 - val_loss: 2.1524\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8738 - val_loss: 2.1952\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8683 - val_loss: 2.1669\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8593 - val_loss: 2.1641\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8859 - val_loss: 2.1313\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8898 - val_loss: 2.1326\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9081 - val_loss: 2.1495\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8548 - val_loss: 2.1340\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8984 - val_loss: 2.1187\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8186 - val_loss: 2.1362\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8398 - val_loss: 2.1507\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8565 - val_loss: 2.1561\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8476 - val_loss: 2.1359\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8545 - val_loss: 2.1431\n",
      "Epoch 00015: early stopping\n",
      "0.8\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3764 - val_loss: 1.5647\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3535 - val_loss: 1.5793\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3668 - val_loss: 1.5952\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3629 - val_loss: 1.5926\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3604 - val_loss: 1.5817\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3717 - val_loss: 1.5411\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3783 - val_loss: 1.5812\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3892 - val_loss: 1.5566\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3476 - val_loss: 1.5711\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3743 - val_loss: 1.5471\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3227 - val_loss: 1.5437\n",
      "Epoch 00011: early stopping\n",
      "0.9\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 0.7648 - val_loss: 0.8723\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7590 - val_loss: 0.8707\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7636 - val_loss: 0.8622\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7608 - val_loss: 0.8849\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7598 - val_loss: 0.8709\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7613 - val_loss: 0.8635\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7714 - val_loss: 0.8558\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7687 - val_loss: 0.8525\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7481 - val_loss: 0.8758\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7672 - val_loss: 0.8520\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7338 - val_loss: 0.8657\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7511 - val_loss: 0.8731\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7538 - val_loss: 0.8714\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7498 - val_loss: 0.8583\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7382 - val_loss: 0.8613\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.002349</td>\n",
       "      <td>-0.000611</td>\n",
       "      <td>-5.322974e-05</td>\n",
       "      <td>-0.001132</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.005992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002349</td>\n",
       "      <td>-0.000580</td>\n",
       "      <td>-4.089065e-05</td>\n",
       "      <td>-0.001100</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-0.000804</td>\n",
       "      <td>0.005994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002198</td>\n",
       "      <td>-0.000492</td>\n",
       "      <td>4.525296e-06</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>-0.000440</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>0.005838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002197</td>\n",
       "      <td>-0.000501</td>\n",
       "      <td>-4.300103e-05</td>\n",
       "      <td>-0.001075</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>-0.000448</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>0.005836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.000492</td>\n",
       "      <td>-5.187932e-05</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.005708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.002055</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>-1.704227e-05</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>-0.000440</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.000680</td>\n",
       "      <td>0.005774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.001904</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>-1.559872e-05</td>\n",
       "      <td>-0.000992</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>-0.000428</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000629</td>\n",
       "      <td>0.005673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.001921</td>\n",
       "      <td>-0.000624</td>\n",
       "      <td>1.359722e-04</td>\n",
       "      <td>-0.000885</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-0.000778</td>\n",
       "      <td>0.005802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.001781</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>3.316626e-05</td>\n",
       "      <td>-0.000867</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.000742</td>\n",
       "      <td>0.005954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.001783</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>4.380196e-05</td>\n",
       "      <td>-0.000829</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>0.006219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.001570</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>-1.795962e-05</td>\n",
       "      <td>-0.000867</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>0.006429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.001552</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>-7.804483e-07</td>\n",
       "      <td>-0.000892</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000781</td>\n",
       "      <td>0.006506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.001363</td>\n",
       "      <td>-0.000829</td>\n",
       "      <td>2.159970e-04</td>\n",
       "      <td>-0.000933</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.005964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.001346</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>2.551870e-04</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>0.006066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.001173</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>4.477808e-04</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.010319</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>0.007699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.001138</td>\n",
       "      <td>0.026840</td>\n",
       "      <td>4.269292e-01</td>\n",
       "      <td>0.579896</td>\n",
       "      <td>1.208835</td>\n",
       "      <td>1.931380</td>\n",
       "      <td>2.264489</td>\n",
       "      <td>3.505924</td>\n",
       "      <td>6.697193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.545917</td>\n",
       "      <td>2.923156</td>\n",
       "      <td>4.276439e+00</td>\n",
       "      <td>6.209582</td>\n",
       "      <td>7.237523</td>\n",
       "      <td>13.840117</td>\n",
       "      <td>11.925848</td>\n",
       "      <td>14.819144</td>\n",
       "      <td>18.424410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.145126</td>\n",
       "      <td>6.491627</td>\n",
       "      <td>8.360603e+00</td>\n",
       "      <td>9.964477</td>\n",
       "      <td>11.853331</td>\n",
       "      <td>18.825411</td>\n",
       "      <td>15.293234</td>\n",
       "      <td>18.488459</td>\n",
       "      <td>22.968403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.374944</td>\n",
       "      <td>12.183352</td>\n",
       "      <td>1.560298e+01</td>\n",
       "      <td>18.717142</td>\n",
       "      <td>21.110786</td>\n",
       "      <td>31.235043</td>\n",
       "      <td>25.608833</td>\n",
       "      <td>28.988262</td>\n",
       "      <td>35.066109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.854895</td>\n",
       "      <td>14.276540</td>\n",
       "      <td>1.781837e+01</td>\n",
       "      <td>20.075821</td>\n",
       "      <td>22.135254</td>\n",
       "      <td>32.518833</td>\n",
       "      <td>27.164389</td>\n",
       "      <td>29.940098</td>\n",
       "      <td>34.373230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11.239605</td>\n",
       "      <td>21.590355</td>\n",
       "      <td>2.684600e+01</td>\n",
       "      <td>29.139370</td>\n",
       "      <td>31.640022</td>\n",
       "      <td>40.497013</td>\n",
       "      <td>34.992290</td>\n",
       "      <td>34.357838</td>\n",
       "      <td>35.040707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.736603</td>\n",
       "      <td>22.990986</td>\n",
       "      <td>2.826448e+01</td>\n",
       "      <td>29.901171</td>\n",
       "      <td>33.216015</td>\n",
       "      <td>40.763508</td>\n",
       "      <td>37.388153</td>\n",
       "      <td>37.496067</td>\n",
       "      <td>38.646065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.094769</td>\n",
       "      <td>25.610161</td>\n",
       "      <td>3.147693e+01</td>\n",
       "      <td>35.560806</td>\n",
       "      <td>39.449242</td>\n",
       "      <td>48.078983</td>\n",
       "      <td>43.581604</td>\n",
       "      <td>45.140259</td>\n",
       "      <td>46.526817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11.841318</td>\n",
       "      <td>23.580547</td>\n",
       "      <td>2.970623e+01</td>\n",
       "      <td>33.425858</td>\n",
       "      <td>38.103825</td>\n",
       "      <td>46.693314</td>\n",
       "      <td>41.665810</td>\n",
       "      <td>42.564568</td>\n",
       "      <td>46.725201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13.024812</td>\n",
       "      <td>24.271233</td>\n",
       "      <td>2.935627e+01</td>\n",
       "      <td>33.446907</td>\n",
       "      <td>36.811199</td>\n",
       "      <td>44.081230</td>\n",
       "      <td>40.477814</td>\n",
       "      <td>41.763020</td>\n",
       "      <td>44.571140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>14.557906</td>\n",
       "      <td>25.263062</td>\n",
       "      <td>3.020491e+01</td>\n",
       "      <td>34.559525</td>\n",
       "      <td>36.564816</td>\n",
       "      <td>43.959026</td>\n",
       "      <td>40.759903</td>\n",
       "      <td>41.848156</td>\n",
       "      <td>40.766285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.561514</td>\n",
       "      <td>23.310167</td>\n",
       "      <td>2.627851e+01</td>\n",
       "      <td>30.077080</td>\n",
       "      <td>32.658752</td>\n",
       "      <td>40.300842</td>\n",
       "      <td>38.116993</td>\n",
       "      <td>38.818661</td>\n",
       "      <td>38.841042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.277952</td>\n",
       "      <td>21.480835</td>\n",
       "      <td>2.459570e+01</td>\n",
       "      <td>29.271950</td>\n",
       "      <td>32.528629</td>\n",
       "      <td>38.862167</td>\n",
       "      <td>38.129791</td>\n",
       "      <td>39.263615</td>\n",
       "      <td>44.214870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.884785</td>\n",
       "      <td>20.234631</td>\n",
       "      <td>2.113459e+01</td>\n",
       "      <td>21.924711</td>\n",
       "      <td>26.722857</td>\n",
       "      <td>32.588055</td>\n",
       "      <td>32.295059</td>\n",
       "      <td>33.063667</td>\n",
       "      <td>33.163689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.425123</td>\n",
       "      <td>20.989697</td>\n",
       "      <td>2.175147e+01</td>\n",
       "      <td>22.493423</td>\n",
       "      <td>27.621571</td>\n",
       "      <td>33.672691</td>\n",
       "      <td>32.862137</td>\n",
       "      <td>33.320732</td>\n",
       "      <td>32.327194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11.088366</td>\n",
       "      <td>17.787445</td>\n",
       "      <td>1.594661e+01</td>\n",
       "      <td>17.205816</td>\n",
       "      <td>21.964863</td>\n",
       "      <td>26.791088</td>\n",
       "      <td>25.845657</td>\n",
       "      <td>26.645065</td>\n",
       "      <td>26.425034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9.314135</td>\n",
       "      <td>15.365369</td>\n",
       "      <td>1.325009e+01</td>\n",
       "      <td>14.225792</td>\n",
       "      <td>17.608921</td>\n",
       "      <td>21.842791</td>\n",
       "      <td>21.973663</td>\n",
       "      <td>23.257534</td>\n",
       "      <td>23.031939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.630906</td>\n",
       "      <td>7.987370</td>\n",
       "      <td>6.276731e+00</td>\n",
       "      <td>7.385617</td>\n",
       "      <td>8.798072</td>\n",
       "      <td>10.998857</td>\n",
       "      <td>10.916775</td>\n",
       "      <td>13.166090</td>\n",
       "      <td>12.856496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.522015</td>\n",
       "      <td>1.900805</td>\n",
       "      <td>1.296174e+00</td>\n",
       "      <td>1.844411</td>\n",
       "      <td>2.720329</td>\n",
       "      <td>4.209159</td>\n",
       "      <td>5.963072</td>\n",
       "      <td>8.370705</td>\n",
       "      <td>8.801943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.037034</td>\n",
       "      <td>0.010962</td>\n",
       "      <td>8.842414e-04</td>\n",
       "      <td>-0.001576</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.054169</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.052519</td>\n",
       "      <td>0.007258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.034528</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>8.832077e-04</td>\n",
       "      <td>-0.001561</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.052865</td>\n",
       "      <td>0.009361</td>\n",
       "      <td>0.051644</td>\n",
       "      <td>0.007157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.000706</td>\n",
       "      <td>-0.001530</td>\n",
       "      <td>5.644215e-04</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.007785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.000807</td>\n",
       "      <td>-0.001525</td>\n",
       "      <td>5.634045e-04</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>-0.000662</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>0.007731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.001106</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>5.002841e-04</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>0.008011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.001109</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>5.027624e-04</td>\n",
       "      <td>-0.001622</td>\n",
       "      <td>-0.000488</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>0.007963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.001233</td>\n",
       "      <td>-0.001474</td>\n",
       "      <td>5.913982e-04</td>\n",
       "      <td>-0.001613</td>\n",
       "      <td>-0.000305</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.001124</td>\n",
       "      <td>0.007758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.001230</td>\n",
       "      <td>-0.001482</td>\n",
       "      <td>5.892115e-04</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>0.007798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.001413</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>6.789351e-04</td>\n",
       "      <td>-0.001476</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>-0.000464</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.007587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.001415</td>\n",
       "      <td>-0.001408</td>\n",
       "      <td>6.799670e-04</td>\n",
       "      <td>-0.001471</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.007565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.001331</td>\n",
       "      <td>7.679081e-04</td>\n",
       "      <td>-0.001395</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.000951</td>\n",
       "      <td>-0.001788</td>\n",
       "      <td>0.007780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.001339</td>\n",
       "      <td>7.660976e-04</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.001787</td>\n",
       "      <td>0.007786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.001781</td>\n",
       "      <td>-0.001350</td>\n",
       "      <td>8.528503e-04</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>-0.001381</td>\n",
       "      <td>-0.001930</td>\n",
       "      <td>0.008140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.001781</td>\n",
       "      <td>-0.001350</td>\n",
       "      <td>8.528503e-04</td>\n",
       "      <td>-0.001394</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>-0.001381</td>\n",
       "      <td>-0.001930</td>\n",
       "      <td>0.008140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0             0          0          0          0  \\\n",
       "0   -0.002349  -0.000611 -5.322974e-05  -0.001132   0.001287  -0.000453   \n",
       "1   -0.002349  -0.000580 -4.089065e-05  -0.001100   0.001281  -0.000442   \n",
       "2   -0.002198  -0.000492  4.525296e-06  -0.001046   0.001263  -0.000440   \n",
       "3   -0.002197  -0.000501 -4.300103e-05  -0.001075   0.001268  -0.000448   \n",
       "4   -0.002052  -0.000492 -5.187932e-05  -0.001054   0.001242  -0.000459   \n",
       "5   -0.002055  -0.000510 -1.704227e-05  -0.001013   0.001232  -0.000440   \n",
       "6   -0.001904  -0.000562 -1.559872e-05  -0.000992   0.001202  -0.000428   \n",
       "7   -0.001921  -0.000624  1.359722e-04  -0.000885   0.001193  -0.000394   \n",
       "8   -0.001781  -0.000720  3.316626e-05  -0.000867   0.001141  -0.000372   \n",
       "9   -0.001783  -0.000745  4.380196e-05  -0.000829   0.001127  -0.000356   \n",
       "10  -0.001570  -0.000736 -1.795962e-05  -0.000867   0.001075  -0.000251   \n",
       "11  -0.001552  -0.000732 -7.804483e-07  -0.000892   0.001063  -0.000223   \n",
       "12  -0.001363  -0.000829  2.159970e-04  -0.000933   0.000997  -0.000057   \n",
       "13  -0.001346  -0.000855  2.551870e-04  -0.000945   0.000965  -0.000044   \n",
       "14  -0.001173  -0.000945  4.477808e-04  -0.001016   0.000801   0.010319   \n",
       "15  -0.001138   0.026840  4.269292e-01   0.579896   1.208835   1.931380   \n",
       "16   0.545917   2.923156  4.276439e+00   6.209582   7.237523  13.840117   \n",
       "17   2.145126   6.491627  8.360603e+00   9.964477  11.853331  18.825411   \n",
       "18   4.374944  12.183352  1.560298e+01  18.717142  21.110786  31.235043   \n",
       "19   5.854895  14.276540  1.781837e+01  20.075821  22.135254  32.518833   \n",
       "20  11.239605  21.590355  2.684600e+01  29.139370  31.640022  40.497013   \n",
       "21  12.736603  22.990986  2.826448e+01  29.901171  33.216015  40.763508   \n",
       "22  14.094769  25.610161  3.147693e+01  35.560806  39.449242  48.078983   \n",
       "23  11.841318  23.580547  2.970623e+01  33.425858  38.103825  46.693314   \n",
       "24  13.024812  24.271233  2.935627e+01  33.446907  36.811199  44.081230   \n",
       "25  14.557906  25.263062  3.020491e+01  34.559525  36.564816  43.959026   \n",
       "26  13.561514  23.310167  2.627851e+01  30.077080  32.658752  40.300842   \n",
       "27  11.277952  21.480835  2.459570e+01  29.271950  32.528629  38.862167   \n",
       "28  11.884785  20.234631  2.113459e+01  21.924711  26.722857  32.588055   \n",
       "29  12.425123  20.989697  2.175147e+01  22.493423  27.621571  33.672691   \n",
       "30  11.088366  17.787445  1.594661e+01  17.205816  21.964863  26.791088   \n",
       "31   9.314135  15.365369  1.325009e+01  14.225792  17.608921  21.842791   \n",
       "32   5.630906   7.987370  6.276731e+00   7.385617   8.798072  10.998857   \n",
       "33   1.522015   1.900805  1.296174e+00   1.844411   2.720329   4.209159   \n",
       "34   0.037034   0.010962  8.842414e-04  -0.001576   0.001716   0.054169   \n",
       "35   0.034528   0.008469  8.832077e-04  -0.001561   0.001289   0.052865   \n",
       "36  -0.000706  -0.001530  5.644215e-04  -0.001596  -0.000671   0.000490   \n",
       "37  -0.000807  -0.001525  5.634045e-04  -0.001588  -0.000662   0.000485   \n",
       "38  -0.001106  -0.001534  5.002841e-04  -0.001625  -0.000498   0.000347   \n",
       "39  -0.001109  -0.001527  5.027624e-04  -0.001622  -0.000488   0.000341   \n",
       "40  -0.001233  -0.001474  5.913982e-04  -0.001613  -0.000305   0.000209   \n",
       "41  -0.001230  -0.001482  5.892115e-04  -0.001621  -0.000315   0.000215   \n",
       "42  -0.001413  -0.001412  6.789351e-04  -0.001476  -0.000123   0.000087   \n",
       "43  -0.001415  -0.001408  6.799670e-04  -0.001471  -0.000118   0.000084   \n",
       "44  -0.001596  -0.001331  7.679081e-04  -0.001395   0.000071  -0.000094   \n",
       "45  -0.001592  -0.001339  7.660976e-04  -0.001400   0.000061  -0.000081   \n",
       "46  -0.001781  -0.001350  8.528503e-04  -0.001394   0.000217  -0.000257   \n",
       "47  -0.001781  -0.001350  8.528503e-04  -0.001394   0.000217  -0.000257   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.000094  -0.000818   0.005992  \n",
       "1    0.000066  -0.000804   0.005994  \n",
       "2    0.000095  -0.000725   0.005838  \n",
       "3    0.000118  -0.000736   0.005836  \n",
       "4    0.000053  -0.000646   0.005708  \n",
       "5    0.000047  -0.000680   0.005774  \n",
       "6   -0.000017  -0.000629   0.005673  \n",
       "7   -0.000101  -0.000778   0.005802  \n",
       "8   -0.000097  -0.000742   0.005954  \n",
       "9   -0.000067  -0.000820   0.006219  \n",
       "10  -0.000037  -0.000749   0.006429  \n",
       "11  -0.000041  -0.000781   0.006506  \n",
       "12   0.000063  -0.000552   0.005964  \n",
       "13   0.000060  -0.000556   0.006066  \n",
       "14   0.000025  -0.000330   0.007699  \n",
       "15   2.264489   3.505924   6.697193  \n",
       "16  11.925848  14.819144  18.424410  \n",
       "17  15.293234  18.488459  22.968403  \n",
       "18  25.608833  28.988262  35.066109  \n",
       "19  27.164389  29.940098  34.373230  \n",
       "20  34.992290  34.357838  35.040707  \n",
       "21  37.388153  37.496067  38.646065  \n",
       "22  43.581604  45.140259  46.526817  \n",
       "23  41.665810  42.564568  46.725201  \n",
       "24  40.477814  41.763020  44.571140  \n",
       "25  40.759903  41.848156  40.766285  \n",
       "26  38.116993  38.818661  38.841042  \n",
       "27  38.129791  39.263615  44.214870  \n",
       "28  32.295059  33.063667  33.163689  \n",
       "29  32.862137  33.320732  32.327194  \n",
       "30  25.845657  26.645065  26.425034  \n",
       "31  21.973663  23.257534  23.031939  \n",
       "32  10.916775  13.166090  12.856496  \n",
       "33   5.963072   8.370705   8.801943  \n",
       "34   0.010421   0.052519   0.007258  \n",
       "35   0.009361   0.051644   0.007157  \n",
       "36   0.000167  -0.000405   0.007785  \n",
       "37   0.000169  -0.000404   0.007731  \n",
       "38   0.000141  -0.000764   0.008011  \n",
       "39   0.000144  -0.000764   0.007963  \n",
       "40  -0.000176  -0.001124   0.007758  \n",
       "41  -0.000174  -0.001123   0.007798  \n",
       "42  -0.000464  -0.001458   0.007587  \n",
       "43  -0.000463  -0.001458   0.007565  \n",
       "44  -0.000951  -0.001788   0.007780  \n",
       "45  -0.000945  -0.001787   0.007786  \n",
       "46  -0.001381  -0.001930   0.008140  \n",
       "47  -0.001381  -0.001930   0.008140  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model7.fit(np.array(Day).reshape(52464, 1, 5), np.array(Day7).reshape(52464, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred7 = np.squeeze(model7.predict(np.array(df_test).reshape(3888, 1, 5)))\n",
    "    pred7 = pd.DataFrame(pred7)\n",
    "    result7 = pd.concat([result7, pred7], axis=1)\n",
    "    \n",
    "result7[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.5378 - val_loss: 1.6347\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4440 - val_loss: 1.6298\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4246 - val_loss: 1.6211\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4092 - val_loss: 1.6122\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4206 - val_loss: 1.6020\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4106 - val_loss: 1.5911\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4145 - val_loss: 1.5921\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4157 - val_loss: 1.5877\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4164 - val_loss: 1.5759\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4156 - val_loss: 1.5668\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3859 - val_loss: 1.5689\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3884 - val_loss: 1.5618\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3981 - val_loss: 1.5751\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3771 - val_loss: 1.5692\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4051 - val_loss: 1.5686\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3910 - val_loss: 1.5615\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3814 - val_loss: 1.5686\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3945 - val_loss: 1.5664\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3781 - val_loss: 1.5705\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3882 - val_loss: 1.5610\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3862 - val_loss: 1.5634\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3759 - val_loss: 1.5572\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3719 - val_loss: 1.5531\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3720 - val_loss: 1.5480\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3771 - val_loss: 1.5638\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3786 - val_loss: 1.5561\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3787 - val_loss: 1.5499\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3737 - val_loss: 1.5791\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3680 - val_loss: 1.5603\n",
      "Epoch 00029: early stopping\n",
      "0.2\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2808 - val_loss: 2.5798\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2635 - val_loss: 2.5642\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2510 - val_loss: 2.5527\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2385 - val_loss: 2.5480\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2541 - val_loss: 2.5603\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2455 - val_loss: 2.5474\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2665 - val_loss: 2.5534\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2626 - val_loss: 2.5770\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 2.2649 - val_loss: 2.5579\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 2.2744 - val_loss: 2.5603\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 2.2246 - val_loss: 2.5516\n",
      "Epoch 00011: early stopping\n",
      "0.3\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7217 - val_loss: 3.0424\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6954 - val_loss: 3.0666\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7159 - val_loss: 3.0677\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6872 - val_loss: 3.0982\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7129 - val_loss: 3.0953\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6983 - val_loss: 3.0489\n",
      "Epoch 00006: early stopping\n",
      "0.4\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 2.8475 - val_loss: 3.2036\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8184 - val_loss: 3.2083\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8360 - val_loss: 3.2260\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8243 - val_loss: 3.2227\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8397 - val_loss: 3.2620\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8118 - val_loss: 3.1845\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8465 - val_loss: 3.2167\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8496 - val_loss: 3.2780\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8206 - val_loss: 3.2304\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8518 - val_loss: 3.2319\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7875 - val_loss: 3.2030\n",
      "Epoch 00011: early stopping\n",
      "0.5\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7103 - val_loss: 3.1265\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6993 - val_loss: 3.0932\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7024 - val_loss: 3.0814\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7079 - val_loss: 3.1219\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7165 - val_loss: 3.1404\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6709 - val_loss: 3.0698\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7231 - val_loss: 3.0784\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7259 - val_loss: 3.1288\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6898 - val_loss: 3.0683\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7235 - val_loss: 3.0736\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6678 - val_loss: 3.0584\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6813 - val_loss: 3.0780\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6938 - val_loss: 3.1280\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6893 - val_loss: 3.0746\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6977 - val_loss: 3.0788\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.6673 - val_loss: 3.1310\n",
      "Epoch 00016: early stopping\n",
      "0.6\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 2.3912 - val_loss: 2.7842\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3931 - val_loss: 2.7693\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3913 - val_loss: 2.7783\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4027 - val_loss: 2.8250\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4044 - val_loss: 2.7689\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3586 - val_loss: 2.7584\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4080 - val_loss: 2.7531\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4090 - val_loss: 2.7524\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3701 - val_loss: 2.7409\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4072 - val_loss: 2.7400\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3565 - val_loss: 2.7355\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3773 - val_loss: 2.7603\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3850 - val_loss: 2.7672\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3814 - val_loss: 2.7338\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3855 - val_loss: 2.7301\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3650 - val_loss: 2.7790\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4100 - val_loss: 2.8271\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3802 - val_loss: 2.7344\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3798 - val_loss: 2.7528\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3590 - val_loss: 2.7406\n",
      "Epoch 00020: early stopping\n",
      "0.7\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.9581 - val_loss: 2.2765\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9531 - val_loss: 2.2911\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9479 - val_loss: 2.2612\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9507 - val_loss: 2.2622\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9579 - val_loss: 2.2417\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9178 - val_loss: 2.2459\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9603 - val_loss: 2.2510\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9588 - val_loss: 2.2709\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9257 - val_loss: 2.2366\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9586 - val_loss: 2.2333\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9120 - val_loss: 2.2394\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9392 - val_loss: 2.2615\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9405 - val_loss: 2.2847\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9441 - val_loss: 2.2587\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9415 - val_loss: 2.2313\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9240 - val_loss: 2.2538\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9568 - val_loss: 2.3028\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9275 - val_loss: 2.2571\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9370 - val_loss: 2.2429\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9216 - val_loss: 2.2422\n",
      "Epoch 00020: early stopping\n",
      "0.8\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4215 - val_loss: 1.6384\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4115 - val_loss: 1.6564\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4097 - val_loss: 1.6394\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4077 - val_loss: 1.6472\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4126 - val_loss: 1.6426\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3857 - val_loss: 1.6359\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4200 - val_loss: 1.6257\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4187 - val_loss: 1.6462\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3953 - val_loss: 1.6145\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4162 - val_loss: 1.6335\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3815 - val_loss: 1.6105\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4019 - val_loss: 1.6611\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4050 - val_loss: 1.6240\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4071 - val_loss: 1.6168\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4054 - val_loss: 1.6123\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3893 - val_loss: 1.6415\n",
      "Epoch 00016: early stopping\n",
      "0.9\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 3s 2ms/step - loss: 0.7952 - val_loss: 0.9022\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7849 - val_loss: 0.9139\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7821 - val_loss: 0.9192\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7843 - val_loss: 0.9102\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7791 - val_loss: 0.9038\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7680 - val_loss: 0.8983\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7894 - val_loss: 0.9216\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7849 - val_loss: 0.9146\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7710 - val_loss: 0.9251\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7844 - val_loss: 0.9062\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7682 - val_loss: 0.8892\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7757 - val_loss: 0.9016\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7772 - val_loss: 0.9200\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7802 - val_loss: 0.8990\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7767 - val_loss: 0.8965\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7708 - val_loss: 0.9040\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000396</td>\n",
       "      <td>-0.001752</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>-0.001709</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.003973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000398</td>\n",
       "      <td>-0.001773</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.003927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000269</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>-0.001559</td>\n",
       "      <td>-0.000366</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.004189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.001626</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.001559</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000142</td>\n",
       "      <td>-0.001491</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>-0.001409</td>\n",
       "      <td>-0.000623</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.004110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.001523</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.001409</td>\n",
       "      <td>-0.000570</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.004115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.000742</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.003943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.001517</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.001455</td>\n",
       "      <td>-0.000634</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.003835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000074</td>\n",
       "      <td>-0.001515</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.001498</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.003537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000083</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.001516</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.003116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000213</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.002102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000216</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.001131</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.002044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000339</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>-0.001243</td>\n",
       "      <td>-0.002276</td>\n",
       "      <td>-0.001056</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.001555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000343</td>\n",
       "      <td>-0.000799</td>\n",
       "      <td>-0.003216</td>\n",
       "      <td>-0.001754</td>\n",
       "      <td>-0.001161</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.001626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004011</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>-0.005842</td>\n",
       "      <td>-0.002838</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.004484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.017486</td>\n",
       "      <td>0.222686</td>\n",
       "      <td>1.002958</td>\n",
       "      <td>1.237214</td>\n",
       "      <td>1.292169</td>\n",
       "      <td>2.190021</td>\n",
       "      <td>2.943276</td>\n",
       "      <td>6.466844</td>\n",
       "      <td>10.525698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.691128</td>\n",
       "      <td>6.586730</td>\n",
       "      <td>9.901240</td>\n",
       "      <td>12.407723</td>\n",
       "      <td>9.092883</td>\n",
       "      <td>9.992775</td>\n",
       "      <td>10.903507</td>\n",
       "      <td>16.167835</td>\n",
       "      <td>18.814754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.540241</td>\n",
       "      <td>10.385428</td>\n",
       "      <td>13.821689</td>\n",
       "      <td>15.545562</td>\n",
       "      <td>12.666628</td>\n",
       "      <td>14.315645</td>\n",
       "      <td>15.625594</td>\n",
       "      <td>21.612095</td>\n",
       "      <td>25.693142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.588242</td>\n",
       "      <td>17.823313</td>\n",
       "      <td>22.400352</td>\n",
       "      <td>23.559832</td>\n",
       "      <td>22.424345</td>\n",
       "      <td>27.518906</td>\n",
       "      <td>28.988129</td>\n",
       "      <td>33.472340</td>\n",
       "      <td>35.581390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12.466933</td>\n",
       "      <td>19.602722</td>\n",
       "      <td>23.999542</td>\n",
       "      <td>24.469389</td>\n",
       "      <td>23.792498</td>\n",
       "      <td>27.176516</td>\n",
       "      <td>28.254473</td>\n",
       "      <td>31.362869</td>\n",
       "      <td>33.813637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18.853132</td>\n",
       "      <td>27.416143</td>\n",
       "      <td>32.754463</td>\n",
       "      <td>32.085995</td>\n",
       "      <td>32.923447</td>\n",
       "      <td>33.329865</td>\n",
       "      <td>31.138269</td>\n",
       "      <td>31.690666</td>\n",
       "      <td>31.316311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.744320</td>\n",
       "      <td>29.228422</td>\n",
       "      <td>34.647804</td>\n",
       "      <td>34.704678</td>\n",
       "      <td>36.452049</td>\n",
       "      <td>37.183338</td>\n",
       "      <td>35.182701</td>\n",
       "      <td>35.765186</td>\n",
       "      <td>36.153027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21.157759</td>\n",
       "      <td>31.924446</td>\n",
       "      <td>39.371681</td>\n",
       "      <td>39.460178</td>\n",
       "      <td>40.083176</td>\n",
       "      <td>40.096821</td>\n",
       "      <td>39.935478</td>\n",
       "      <td>40.717480</td>\n",
       "      <td>39.805191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19.945671</td>\n",
       "      <td>29.420679</td>\n",
       "      <td>36.505241</td>\n",
       "      <td>36.582367</td>\n",
       "      <td>37.131710</td>\n",
       "      <td>38.006077</td>\n",
       "      <td>37.175186</td>\n",
       "      <td>38.741764</td>\n",
       "      <td>39.146278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20.917639</td>\n",
       "      <td>30.232450</td>\n",
       "      <td>37.489964</td>\n",
       "      <td>37.085365</td>\n",
       "      <td>37.631706</td>\n",
       "      <td>37.507427</td>\n",
       "      <td>36.675224</td>\n",
       "      <td>37.144321</td>\n",
       "      <td>36.546013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.732500</td>\n",
       "      <td>31.197580</td>\n",
       "      <td>38.415173</td>\n",
       "      <td>38.250229</td>\n",
       "      <td>38.764393</td>\n",
       "      <td>38.353283</td>\n",
       "      <td>37.521572</td>\n",
       "      <td>37.369610</td>\n",
       "      <td>36.999363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20.399637</td>\n",
       "      <td>27.915018</td>\n",
       "      <td>34.611034</td>\n",
       "      <td>34.697094</td>\n",
       "      <td>36.229294</td>\n",
       "      <td>36.442986</td>\n",
       "      <td>35.249226</td>\n",
       "      <td>35.462124</td>\n",
       "      <td>34.864410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20.093973</td>\n",
       "      <td>27.053692</td>\n",
       "      <td>33.820068</td>\n",
       "      <td>34.745510</td>\n",
       "      <td>36.994461</td>\n",
       "      <td>37.764042</td>\n",
       "      <td>37.734264</td>\n",
       "      <td>39.467297</td>\n",
       "      <td>41.990086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18.377745</td>\n",
       "      <td>22.435568</td>\n",
       "      <td>26.555042</td>\n",
       "      <td>29.483503</td>\n",
       "      <td>30.820349</td>\n",
       "      <td>31.693523</td>\n",
       "      <td>30.535620</td>\n",
       "      <td>30.584875</td>\n",
       "      <td>29.616419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18.525694</td>\n",
       "      <td>22.895683</td>\n",
       "      <td>27.045052</td>\n",
       "      <td>30.039558</td>\n",
       "      <td>31.452610</td>\n",
       "      <td>31.946228</td>\n",
       "      <td>29.986832</td>\n",
       "      <td>30.030128</td>\n",
       "      <td>29.792023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>15.672346</td>\n",
       "      <td>16.768629</td>\n",
       "      <td>20.089159</td>\n",
       "      <td>24.198406</td>\n",
       "      <td>25.418053</td>\n",
       "      <td>25.529333</td>\n",
       "      <td>24.048344</td>\n",
       "      <td>25.379509</td>\n",
       "      <td>25.327379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14.764792</td>\n",
       "      <td>15.435010</td>\n",
       "      <td>17.341015</td>\n",
       "      <td>21.095791</td>\n",
       "      <td>22.376963</td>\n",
       "      <td>21.410133</td>\n",
       "      <td>21.126627</td>\n",
       "      <td>21.260010</td>\n",
       "      <td>20.395288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8.258614</td>\n",
       "      <td>7.750056</td>\n",
       "      <td>8.947626</td>\n",
       "      <td>10.947015</td>\n",
       "      <td>10.676216</td>\n",
       "      <td>10.350157</td>\n",
       "      <td>11.645658</td>\n",
       "      <td>12.217465</td>\n",
       "      <td>12.438930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.570043</td>\n",
       "      <td>2.369405</td>\n",
       "      <td>2.285835</td>\n",
       "      <td>2.891042</td>\n",
       "      <td>3.186112</td>\n",
       "      <td>5.606585</td>\n",
       "      <td>7.594111</td>\n",
       "      <td>7.828644</td>\n",
       "      <td>6.413304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.042673</td>\n",
       "      <td>-0.000757</td>\n",
       "      <td>-0.001928</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.004048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.000190</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.039497</td>\n",
       "      <td>-0.000940</td>\n",
       "      <td>-0.001875</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.004120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000444</td>\n",
       "      <td>-0.000263</td>\n",
       "      <td>-0.001721</td>\n",
       "      <td>-0.001591</td>\n",
       "      <td>-0.002982</td>\n",
       "      <td>-0.000575</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.004079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>-0.001624</td>\n",
       "      <td>-0.002953</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.004086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.001394</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>-0.002146</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>-0.001045</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.002950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.001407</td>\n",
       "      <td>-0.000428</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>-0.002214</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>-0.000940</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.002950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001777</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>-0.000877</td>\n",
       "      <td>-0.001275</td>\n",
       "      <td>-0.003406</td>\n",
       "      <td>-0.001452</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.001633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.001784</td>\n",
       "      <td>-0.000629</td>\n",
       "      <td>-0.000913</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>-0.003512</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.001619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.001692</td>\n",
       "      <td>-0.000841</td>\n",
       "      <td>-0.000445</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>-0.003729</td>\n",
       "      <td>-0.001280</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.001688</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>-0.000567</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.001595</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.001586</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>-0.000542</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.001605</td>\n",
       "      <td>-0.000679</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.001579</td>\n",
       "      <td>-0.002619</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>-0.002577</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>-0.002577</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.002929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.000396  -0.001752   0.000256  -0.001709  -0.000065   0.001711   \n",
       "1   -0.000398  -0.001773   0.000254  -0.001708  -0.000039   0.001809   \n",
       "2   -0.000269  -0.001641   0.000164  -0.001559  -0.000366   0.001501   \n",
       "3   -0.000268  -0.001626   0.000163  -0.001559  -0.000385   0.001375   \n",
       "4   -0.000142  -0.001491   0.000112  -0.001409  -0.000623   0.001391   \n",
       "5   -0.000145  -0.001523   0.000115  -0.001409  -0.000570   0.001585   \n",
       "6   -0.000021  -0.001444   0.000066  -0.001333  -0.000742   0.001488   \n",
       "7   -0.000028  -0.001517   0.000069  -0.001455  -0.000634   0.001444   \n",
       "8    0.000074  -0.001515   0.000018  -0.001498  -0.000749   0.001085   \n",
       "9    0.000083  -0.001446   0.000069  -0.001516  -0.000791   0.000823   \n",
       "10   0.000213  -0.001151   0.000042  -0.001191  -0.000641   0.001461   \n",
       "11   0.000216  -0.001091   0.000050  -0.001131  -0.000601   0.001496   \n",
       "12   0.000339  -0.000861  -0.001243  -0.002276  -0.001056   0.001383   \n",
       "13   0.000343  -0.000799  -0.003216  -0.001754  -0.001161   0.001428   \n",
       "14   0.004011  -0.003799   0.013782  -0.005842  -0.002838   0.001573   \n",
       "15   0.017486   0.222686   1.002958   1.237214   1.292169   2.190021   \n",
       "16   2.691128   6.586730   9.901240  12.407723   9.092883   9.992775   \n",
       "17   5.540241  10.385428  13.821689  15.545562  12.666628  14.315645   \n",
       "18  10.588242  17.823313  22.400352  23.559832  22.424345  27.518906   \n",
       "19  12.466933  19.602722  23.999542  24.469389  23.792498  27.176516   \n",
       "20  18.853132  27.416143  32.754463  32.085995  32.923447  33.329865   \n",
       "21  19.744320  29.228422  34.647804  34.704678  36.452049  37.183338   \n",
       "22  21.157759  31.924446  39.371681  39.460178  40.083176  40.096821   \n",
       "23  19.945671  29.420679  36.505241  36.582367  37.131710  38.006077   \n",
       "24  20.917639  30.232450  37.489964  37.085365  37.631706  37.507427   \n",
       "25  21.732500  31.197580  38.415173  38.250229  38.764393  38.353283   \n",
       "26  20.399637  27.915018  34.611034  34.697094  36.229294  36.442986   \n",
       "27  20.093973  27.053692  33.820068  34.745510  36.994461  37.764042   \n",
       "28  18.377745  22.435568  26.555042  29.483503  30.820349  31.693523   \n",
       "29  18.525694  22.895683  27.045052  30.039558  31.452610  31.946228   \n",
       "30  15.672346  16.768629  20.089159  24.198406  25.418053  25.529333   \n",
       "31  14.764792  15.435010  17.341015  21.095791  22.376963  21.410133   \n",
       "32   8.258614   7.750056   8.947626  10.947015  10.676216  10.350157   \n",
       "33   2.570043   2.369405   2.285835   2.891042   3.186112   5.606585   \n",
       "34  -0.000142   0.006080   0.042673  -0.000757  -0.001928  -0.000374   \n",
       "35  -0.000190   0.005573   0.039497  -0.000940  -0.001875  -0.000244   \n",
       "36   0.000444  -0.000263  -0.001721  -0.001591  -0.002982  -0.000575   \n",
       "37   0.000505  -0.000267  -0.001514  -0.001624  -0.002953  -0.000490   \n",
       "38   0.001394  -0.000420  -0.000411  -0.002146  -0.003742  -0.001045   \n",
       "39   0.001407  -0.000428  -0.000547  -0.002214  -0.003799  -0.000940   \n",
       "40   0.001777  -0.000636  -0.000877  -0.001275  -0.003406  -0.001452   \n",
       "41   0.001784  -0.000629  -0.000913  -0.001329  -0.003512  -0.001545   \n",
       "42   0.001692  -0.000841  -0.000445  -0.000563  -0.003729  -0.001280   \n",
       "43   0.001688  -0.000844  -0.000425  -0.000567  -0.003691  -0.001265   \n",
       "44   0.001595  -0.000684   0.000014  -0.001586  -0.002582  -0.000542   \n",
       "45   0.001605  -0.000679  -0.000022  -0.001579  -0.002619  -0.000568   \n",
       "46   0.001534  -0.000671  -0.000272  -0.002577  -0.001166   0.002689   \n",
       "47   0.001534  -0.000671  -0.000272  -0.002577  -0.001166   0.002689   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.001545   0.000740   0.003973  \n",
       "1    0.001610   0.000740   0.003927  \n",
       "2    0.001440   0.000736   0.004189  \n",
       "3    0.001374   0.000709   0.004200  \n",
       "4    0.000862   0.000731   0.004110  \n",
       "5    0.001127   0.000807   0.004115  \n",
       "6    0.000983   0.000824   0.003943  \n",
       "7    0.001430   0.000915   0.003835  \n",
       "8    0.001843   0.000844   0.003537  \n",
       "9    0.001793   0.000754   0.003116  \n",
       "10   0.001838   0.000713   0.002102  \n",
       "11   0.001835   0.000814   0.002044  \n",
       "12   0.001732   0.000735   0.001555  \n",
       "13   0.001729   0.000818   0.001626  \n",
       "14   0.001337   0.000226   0.004484  \n",
       "15   2.943276   6.466844  10.525698  \n",
       "16  10.903507  16.167835  18.814754  \n",
       "17  15.625594  21.612095  25.693142  \n",
       "18  28.988129  33.472340  35.581390  \n",
       "19  28.254473  31.362869  33.813637  \n",
       "20  31.138269  31.690666  31.316311  \n",
       "21  35.182701  35.765186  36.153027  \n",
       "22  39.935478  40.717480  39.805191  \n",
       "23  37.175186  38.741764  39.146278  \n",
       "24  36.675224  37.144321  36.546013  \n",
       "25  37.521572  37.369610  36.999363  \n",
       "26  35.249226  35.462124  34.864410  \n",
       "27  37.734264  39.467297  41.990086  \n",
       "28  30.535620  30.584875  29.616419  \n",
       "29  29.986832  30.030128  29.792023  \n",
       "30  24.048344  25.379509  25.327379  \n",
       "31  21.126627  21.260010  20.395288  \n",
       "32  11.645658  12.217465  12.438930  \n",
       "33   7.594111   7.828644   6.413304  \n",
       "34   0.002443   0.001660   0.004048  \n",
       "35   0.002438   0.001636   0.004120  \n",
       "36   0.001747   0.001549   0.004079  \n",
       "37   0.001759   0.001536   0.004086  \n",
       "38   0.001533   0.001411   0.002950  \n",
       "39   0.001538   0.001394   0.002950  \n",
       "40   0.001844   0.000930   0.001633  \n",
       "41   0.001841   0.000923   0.001619  \n",
       "42   0.002176   0.000568   0.002929  \n",
       "43   0.002177   0.000575   0.002929  \n",
       "44   0.002512   0.000302   0.002929  \n",
       "45   0.002509   0.000302   0.002929  \n",
       "46   0.003031   0.000302   0.002929  \n",
       "47   0.003031   0.000302   0.002929  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8 = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model8.fit(np.array(Day).reshape(52464, 1, 5), np.array(Day8).reshape(52464, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred8 = np.squeeze(model8.predict(np.array(df_test).reshape(3888, 1, 5)))\n",
    "    pred8 = pd.DataFrame(pred8)\n",
    "    result8 = pd.concat([result8, pred8], axis=1)\n",
    "    \n",
    "result8[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 39348, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 39348, 7), dtype=tf.float32, name='gru_8_input'), name='gru_8_input', description=\"created by layer 'gru_8_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 39348, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 39348, 7), dtype=tf.float32, name='gru_8_input'), name='gru_8_input', description=\"created by layer 'gru_8_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "607/615 [============================>.] - ETA: 0s - loss: 704.4749WARNING:tensorflow:Model was constructed with shape (None, 39348, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 39348, 7), dtype=tf.float32, name='gru_8_input'), name='gru_8_input', description=\"created by layer 'gru_8_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "615/615 [==============================] - 4s 4ms/step - loss: 702.2539 - val_loss: 363.4023\n",
      "Epoch 2/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 303.1173 - val_loss: 228.7125\n",
      "Epoch 3/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 207.5436 - val_loss: 184.3359\n",
      "Epoch 4/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 179.8817 - val_loss: 165.4278\n",
      "Epoch 5/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 161.5876 - val_loss: 157.0628\n",
      "Epoch 6/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 155.0740 - val_loss: 152.5167\n",
      "Epoch 7/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 153.6500 - val_loss: 151.7548\n",
      "Epoch 8/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 146.0424 - val_loss: 148.0220\n",
      "Epoch 9/100\n",
      "615/615 [==============================] - 2s 4ms/step - loss: 149.6004 - val_loss: 146.1684\n",
      "Epoch 10/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 146.7912 - val_loss: 146.7528\n",
      "Epoch 11/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 152.2324 - val_loss: 142.6069\n",
      "Epoch 12/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 145.2449 - val_loss: 140.1284\n",
      "Epoch 13/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 143.5675 - val_loss: 138.9807\n",
      "Epoch 14/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 141.0147 - val_loss: 137.6361\n",
      "Epoch 15/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 135.7258 - val_loss: 135.8025\n",
      "Epoch 16/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 139.8642 - val_loss: 135.2384\n",
      "Epoch 17/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 138.3552 - val_loss: 134.8135\n",
      "Epoch 18/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 134.4569 - val_loss: 134.3355\n",
      "Epoch 19/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 133.3664 - val_loss: 132.0682\n",
      "Epoch 20/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 129.3797 - val_loss: 133.4209\n",
      "Epoch 21/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 133.1058 - val_loss: 131.6034\n",
      "Epoch 22/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 130.8681 - val_loss: 131.1691\n",
      "Epoch 23/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 131.1475 - val_loss: 133.7024\n",
      "Epoch 24/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 131.3954 - val_loss: 128.5638\n",
      "Epoch 25/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 128.9703 - val_loss: 128.7501\n",
      "Epoch 26/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 131.2387 - val_loss: 127.3997\n",
      "Epoch 27/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 132.2509 - val_loss: 126.8751\n",
      "Epoch 28/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 124.3125 - val_loss: 126.8339\n",
      "Epoch 29/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 129.5253 - val_loss: 127.5163\n",
      "Epoch 30/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 128.3112 - val_loss: 126.5504\n",
      "Epoch 31/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 126.3916 - val_loss: 126.3516\n",
      "Epoch 32/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 126.9018 - val_loss: 127.6583\n",
      "Epoch 33/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 132.0531 - val_loss: 126.3957\n",
      "Epoch 34/100\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 127.2364 - val_loss: 127.8944\n",
      "Epoch 00034: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected axis -1 of input shape to have value 5 but received input with shape (None, 1, 7)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-7b59bfdd7eee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m model.fit(np.array(X_train_1).reshape(39348, 1, 7), np.array(Y_train_1).reshape(39348, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n\u001b[0;32m      9\u001b[0m             callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m13116\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m-> 2941\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[1;32m-> 3358\u001b[1;33m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 3280\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\okso6\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected axis -1 of input shape to have value 5 but received input with shape (None, 1, 7)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.GRU(units=64, return_sequences=True, input_shape=[39348, 7]),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(np.array(X_train_1).reshape(39348, 1, 7), np.array(Y_train_1).reshape(39348, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "            callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "pred = np.squeeze(model8.predict(np.array(X_valid_1).reshape(13116, 1, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 1.5721WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 5s 3ms/step - loss: 1.5716 - val_loss: 1.6340\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4335 - val_loss: 1.6106\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4189 - val_loss: 1.6028\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.4084 - val_loss: 1.5895\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4090 - val_loss: 1.5825\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3946 - val_loss: 1.5906\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3963 - val_loss: 1.5829\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4096 - val_loss: 1.5721\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3922 - val_loss: 1.5765\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.4108 - val_loss: 1.5779\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3784 - val_loss: 1.5658\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3728 - val_loss: 1.5715\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3820 - val_loss: 1.5661\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3674 - val_loss: 1.5687\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3981 - val_loss: 1.5661\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3874 - val_loss: 1.5656\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3859 - val_loss: 1.5749\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3774 - val_loss: 1.5683\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3778 - val_loss: 1.5703\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3805 - val_loss: 1.5722\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3864 - val_loss: 1.5905\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "817/820 [============================>.] - ETA: 0s - loss: 2.2430WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.2430 - val_loss: 2.5497\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2275 - val_loss: 2.5430\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2156 - val_loss: 2.5374\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1991 - val_loss: 2.5327\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2187 - val_loss: 2.5284\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2032 - val_loss: 2.5459\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2153 - val_loss: 2.5237\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2388 - val_loss: 2.5223\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2071 - val_loss: 2.5251\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2414 - val_loss: 2.5232\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1659 - val_loss: 2.5264\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1642 - val_loss: 2.5179\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2010 - val_loss: 2.5159\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1930 - val_loss: 2.5171\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2032 - val_loss: 2.5240\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1959 - val_loss: 2.5151\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2033 - val_loss: 2.5250\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1905 - val_loss: 2.5136\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1954 - val_loss: 2.5202\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2135 - val_loss: 2.5113\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2212 - val_loss: 2.5175\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1776 - val_loss: 2.5152\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1839 - val_loss: 2.5128\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2048 - val_loss: 2.5161\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.1711 - val_loss: 2.5138\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "808/820 [============================>.] - ETA: 0s - loss: 2.6289WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6290 - val_loss: 2.9850\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6030 - val_loss: 2.9764\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6041 - val_loss: 2.9705\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5737 - val_loss: 2.9727\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6136 - val_loss: 2.9682\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6072 - val_loss: 2.9951\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6201 - val_loss: 2.9628\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6437 - val_loss: 2.9642\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5943 - val_loss: 2.9549\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6463 - val_loss: 2.9621\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5430 - val_loss: 2.9644\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5566 - val_loss: 2.9676\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5994 - val_loss: 2.9536\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5952 - val_loss: 2.9726\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5891 - val_loss: 2.9886\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5867 - val_loss: 2.9582\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5955 - val_loss: 2.9754\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.5957 - val_loss: 2.9641\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "803/820 [============================>.] - ETA: 0s - loss: 2.7448WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.7449 - val_loss: 3.1089\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7201 - val_loss: 3.0908\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7149 - val_loss: 3.1008\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6922 - val_loss: 3.0924\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7292 - val_loss: 3.1026\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7322 - val_loss: 3.0939\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7433 - val_loss: 3.0840\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7766 - val_loss: 3.0792\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7155 - val_loss: 3.0912\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7830 - val_loss: 3.0831\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6651 - val_loss: 3.0713\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6734 - val_loss: 3.0979\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7186 - val_loss: 3.0970\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7181 - val_loss: 3.0694\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7039 - val_loss: 3.0829\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.6995 - val_loss: 3.0767\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.7131 - val_loss: 3.1430\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7264 - val_loss: 3.0787\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7200 - val_loss: 3.0632\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7305 - val_loss: 3.0708\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7733 - val_loss: 3.0725\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7094 - val_loss: 3.0756\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7075 - val_loss: 3.0713\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7247 - val_loss: 3.0802\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "814/820 [============================>.] - ETA: 0s - loss: 2.6374WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6374 - val_loss: 2.9624\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6049 - val_loss: 2.9594\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6047 - val_loss: 2.9652\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5876 - val_loss: 2.9600\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6135 - val_loss: 2.9673\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6247 - val_loss: 2.9534\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6293 - val_loss: 2.9450\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6642 - val_loss: 2.9308\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6002 - val_loss: 2.9231\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6597 - val_loss: 2.9261\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5480 - val_loss: 2.9384\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5595 - val_loss: 2.9290\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5949 - val_loss: 2.9388\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5888 - val_loss: 2.9368\n",
      "Epoch 00014: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "818/820 [============================>.] - ETA: 0s - loss: 2.3702WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.3702 - val_loss: 2.6541\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3352 - val_loss: 2.6476\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3317 - val_loss: 2.6360\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3217 - val_loss: 2.6633\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3355 - val_loss: 2.6596\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3451 - val_loss: 2.6344\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3392 - val_loss: 2.6141\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3714 - val_loss: 2.6154\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3026 - val_loss: 2.6186\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.3552 - val_loss: 2.6143\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2578 - val_loss: 2.6225\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2713 - val_loss: 2.6100\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2981 - val_loss: 2.6259\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2834 - val_loss: 2.5996\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2762 - val_loss: 2.6058\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.2676 - val_loss: 2.5999\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2998 - val_loss: 2.5970\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2957 - val_loss: 2.5897\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2800 - val_loss: 2.6007\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3078 - val_loss: 2.6019\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3278 - val_loss: 2.5892\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2688 - val_loss: 2.5998\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2785 - val_loss: 2.6051\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2866 - val_loss: 2.5883\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2456 - val_loss: 2.5871\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2545 - val_loss: 2.6017\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2949 - val_loss: 2.5887\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2642 - val_loss: 2.5946\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2493 - val_loss: 2.5970\n",
      "Epoch 30/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2519 - val_loss: 2.5772\n",
      "Epoch 31/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2595 - val_loss: 2.5860\n",
      "Epoch 32/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2721 - val_loss: 2.6041\n",
      "Epoch 33/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2443 - val_loss: 2.5890\n",
      "Epoch 34/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2349 - val_loss: 2.5987\n",
      "Epoch 35/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2418 - val_loss: 2.5977\n",
      "Epoch 00035: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "814/820 [============================>.] - ETA: 0s - loss: 1.8748WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.8748 - val_loss: 2.1348\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8434 - val_loss: 2.1459\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8584 - val_loss: 2.1536\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8493 - val_loss: 2.1473\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8494 - val_loss: 2.1382\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8614 - val_loss: 2.1359\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "817/820 [============================>.] - ETA: 0s - loss: 1.3754WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 6s 4ms/step - loss: 1.3754 - val_loss: 1.5805\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3488 - val_loss: 1.5820\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3710 - val_loss: 1.5807\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3588 - val_loss: 1.5834\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3609 - val_loss: 1.5714\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3638 - val_loss: 1.5621\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3704 - val_loss: 1.5668\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3826 - val_loss: 1.5616\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3480 - val_loss: 1.5510\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3738 - val_loss: 1.5505\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3204 - val_loss: 1.5586\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3385 - val_loss: 1.5872\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3540 - val_loss: 1.5878\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3446 - val_loss: 1.5921\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 1.3339 - val_loss: 1.5781\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "819/820 [============================>.] - ETA: 0s - loss: 0.7646WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 0.7646 - val_loss: 0.8823\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7520 - val_loss: 0.8899\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7617 - val_loss: 0.8970\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7571 - val_loss: 0.8897\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7544 - val_loss: 0.8908\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7619 - val_loss: 0.8715\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7634 - val_loss: 0.8825\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7702 - val_loss: 0.8730\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7490 - val_loss: 0.8676\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7649 - val_loss: 0.8664\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7353 - val_loss: 0.8739\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7473 - val_loss: 0.8817\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7554 - val_loss: 0.8809\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7496 - val_loss: 0.8736\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 0.7419 - val_loss: 0.8770\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_10_input'), name='gru_10_input', description=\"created by layer 'gru_10_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.075949</td>\n",
       "      <td>-0.011083</td>\n",
       "      <td>-0.016541</td>\n",
       "      <td>0.042628</td>\n",
       "      <td>0.027870</td>\n",
       "      <td>0.071423</td>\n",
       "      <td>0.067487</td>\n",
       "      <td>-0.037570</td>\n",
       "      <td>-0.059470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.076503</td>\n",
       "      <td>-0.010007</td>\n",
       "      <td>-0.014449</td>\n",
       "      <td>0.043075</td>\n",
       "      <td>0.028451</td>\n",
       "      <td>0.069447</td>\n",
       "      <td>0.066699</td>\n",
       "      <td>-0.036306</td>\n",
       "      <td>-0.059932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.070272</td>\n",
       "      <td>-0.006208</td>\n",
       "      <td>-0.014810</td>\n",
       "      <td>0.038965</td>\n",
       "      <td>0.060111</td>\n",
       "      <td>0.053406</td>\n",
       "      <td>0.043162</td>\n",
       "      <td>-0.032847</td>\n",
       "      <td>-0.065703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.069469</td>\n",
       "      <td>-0.006344</td>\n",
       "      <td>-0.016052</td>\n",
       "      <td>0.037848</td>\n",
       "      <td>0.059456</td>\n",
       "      <td>0.054532</td>\n",
       "      <td>0.043316</td>\n",
       "      <td>-0.033691</td>\n",
       "      <td>-0.065018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.063129</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>-0.016780</td>\n",
       "      <td>0.033835</td>\n",
       "      <td>0.072607</td>\n",
       "      <td>0.045861</td>\n",
       "      <td>0.021891</td>\n",
       "      <td>-0.024199</td>\n",
       "      <td>-0.064344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.064535</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>-0.014931</td>\n",
       "      <td>0.035787</td>\n",
       "      <td>0.071453</td>\n",
       "      <td>0.042038</td>\n",
       "      <td>0.020673</td>\n",
       "      <td>-0.021845</td>\n",
       "      <td>-0.064828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.059198</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>-0.015022</td>\n",
       "      <td>0.033721</td>\n",
       "      <td>0.066095</td>\n",
       "      <td>0.035305</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>-0.052623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.063503</td>\n",
       "      <td>-0.003227</td>\n",
       "      <td>-0.018111</td>\n",
       "      <td>0.037491</td>\n",
       "      <td>0.062315</td>\n",
       "      <td>0.030921</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.011485</td>\n",
       "      <td>-0.045988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.061465</td>\n",
       "      <td>-0.005727</td>\n",
       "      <td>-0.026101</td>\n",
       "      <td>0.038126</td>\n",
       "      <td>0.042666</td>\n",
       "      <td>0.027945</td>\n",
       "      <td>-0.011380</td>\n",
       "      <td>0.068812</td>\n",
       "      <td>0.004021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.063422</td>\n",
       "      <td>-0.009617</td>\n",
       "      <td>-0.032237</td>\n",
       "      <td>0.038227</td>\n",
       "      <td>0.038016</td>\n",
       "      <td>0.026615</td>\n",
       "      <td>-0.006409</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.012314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.060889</td>\n",
       "      <td>-0.012403</td>\n",
       "      <td>-0.041833</td>\n",
       "      <td>0.039961</td>\n",
       "      <td>0.024287</td>\n",
       "      <td>0.026250</td>\n",
       "      <td>-0.027262</td>\n",
       "      <td>0.176760</td>\n",
       "      <td>0.100227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.060218</td>\n",
       "      <td>-0.012688</td>\n",
       "      <td>-0.042194</td>\n",
       "      <td>0.039490</td>\n",
       "      <td>0.024255</td>\n",
       "      <td>0.025328</td>\n",
       "      <td>-0.022023</td>\n",
       "      <td>0.181764</td>\n",
       "      <td>0.101153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.058330</td>\n",
       "      <td>-0.014084</td>\n",
       "      <td>-0.047797</td>\n",
       "      <td>0.043548</td>\n",
       "      <td>0.036918</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>-0.042680</td>\n",
       "      <td>0.299701</td>\n",
       "      <td>0.225337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.057740</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>-0.048373</td>\n",
       "      <td>0.043455</td>\n",
       "      <td>0.037798</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>-0.029952</td>\n",
       "      <td>0.311315</td>\n",
       "      <td>0.233721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.058301</td>\n",
       "      <td>-0.017272</td>\n",
       "      <td>-0.055248</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.085521</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>0.399456</td>\n",
       "      <td>0.728253</td>\n",
       "      <td>0.777782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.431710</td>\n",
       "      <td>0.670642</td>\n",
       "      <td>0.695888</td>\n",
       "      <td>1.096229</td>\n",
       "      <td>1.431238</td>\n",
       "      <td>1.099163</td>\n",
       "      <td>1.765496</td>\n",
       "      <td>2.347492</td>\n",
       "      <td>3.400994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.032327</td>\n",
       "      <td>3.274049</td>\n",
       "      <td>3.704694</td>\n",
       "      <td>5.139728</td>\n",
       "      <td>6.670251</td>\n",
       "      <td>6.420875</td>\n",
       "      <td>10.278186</td>\n",
       "      <td>11.965900</td>\n",
       "      <td>17.002937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.698995</td>\n",
       "      <td>6.108618</td>\n",
       "      <td>6.972059</td>\n",
       "      <td>9.153055</td>\n",
       "      <td>11.167306</td>\n",
       "      <td>10.545249</td>\n",
       "      <td>15.058845</td>\n",
       "      <td>17.018358</td>\n",
       "      <td>23.022182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.404993</td>\n",
       "      <td>9.451697</td>\n",
       "      <td>10.938136</td>\n",
       "      <td>14.136795</td>\n",
       "      <td>17.835577</td>\n",
       "      <td>18.739080</td>\n",
       "      <td>27.674852</td>\n",
       "      <td>30.491608</td>\n",
       "      <td>38.536106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.331047</td>\n",
       "      <td>12.912456</td>\n",
       "      <td>14.750125</td>\n",
       "      <td>18.344187</td>\n",
       "      <td>22.081629</td>\n",
       "      <td>22.219400</td>\n",
       "      <td>30.027628</td>\n",
       "      <td>32.220806</td>\n",
       "      <td>39.539894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12.082921</td>\n",
       "      <td>21.278154</td>\n",
       "      <td>23.895386</td>\n",
       "      <td>28.812111</td>\n",
       "      <td>33.266743</td>\n",
       "      <td>33.967110</td>\n",
       "      <td>39.919140</td>\n",
       "      <td>39.649029</td>\n",
       "      <td>43.025505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.856988</td>\n",
       "      <td>24.003920</td>\n",
       "      <td>26.730820</td>\n",
       "      <td>31.625322</td>\n",
       "      <td>35.263020</td>\n",
       "      <td>36.456078</td>\n",
       "      <td>41.386456</td>\n",
       "      <td>40.348499</td>\n",
       "      <td>41.715477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12.915326</td>\n",
       "      <td>23.330498</td>\n",
       "      <td>26.902218</td>\n",
       "      <td>33.234150</td>\n",
       "      <td>39.960140</td>\n",
       "      <td>41.426609</td>\n",
       "      <td>47.899578</td>\n",
       "      <td>48.906429</td>\n",
       "      <td>54.869659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.707197</td>\n",
       "      <td>19.704390</td>\n",
       "      <td>22.790220</td>\n",
       "      <td>28.605406</td>\n",
       "      <td>35.884815</td>\n",
       "      <td>39.034519</td>\n",
       "      <td>46.664230</td>\n",
       "      <td>47.513596</td>\n",
       "      <td>54.997051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.423544</td>\n",
       "      <td>19.560207</td>\n",
       "      <td>22.723646</td>\n",
       "      <td>28.868301</td>\n",
       "      <td>37.091862</td>\n",
       "      <td>41.426685</td>\n",
       "      <td>46.891632</td>\n",
       "      <td>46.490215</td>\n",
       "      <td>53.178963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12.349412</td>\n",
       "      <td>22.615143</td>\n",
       "      <td>25.992271</td>\n",
       "      <td>32.417633</td>\n",
       "      <td>40.075794</td>\n",
       "      <td>41.960548</td>\n",
       "      <td>47.039780</td>\n",
       "      <td>47.080353</td>\n",
       "      <td>51.653759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.377677</td>\n",
       "      <td>19.760950</td>\n",
       "      <td>22.983604</td>\n",
       "      <td>29.360373</td>\n",
       "      <td>37.578064</td>\n",
       "      <td>40.933220</td>\n",
       "      <td>45.127369</td>\n",
       "      <td>43.915695</td>\n",
       "      <td>49.190712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.115631</td>\n",
       "      <td>14.143718</td>\n",
       "      <td>16.883148</td>\n",
       "      <td>22.537451</td>\n",
       "      <td>31.190050</td>\n",
       "      <td>42.073277</td>\n",
       "      <td>47.998390</td>\n",
       "      <td>47.577015</td>\n",
       "      <td>56.211308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.010043</td>\n",
       "      <td>16.006372</td>\n",
       "      <td>18.848068</td>\n",
       "      <td>24.732981</td>\n",
       "      <td>32.728916</td>\n",
       "      <td>37.727089</td>\n",
       "      <td>41.062660</td>\n",
       "      <td>39.765026</td>\n",
       "      <td>44.686806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.745542</td>\n",
       "      <td>17.116560</td>\n",
       "      <td>19.908163</td>\n",
       "      <td>25.900126</td>\n",
       "      <td>33.619946</td>\n",
       "      <td>36.828396</td>\n",
       "      <td>40.055161</td>\n",
       "      <td>38.048893</td>\n",
       "      <td>41.940247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7.873890</td>\n",
       "      <td>15.843387</td>\n",
       "      <td>18.399843</td>\n",
       "      <td>23.898954</td>\n",
       "      <td>30.613064</td>\n",
       "      <td>30.968691</td>\n",
       "      <td>32.552818</td>\n",
       "      <td>31.655125</td>\n",
       "      <td>33.889275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.359801</td>\n",
       "      <td>11.186870</td>\n",
       "      <td>13.392621</td>\n",
       "      <td>18.200813</td>\n",
       "      <td>25.133646</td>\n",
       "      <td>29.300283</td>\n",
       "      <td>30.001242</td>\n",
       "      <td>31.335520</td>\n",
       "      <td>37.509048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.651395</td>\n",
       "      <td>5.862431</td>\n",
       "      <td>7.325083</td>\n",
       "      <td>10.502310</td>\n",
       "      <td>15.610394</td>\n",
       "      <td>17.168150</td>\n",
       "      <td>16.521379</td>\n",
       "      <td>18.074160</td>\n",
       "      <td>23.327524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.607011</td>\n",
       "      <td>1.370581</td>\n",
       "      <td>1.706633</td>\n",
       "      <td>2.666871</td>\n",
       "      <td>4.647159</td>\n",
       "      <td>6.391905</td>\n",
       "      <td>5.914311</td>\n",
       "      <td>8.872417</td>\n",
       "      <td>12.147826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.018686</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>-0.066219</td>\n",
       "      <td>0.048272</td>\n",
       "      <td>0.383702</td>\n",
       "      <td>0.320101</td>\n",
       "      <td>0.248948</td>\n",
       "      <td>0.320272</td>\n",
       "      <td>0.274428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.018591</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>-0.064182</td>\n",
       "      <td>0.046856</td>\n",
       "      <td>0.361231</td>\n",
       "      <td>0.273943</td>\n",
       "      <td>0.214364</td>\n",
       "      <td>0.286921</td>\n",
       "      <td>0.231477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.016708</td>\n",
       "      <td>0.006199</td>\n",
       "      <td>-0.064015</td>\n",
       "      <td>0.038820</td>\n",
       "      <td>0.241165</td>\n",
       "      <td>0.061352</td>\n",
       "      <td>0.051438</td>\n",
       "      <td>0.093741</td>\n",
       "      <td>0.051620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.016704</td>\n",
       "      <td>0.005725</td>\n",
       "      <td>-0.062413</td>\n",
       "      <td>0.038023</td>\n",
       "      <td>0.230339</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>0.047684</td>\n",
       "      <td>0.090578</td>\n",
       "      <td>0.048201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.015256</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>-0.063016</td>\n",
       "      <td>0.029434</td>\n",
       "      <td>0.139539</td>\n",
       "      <td>0.037390</td>\n",
       "      <td>0.027684</td>\n",
       "      <td>0.051646</td>\n",
       "      <td>0.027003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.015307</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>-0.061230</td>\n",
       "      <td>0.028803</td>\n",
       "      <td>0.133845</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>0.027881</td>\n",
       "      <td>0.051082</td>\n",
       "      <td>0.024909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.014210</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>-0.062657</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.076773</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.018587</td>\n",
       "      <td>-0.013091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.014314</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>-0.061074</td>\n",
       "      <td>0.020983</td>\n",
       "      <td>0.077586</td>\n",
       "      <td>0.027102</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>0.019473</td>\n",
       "      <td>-0.012896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.013486</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>-0.063481</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>0.048725</td>\n",
       "      <td>0.017235</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>-0.028403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.013661</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>-0.060726</td>\n",
       "      <td>0.014092</td>\n",
       "      <td>0.052172</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>-0.027395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.013122</td>\n",
       "      <td>-0.002533</td>\n",
       "      <td>-0.062864</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.046887</td>\n",
       "      <td>0.013539</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>-0.021610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.002405</td>\n",
       "      <td>-0.062392</td>\n",
       "      <td>0.009651</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.013374</td>\n",
       "      <td>0.006705</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>-0.021657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.012736</td>\n",
       "      <td>-0.005070</td>\n",
       "      <td>-0.065544</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>0.011944</td>\n",
       "      <td>0.011234</td>\n",
       "      <td>-0.006435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.012736</td>\n",
       "      <td>-0.005070</td>\n",
       "      <td>-0.065544</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.012819</td>\n",
       "      <td>0.011944</td>\n",
       "      <td>0.011234</td>\n",
       "      <td>-0.006435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.075949  -0.011083  -0.016541   0.042628   0.027870   0.071423   \n",
       "1   -0.076503  -0.010007  -0.014449   0.043075   0.028451   0.069447   \n",
       "2   -0.070272  -0.006208  -0.014810   0.038965   0.060111   0.053406   \n",
       "3   -0.069469  -0.006344  -0.016052   0.037848   0.059456   0.054532   \n",
       "4   -0.063129  -0.002772  -0.016780   0.033835   0.072607   0.045861   \n",
       "5   -0.064535  -0.002502  -0.014931   0.035787   0.071453   0.042038   \n",
       "6   -0.059198   0.000969  -0.015022   0.033721   0.066095   0.035305   \n",
       "7   -0.063503  -0.003227  -0.018111   0.037491   0.062315   0.030921   \n",
       "8   -0.061465  -0.005727  -0.026101   0.038126   0.042666   0.027945   \n",
       "9   -0.063422  -0.009617  -0.032237   0.038227   0.038016   0.026615   \n",
       "10  -0.060889  -0.012403  -0.041833   0.039961   0.024287   0.026250   \n",
       "11  -0.060218  -0.012688  -0.042194   0.039490   0.024255   0.025328   \n",
       "12  -0.058330  -0.014084  -0.047797   0.043548   0.036918   0.007760   \n",
       "13  -0.057740  -0.014549  -0.048373   0.043455   0.037798   0.009546   \n",
       "14  -0.058301  -0.017272  -0.055248   0.049773   0.085521   0.014430   \n",
       "15   0.431710   0.670642   0.695888   1.096229   1.431238   1.099163   \n",
       "16   2.032327   3.274049   3.704694   5.139728   6.670251   6.420875   \n",
       "17   3.698995   6.108618   6.972059   9.153055  11.167306  10.545249   \n",
       "18   5.404993   9.451697  10.938136  14.136795  17.835577  18.739080   \n",
       "19   7.331047  12.912456  14.750125  18.344187  22.081629  22.219400   \n",
       "20  12.082921  21.278154  23.895386  28.812111  33.266743  33.967110   \n",
       "21  13.856988  24.003920  26.730820  31.625322  35.263020  36.456078   \n",
       "22  12.915326  23.330498  26.902218  33.234150  39.960140  41.426609   \n",
       "23  10.707197  19.704390  22.790220  28.605406  35.884815  39.034519   \n",
       "24  10.423544  19.560207  22.723646  28.868301  37.091862  41.426685   \n",
       "25  12.349412  22.615143  25.992271  32.417633  40.075794  41.960548   \n",
       "26  10.377677  19.760950  22.983604  29.360373  37.578064  40.933220   \n",
       "27   7.115631  14.143718  16.883148  22.537451  31.190050  42.073277   \n",
       "28   8.010043  16.006372  18.848068  24.732981  32.728916  37.727089   \n",
       "29   8.745542  17.116560  19.908163  25.900126  33.619946  36.828396   \n",
       "30   7.873890  15.843387  18.399843  23.898954  30.613064  30.968691   \n",
       "31   5.359801  11.186870  13.392621  18.200813  25.133646  29.300283   \n",
       "32   2.651395   5.862431   7.325083  10.502310  15.610394  17.168150   \n",
       "33   0.607011   1.370581   1.706633   2.666871   4.647159   6.391905   \n",
       "34  -0.018686   0.006316  -0.066219   0.048272   0.383702   0.320101   \n",
       "35  -0.018591   0.005642  -0.064182   0.046856   0.361231   0.273943   \n",
       "36  -0.016708   0.006199  -0.064015   0.038820   0.241165   0.061352   \n",
       "37  -0.016704   0.005725  -0.062413   0.038023   0.230339   0.054719   \n",
       "38  -0.015256   0.005458  -0.063016   0.029434   0.139539   0.037390   \n",
       "39  -0.015307   0.004999  -0.061230   0.028803   0.133845   0.036290   \n",
       "40  -0.014210   0.003633  -0.062657   0.020538   0.076773   0.027211   \n",
       "41  -0.014314   0.003463  -0.061074   0.020983   0.077586   0.027102   \n",
       "42  -0.013486   0.000969  -0.063481   0.013526   0.048725   0.017235   \n",
       "43  -0.013661   0.000701  -0.060726   0.014092   0.052172   0.018280   \n",
       "44  -0.013122  -0.002533  -0.062864   0.009250   0.046887   0.013539   \n",
       "45  -0.013175  -0.002405  -0.062392   0.009651   0.047458   0.013374   \n",
       "46  -0.012736  -0.005070  -0.065544   0.007408   0.051039   0.012819   \n",
       "47  -0.012736  -0.005070  -0.065544   0.007408   0.051039   0.012819   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.067487  -0.037570  -0.059470  \n",
       "1    0.066699  -0.036306  -0.059932  \n",
       "2    0.043162  -0.032847  -0.065703  \n",
       "3    0.043316  -0.033691  -0.065018  \n",
       "4    0.021891  -0.024199  -0.064344  \n",
       "5    0.020673  -0.021845  -0.064828  \n",
       "6   -0.000146   0.000118  -0.052623  \n",
       "7    0.002504   0.011485  -0.045988  \n",
       "8   -0.011380   0.068812   0.004021  \n",
       "9   -0.006409   0.080200   0.012314  \n",
       "10  -0.027262   0.176760   0.100227  \n",
       "11  -0.022023   0.181764   0.101153  \n",
       "12  -0.042680   0.299701   0.225337  \n",
       "13  -0.029952   0.311315   0.233721  \n",
       "14   0.399456   0.728253   0.777782  \n",
       "15   1.765496   2.347492   3.400994  \n",
       "16  10.278186  11.965900  17.002937  \n",
       "17  15.058845  17.018358  23.022182  \n",
       "18  27.674852  30.491608  38.536106  \n",
       "19  30.027628  32.220806  39.539894  \n",
       "20  39.919140  39.649029  43.025505  \n",
       "21  41.386456  40.348499  41.715477  \n",
       "22  47.899578  48.906429  54.869659  \n",
       "23  46.664230  47.513596  54.997051  \n",
       "24  46.891632  46.490215  53.178963  \n",
       "25  47.039780  47.080353  51.653759  \n",
       "26  45.127369  43.915695  49.190712  \n",
       "27  47.998390  47.577015  56.211308  \n",
       "28  41.062660  39.765026  44.686806  \n",
       "29  40.055161  38.048893  41.940247  \n",
       "30  32.552818  31.655125  33.889275  \n",
       "31  30.001242  31.335520  37.509048  \n",
       "32  16.521379  18.074160  23.327524  \n",
       "33   5.914311   8.872417  12.147826  \n",
       "34   0.248948   0.320272   0.274428  \n",
       "35   0.214364   0.286921   0.231477  \n",
       "36   0.051438   0.093741   0.051620  \n",
       "37   0.047684   0.090578   0.048201  \n",
       "38   0.027684   0.051646   0.027003  \n",
       "39   0.027881   0.051082   0.024909  \n",
       "40   0.013049   0.018587  -0.013091  \n",
       "41   0.013551   0.019473  -0.012896  \n",
       "42   0.005030   0.003685  -0.028403  \n",
       "43   0.006608   0.005081  -0.027395  \n",
       "44   0.006903   0.004366  -0.021610  \n",
       "45   0.006705   0.004357  -0.021657  \n",
       "46   0.011944   0.011234  -0.006435  \n",
       "47   0.011944   0.011234  -0.006435  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_G7 = tf.keras.Sequential([\n",
    "    layers.GRU(units=64, return_sequences=True, input_shape=[52464, 7]),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_G7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_G7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_G7.fit(np.array(Day).reshape(52464, 1, 7), np.array(Day7).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_G7 = np.squeeze(model_G7.predict(np.array(df_test).reshape(3888, 1, 7)))\n",
    "    pred_G7 = pd.DataFrame(pred_G7)\n",
    "    result_G7 = pd.concat([result_G7, pred_G7], axis=1)\n",
    "    \n",
    "result_G7[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "812/820 [============================>.] - ETA: 0s - loss: 1.5688WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 8s 4ms/step - loss: 1.5680 - val_loss: 1.6295\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4339 - val_loss: 1.6128\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4202 - val_loss: 1.6026\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4095 - val_loss: 1.5938\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4113 - val_loss: 1.5836\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3964 - val_loss: 1.5907\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3974 - val_loss: 1.5858\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4109 - val_loss: 1.5790\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3936 - val_loss: 1.5737\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.4113 - val_loss: 1.5767\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3797 - val_loss: 1.5658\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3738 - val_loss: 1.5721\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3829 - val_loss: 1.5650\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3676 - val_loss: 1.5691\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3985 - val_loss: 1.5660\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3872 - val_loss: 1.5657\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3852 - val_loss: 1.5733\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3779 - val_loss: 1.5660\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "805/820 [============================>.] - ETA: 0s - loss: 2.2591WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.2590 - val_loss: 2.5549\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2328 - val_loss: 2.5478\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2202 - val_loss: 2.5407\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2039 - val_loss: 2.5387\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2238 - val_loss: 2.5297\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2076 - val_loss: 2.5379\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2188 - val_loss: 2.5249\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2400 - val_loss: 2.5239\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2097 - val_loss: 2.5235\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2444 - val_loss: 2.5215\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1691 - val_loss: 2.5261\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1671 - val_loss: 2.5158\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2022 - val_loss: 2.5166\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1950 - val_loss: 2.5148\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2061 - val_loss: 2.5252\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1986 - val_loss: 2.5145\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2047 - val_loss: 2.5219\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1961 - val_loss: 2.5103\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1989 - val_loss: 2.5183\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2158 - val_loss: 2.5136\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2238 - val_loss: 2.5153\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1807 - val_loss: 2.5145\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.1858 - val_loss: 2.5167\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "819/820 [============================>.] - ETA: 0s - loss: 2.6332WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6332 - val_loss: 2.9860\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6080 - val_loss: 2.9711\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6076 - val_loss: 2.9673\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5810 - val_loss: 2.9672\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6173 - val_loss: 2.9661\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6135 - val_loss: 3.0007\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6243 - val_loss: 2.9670\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 4s 4ms/step - loss: 2.6486 - val_loss: 2.9605\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5973 - val_loss: 2.9562\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6512 - val_loss: 2.9588\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5456 - val_loss: 2.9682\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5610 - val_loss: 2.9608\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6012 - val_loss: 2.9513\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5980 - val_loss: 2.9715\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5899 - val_loss: 2.9876\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5907 - val_loss: 2.9557\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5983 - val_loss: 2.9805\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5988 - val_loss: 2.9600\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "807/820 [============================>.] - ETA: 0s - loss: 2.7470WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.7471 - val_loss: 3.1018\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7194 - val_loss: 3.0828\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7137 - val_loss: 3.0952\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6918 - val_loss: 3.0961\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 3ms/step - loss: 2.7316 - val_loss: 3.1019\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7340 - val_loss: 3.1063\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7461 - val_loss: 3.0820\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7781 - val_loss: 3.0753\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7165 - val_loss: 3.0773\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7836 - val_loss: 3.0733\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6637 - val_loss: 3.0642\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6747 - val_loss: 3.0807\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7174 - val_loss: 3.0745\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7176 - val_loss: 3.0741\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.7069 - val_loss: 3.0848\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6999 - val_loss: 3.0729\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "819/820 [============================>.] - ETA: 0s - loss: 2.6514WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.6514 - val_loss: 2.9765\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6190 - val_loss: 2.9526\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6151 - val_loss: 2.9718\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6010 - val_loss: 2.9793\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6297 - val_loss: 2.9812\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6392 - val_loss: 2.9468\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6395 - val_loss: 2.9542\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6775 - val_loss: 2.9286\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6107 - val_loss: 2.9230\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6715 - val_loss: 2.9307\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5565 - val_loss: 2.9237\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5666 - val_loss: 2.9402\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.6032 - val_loss: 2.9243\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.5962 - val_loss: 2.9331\n",
      "Epoch 00014: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - ETA: 0s - loss: 2.3747WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 2.3747 - val_loss: 2.6428\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3424 - val_loss: 2.6389\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.3405 - val_loss: 2.6333\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3271 - val_loss: 2.6504\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3412 - val_loss: 2.6350\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3550 - val_loss: 2.6302\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3466 - val_loss: 2.6197\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3805 - val_loss: 2.6146\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3123 - val_loss: 2.6174\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3660 - val_loss: 2.6057\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2681 - val_loss: 2.6220\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2818 - val_loss: 2.6170\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3134 - val_loss: 2.5988\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2933 - val_loss: 2.6017\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2890 - val_loss: 2.6046\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2778 - val_loss: 2.5877\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3139 - val_loss: 2.5920\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3120 - val_loss: 2.5845\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2918 - val_loss: 2.5979\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3181 - val_loss: 2.5908\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.3402 - val_loss: 2.5825\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2791 - val_loss: 2.5926\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2933 - val_loss: 2.6038\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2987 - val_loss: 2.5919\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2604 - val_loss: 2.5921\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 2.2654 - val_loss: 2.5971\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "803/820 [============================>.] - ETA: 0s - loss: 1.9041WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 1.9042 - val_loss: 2.1365\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8726 - val_loss: 2.1347\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8890 - val_loss: 2.1454\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8717 - val_loss: 2.1439\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8782 - val_loss: 2.1410\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8877 - val_loss: 2.1562\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.8931 - val_loss: 2.1452\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "811/820 [============================>.] - ETA: 0s - loss: 1.3872WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 5ms/step - loss: 1.3873 - val_loss: 1.5789\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3626 - val_loss: 1.5718\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3852 - val_loss: 1.5694\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3714 - val_loss: 1.5818\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3746 - val_loss: 1.5797\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3787 - val_loss: 1.5650\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3817 - val_loss: 1.5630\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3935 - val_loss: 1.5665\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3594 - val_loss: 1.5508\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3821 - val_loss: 1.5389\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3288 - val_loss: 1.5458\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3456 - val_loss: 1.5957\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3597 - val_loss: 1.5784\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3512 - val_loss: 1.5732\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 1.3459 - val_loss: 1.5842\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 0.7671WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n",
      "820/820 [==============================] - 6s 4ms/step - loss: 0.7671 - val_loss: 0.8700\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7528 - val_loss: 0.8853\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 0.7649 - val_loss: 0.8822\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7608 - val_loss: 0.8781\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7584 - val_loss: 0.8762\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7626 - val_loss: 0.8644\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7664 - val_loss: 0.8749\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7729 - val_loss: 0.8738\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7540 - val_loss: 0.8695\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7672 - val_loss: 0.8592\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7355 - val_loss: 0.8622\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7493 - val_loss: 0.8754\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7571 - val_loss: 0.8713\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7520 - val_loss: 0.8691\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 3s 4ms/step - loss: 0.7423 - val_loss: 0.8611\n",
      "Epoch 00015: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='gru_12_input'), name='gru_12_input', description=\"created by layer 'gru_12_input'\"), but it was called on an input with incompatible shape (None, 1, 7).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.136228</td>\n",
       "      <td>-0.065938</td>\n",
       "      <td>-0.017875</td>\n",
       "      <td>-0.006030</td>\n",
       "      <td>-0.042363</td>\n",
       "      <td>0.124444</td>\n",
       "      <td>0.054111</td>\n",
       "      <td>0.033410</td>\n",
       "      <td>0.149395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.139516</td>\n",
       "      <td>-0.066540</td>\n",
       "      <td>-0.015982</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>-0.040749</td>\n",
       "      <td>0.120525</td>\n",
       "      <td>0.051028</td>\n",
       "      <td>0.029001</td>\n",
       "      <td>0.144378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.130554</td>\n",
       "      <td>-0.063635</td>\n",
       "      <td>-0.013373</td>\n",
       "      <td>-0.003557</td>\n",
       "      <td>-0.016334</td>\n",
       "      <td>0.095406</td>\n",
       "      <td>0.012027</td>\n",
       "      <td>0.016652</td>\n",
       "      <td>0.129127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.127771</td>\n",
       "      <td>-0.062563</td>\n",
       "      <td>-0.014405</td>\n",
       "      <td>-0.005783</td>\n",
       "      <td>-0.016531</td>\n",
       "      <td>0.096952</td>\n",
       "      <td>0.013146</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.132215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.116360</td>\n",
       "      <td>-0.058564</td>\n",
       "      <td>-0.011282</td>\n",
       "      <td>-0.006463</td>\n",
       "      <td>-0.002232</td>\n",
       "      <td>0.095503</td>\n",
       "      <td>-0.005171</td>\n",
       "      <td>0.025786</td>\n",
       "      <td>0.137172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.121470</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>-0.009799</td>\n",
       "      <td>-0.003458</td>\n",
       "      <td>-0.002374</td>\n",
       "      <td>0.095030</td>\n",
       "      <td>-0.004318</td>\n",
       "      <td>0.023559</td>\n",
       "      <td>0.135302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.112405</td>\n",
       "      <td>-0.058098</td>\n",
       "      <td>-0.005676</td>\n",
       "      <td>-0.002550</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.105383</td>\n",
       "      <td>-0.011543</td>\n",
       "      <td>0.041628</td>\n",
       "      <td>0.160087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.122392</td>\n",
       "      <td>-0.065947</td>\n",
       "      <td>-0.006477</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>-0.005160</td>\n",
       "      <td>0.104707</td>\n",
       "      <td>-0.014943</td>\n",
       "      <td>0.035519</td>\n",
       "      <td>0.163659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.117374</td>\n",
       "      <td>-0.070698</td>\n",
       "      <td>-0.007008</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>-0.014445</td>\n",
       "      <td>0.106983</td>\n",
       "      <td>-0.042816</td>\n",
       "      <td>0.046646</td>\n",
       "      <td>0.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.120800</td>\n",
       "      <td>-0.075948</td>\n",
       "      <td>-0.009812</td>\n",
       "      <td>0.010962</td>\n",
       "      <td>-0.019854</td>\n",
       "      <td>0.103420</td>\n",
       "      <td>-0.049169</td>\n",
       "      <td>0.041006</td>\n",
       "      <td>0.222411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.112522</td>\n",
       "      <td>-0.081098</td>\n",
       "      <td>-0.013335</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.022418</td>\n",
       "      <td>0.097144</td>\n",
       "      <td>-0.088252</td>\n",
       "      <td>0.082815</td>\n",
       "      <td>0.341714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.112094</td>\n",
       "      <td>-0.081045</td>\n",
       "      <td>-0.013007</td>\n",
       "      <td>0.017366</td>\n",
       "      <td>-0.021500</td>\n",
       "      <td>0.096238</td>\n",
       "      <td>-0.087552</td>\n",
       "      <td>0.081017</td>\n",
       "      <td>0.347641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.103269</td>\n",
       "      <td>-0.084355</td>\n",
       "      <td>-0.015816</td>\n",
       "      <td>0.026082</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>0.096770</td>\n",
       "      <td>-0.115912</td>\n",
       "      <td>0.220806</td>\n",
       "      <td>0.621086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.102852</td>\n",
       "      <td>-0.084310</td>\n",
       "      <td>-0.015443</td>\n",
       "      <td>0.024764</td>\n",
       "      <td>-0.006084</td>\n",
       "      <td>0.099037</td>\n",
       "      <td>-0.110983</td>\n",
       "      <td>0.228315</td>\n",
       "      <td>0.643642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.095732</td>\n",
       "      <td>-0.091330</td>\n",
       "      <td>-0.022491</td>\n",
       "      <td>0.039727</td>\n",
       "      <td>0.027375</td>\n",
       "      <td>0.160899</td>\n",
       "      <td>-0.155980</td>\n",
       "      <td>0.564401</td>\n",
       "      <td>1.131276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.369348</td>\n",
       "      <td>0.506905</td>\n",
       "      <td>0.677389</td>\n",
       "      <td>1.012497</td>\n",
       "      <td>1.183186</td>\n",
       "      <td>1.313797</td>\n",
       "      <td>1.357211</td>\n",
       "      <td>2.983646</td>\n",
       "      <td>4.379683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.864161</td>\n",
       "      <td>2.746220</td>\n",
       "      <td>3.418093</td>\n",
       "      <td>4.847100</td>\n",
       "      <td>6.020610</td>\n",
       "      <td>7.469060</td>\n",
       "      <td>8.483461</td>\n",
       "      <td>14.435596</td>\n",
       "      <td>19.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.407993</td>\n",
       "      <td>5.251562</td>\n",
       "      <td>6.494757</td>\n",
       "      <td>8.928370</td>\n",
       "      <td>10.683314</td>\n",
       "      <td>11.824099</td>\n",
       "      <td>13.312223</td>\n",
       "      <td>19.104517</td>\n",
       "      <td>24.550512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.936254</td>\n",
       "      <td>8.306248</td>\n",
       "      <td>10.389914</td>\n",
       "      <td>14.263079</td>\n",
       "      <td>17.529009</td>\n",
       "      <td>21.578634</td>\n",
       "      <td>23.262562</td>\n",
       "      <td>31.169617</td>\n",
       "      <td>38.503490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.667759</td>\n",
       "      <td>11.625317</td>\n",
       "      <td>14.222661</td>\n",
       "      <td>18.866646</td>\n",
       "      <td>21.796297</td>\n",
       "      <td>24.004034</td>\n",
       "      <td>25.834900</td>\n",
       "      <td>32.365593</td>\n",
       "      <td>39.108696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.646495</td>\n",
       "      <td>20.164499</td>\n",
       "      <td>23.253029</td>\n",
       "      <td>29.983934</td>\n",
       "      <td>32.188301</td>\n",
       "      <td>33.887871</td>\n",
       "      <td>35.720451</td>\n",
       "      <td>39.306675</td>\n",
       "      <td>42.739048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.043048</td>\n",
       "      <td>23.106028</td>\n",
       "      <td>25.975401</td>\n",
       "      <td>33.011955</td>\n",
       "      <td>34.107307</td>\n",
       "      <td>35.336517</td>\n",
       "      <td>37.646378</td>\n",
       "      <td>40.409988</td>\n",
       "      <td>41.648247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11.248525</td>\n",
       "      <td>22.238810</td>\n",
       "      <td>26.163748</td>\n",
       "      <td>34.654366</td>\n",
       "      <td>38.338902</td>\n",
       "      <td>42.688370</td>\n",
       "      <td>43.646309</td>\n",
       "      <td>48.332092</td>\n",
       "      <td>55.579094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.437646</td>\n",
       "      <td>18.455217</td>\n",
       "      <td>22.124241</td>\n",
       "      <td>29.627062</td>\n",
       "      <td>34.380100</td>\n",
       "      <td>41.653198</td>\n",
       "      <td>41.864502</td>\n",
       "      <td>45.961647</td>\n",
       "      <td>54.410767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9.098477</td>\n",
       "      <td>18.338068</td>\n",
       "      <td>21.915840</td>\n",
       "      <td>29.724241</td>\n",
       "      <td>35.040905</td>\n",
       "      <td>42.893997</td>\n",
       "      <td>44.667122</td>\n",
       "      <td>45.860283</td>\n",
       "      <td>53.146549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.641381</td>\n",
       "      <td>21.552134</td>\n",
       "      <td>25.038074</td>\n",
       "      <td>33.499081</td>\n",
       "      <td>37.696407</td>\n",
       "      <td>42.653442</td>\n",
       "      <td>44.088566</td>\n",
       "      <td>46.039841</td>\n",
       "      <td>51.154129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.956491</td>\n",
       "      <td>18.569803</td>\n",
       "      <td>21.983406</td>\n",
       "      <td>30.074331</td>\n",
       "      <td>35.125187</td>\n",
       "      <td>40.295162</td>\n",
       "      <td>44.003593</td>\n",
       "      <td>44.212109</td>\n",
       "      <td>49.737137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.244344</td>\n",
       "      <td>12.809286</td>\n",
       "      <td>16.143456</td>\n",
       "      <td>22.943909</td>\n",
       "      <td>29.519659</td>\n",
       "      <td>38.815144</td>\n",
       "      <td>45.067322</td>\n",
       "      <td>48.078381</td>\n",
       "      <td>57.933491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.923687</td>\n",
       "      <td>14.719525</td>\n",
       "      <td>17.888050</td>\n",
       "      <td>25.095015</td>\n",
       "      <td>30.476128</td>\n",
       "      <td>33.850498</td>\n",
       "      <td>39.270145</td>\n",
       "      <td>40.086098</td>\n",
       "      <td>46.328804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.525579</td>\n",
       "      <td>15.883420</td>\n",
       "      <td>18.861889</td>\n",
       "      <td>26.248676</td>\n",
       "      <td>31.153513</td>\n",
       "      <td>33.709053</td>\n",
       "      <td>38.867054</td>\n",
       "      <td>39.115379</td>\n",
       "      <td>43.970497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6.727216</td>\n",
       "      <td>14.646409</td>\n",
       "      <td>17.301058</td>\n",
       "      <td>24.136824</td>\n",
       "      <td>28.185440</td>\n",
       "      <td>27.241558</td>\n",
       "      <td>31.708794</td>\n",
       "      <td>32.004623</td>\n",
       "      <td>35.764736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.633355</td>\n",
       "      <td>9.978655</td>\n",
       "      <td>12.620058</td>\n",
       "      <td>18.337187</td>\n",
       "      <td>23.519548</td>\n",
       "      <td>25.084269</td>\n",
       "      <td>29.849827</td>\n",
       "      <td>30.692434</td>\n",
       "      <td>36.598442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.287090</td>\n",
       "      <td>4.961944</td>\n",
       "      <td>6.789040</td>\n",
       "      <td>10.537251</td>\n",
       "      <td>14.582564</td>\n",
       "      <td>15.097529</td>\n",
       "      <td>19.452991</td>\n",
       "      <td>19.053560</td>\n",
       "      <td>23.005661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.519996</td>\n",
       "      <td>1.048252</td>\n",
       "      <td>1.544802</td>\n",
       "      <td>2.675344</td>\n",
       "      <td>4.108562</td>\n",
       "      <td>4.306191</td>\n",
       "      <td>8.081111</td>\n",
       "      <td>9.148691</td>\n",
       "      <td>13.213209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.025265</td>\n",
       "      <td>-0.065634</td>\n",
       "      <td>-0.030779</td>\n",
       "      <td>0.092690</td>\n",
       "      <td>0.210482</td>\n",
       "      <td>0.223859</td>\n",
       "      <td>0.418085</td>\n",
       "      <td>0.609916</td>\n",
       "      <td>0.848460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.025558</td>\n",
       "      <td>-0.063045</td>\n",
       "      <td>-0.027376</td>\n",
       "      <td>0.081992</td>\n",
       "      <td>0.195307</td>\n",
       "      <td>0.193689</td>\n",
       "      <td>0.349772</td>\n",
       "      <td>0.546725</td>\n",
       "      <td>0.760980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.022511</td>\n",
       "      <td>-0.058099</td>\n",
       "      <td>-0.027911</td>\n",
       "      <td>0.070014</td>\n",
       "      <td>0.117508</td>\n",
       "      <td>0.044645</td>\n",
       "      <td>-0.070036</td>\n",
       "      <td>0.105111</td>\n",
       "      <td>0.138102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.022694</td>\n",
       "      <td>-0.056486</td>\n",
       "      <td>-0.025301</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.110403</td>\n",
       "      <td>0.039599</td>\n",
       "      <td>-0.078060</td>\n",
       "      <td>0.098680</td>\n",
       "      <td>0.129115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.020190</td>\n",
       "      <td>-0.052439</td>\n",
       "      <td>-0.027827</td>\n",
       "      <td>0.052328</td>\n",
       "      <td>0.054510</td>\n",
       "      <td>0.023871</td>\n",
       "      <td>-0.107948</td>\n",
       "      <td>0.059892</td>\n",
       "      <td>0.057035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.020367</td>\n",
       "      <td>-0.051027</td>\n",
       "      <td>-0.025128</td>\n",
       "      <td>0.046541</td>\n",
       "      <td>0.050599</td>\n",
       "      <td>0.023698</td>\n",
       "      <td>-0.104779</td>\n",
       "      <td>0.059453</td>\n",
       "      <td>0.054806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.018274</td>\n",
       "      <td>-0.047662</td>\n",
       "      <td>-0.029014</td>\n",
       "      <td>0.035703</td>\n",
       "      <td>0.015991</td>\n",
       "      <td>0.037380</td>\n",
       "      <td>-0.052085</td>\n",
       "      <td>0.063366</td>\n",
       "      <td>0.031672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.018348</td>\n",
       "      <td>-0.046874</td>\n",
       "      <td>-0.026360</td>\n",
       "      <td>0.031857</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>0.037670</td>\n",
       "      <td>-0.048679</td>\n",
       "      <td>0.062427</td>\n",
       "      <td>0.029004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.016598</td>\n",
       "      <td>-0.044092</td>\n",
       "      <td>-0.031666</td>\n",
       "      <td>0.021905</td>\n",
       "      <td>-0.002380</td>\n",
       "      <td>0.047918</td>\n",
       "      <td>-0.005623</td>\n",
       "      <td>0.050088</td>\n",
       "      <td>0.000499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.016740</td>\n",
       "      <td>-0.042958</td>\n",
       "      <td>-0.027696</td>\n",
       "      <td>0.017449</td>\n",
       "      <td>-0.001795</td>\n",
       "      <td>0.047584</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.048175</td>\n",
       "      <td>-0.002206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.015282</td>\n",
       "      <td>-0.040405</td>\n",
       "      <td>-0.032378</td>\n",
       "      <td>0.008183</td>\n",
       "      <td>-0.007060</td>\n",
       "      <td>0.048644</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.026801</td>\n",
       "      <td>-0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.015290</td>\n",
       "      <td>-0.040372</td>\n",
       "      <td>-0.031572</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>-0.006802</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.014991</td>\n",
       "      <td>0.026887</td>\n",
       "      <td>-0.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.014085</td>\n",
       "      <td>-0.038329</td>\n",
       "      <td>-0.037806</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>-0.005457</td>\n",
       "      <td>0.045036</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.011181</td>\n",
       "      <td>-0.023578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.014085</td>\n",
       "      <td>-0.038329</td>\n",
       "      <td>-0.037806</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>-0.005457</td>\n",
       "      <td>0.045036</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.011181</td>\n",
       "      <td>-0.023578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          0          0          0          0          0  \\\n",
       "0   -0.136228  -0.065938  -0.017875  -0.006030  -0.042363   0.124444   \n",
       "1   -0.139516  -0.066540  -0.015982  -0.003475  -0.040749   0.120525   \n",
       "2   -0.130554  -0.063635  -0.013373  -0.003557  -0.016334   0.095406   \n",
       "3   -0.127771  -0.062563  -0.014405  -0.005783  -0.016531   0.096952   \n",
       "4   -0.116360  -0.058564  -0.011282  -0.006463  -0.002232   0.095503   \n",
       "5   -0.121470  -0.060791  -0.009799  -0.003458  -0.002374   0.095030   \n",
       "6   -0.112405  -0.058098  -0.005676  -0.002550   0.001353   0.105383   \n",
       "7   -0.122392  -0.065947  -0.006477   0.003264  -0.005160   0.104707   \n",
       "8   -0.117374  -0.070698  -0.007008   0.008492  -0.014445   0.106983   \n",
       "9   -0.120800  -0.075948  -0.009812   0.010962  -0.019854   0.103420   \n",
       "10  -0.112522  -0.081098  -0.013335   0.018562  -0.022418   0.097144   \n",
       "11  -0.112094  -0.081045  -0.013007   0.017366  -0.021500   0.096238   \n",
       "12  -0.103269  -0.084355  -0.015816   0.026082  -0.007479   0.096770   \n",
       "13  -0.102852  -0.084310  -0.015443   0.024764  -0.006084   0.099037   \n",
       "14  -0.095732  -0.091330  -0.022491   0.039727   0.027375   0.160899   \n",
       "15   0.369348   0.506905   0.677389   1.012497   1.183186   1.313797   \n",
       "16   1.864161   2.746220   3.418093   4.847100   6.020610   7.469060   \n",
       "17   3.407993   5.251562   6.494757   8.928370  10.683314  11.824099   \n",
       "18   4.936254   8.306248  10.389914  14.263079  17.529009  21.578634   \n",
       "19   6.667759  11.625317  14.222661  18.866646  21.796297  24.004034   \n",
       "20  10.646495  20.164499  23.253029  29.983934  32.188301  33.887871   \n",
       "21  12.043048  23.106028  25.975401  33.011955  34.107307  35.336517   \n",
       "22  11.248525  22.238810  26.163748  34.654366  38.338902  42.688370   \n",
       "23   9.437646  18.455217  22.124241  29.627062  34.380100  41.653198   \n",
       "24   9.098477  18.338068  21.915840  29.724241  35.040905  42.893997   \n",
       "25  10.641381  21.552134  25.038074  33.499081  37.696407  42.653442   \n",
       "26   8.956491  18.569803  21.983406  30.074331  35.125187  40.295162   \n",
       "27   6.244344  12.809286  16.143456  22.943909  29.519659  38.815144   \n",
       "28   6.923687  14.719525  17.888050  25.095015  30.476128  33.850498   \n",
       "29   7.525579  15.883420  18.861889  26.248676  31.153513  33.709053   \n",
       "30   6.727216  14.646409  17.301058  24.136824  28.185440  27.241558   \n",
       "31   4.633355   9.978655  12.620058  18.337187  23.519548  25.084269   \n",
       "32   2.287090   4.961944   6.789040  10.537251  14.582564  15.097529   \n",
       "33   0.519996   1.048252   1.544802   2.675344   4.108562   4.306191   \n",
       "34  -0.025265  -0.065634  -0.030779   0.092690   0.210482   0.223859   \n",
       "35  -0.025558  -0.063045  -0.027376   0.081992   0.195307   0.193689   \n",
       "36  -0.022511  -0.058099  -0.027911   0.070014   0.117508   0.044645   \n",
       "37  -0.022694  -0.056486  -0.025301   0.063300   0.110403   0.039599   \n",
       "38  -0.020190  -0.052439  -0.027827   0.052328   0.054510   0.023871   \n",
       "39  -0.020367  -0.051027  -0.025128   0.046541   0.050599   0.023698   \n",
       "40  -0.018274  -0.047662  -0.029014   0.035703   0.015991   0.037380   \n",
       "41  -0.018348  -0.046874  -0.026360   0.031857   0.015491   0.037670   \n",
       "42  -0.016598  -0.044092  -0.031666   0.021905  -0.002380   0.047918   \n",
       "43  -0.016740  -0.042958  -0.027696   0.017449  -0.001795   0.047584   \n",
       "44  -0.015282  -0.040405  -0.032378   0.008183  -0.007060   0.048644   \n",
       "45  -0.015290  -0.040372  -0.031572   0.007808  -0.006802   0.048771   \n",
       "46  -0.014085  -0.038329  -0.037806   0.001997  -0.005457   0.045036   \n",
       "47  -0.014085  -0.038329  -0.037806   0.001997  -0.005457   0.045036   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.054111   0.033410   0.149395  \n",
       "1    0.051028   0.029001   0.144378  \n",
       "2    0.012027   0.016652   0.129127  \n",
       "3    0.013146   0.018858   0.132215  \n",
       "4   -0.005171   0.025786   0.137172  \n",
       "5   -0.004318   0.023559   0.135302  \n",
       "6   -0.011543   0.041628   0.160087  \n",
       "7   -0.014943   0.035519   0.163659  \n",
       "8   -0.042816   0.046646   0.215700  \n",
       "9   -0.049169   0.041006   0.222411  \n",
       "10  -0.088252   0.082815   0.341714  \n",
       "11  -0.087552   0.081017   0.347641  \n",
       "12  -0.115912   0.220806   0.621086  \n",
       "13  -0.110983   0.228315   0.643642  \n",
       "14  -0.155980   0.564401   1.131276  \n",
       "15   1.357211   2.983646   4.379683  \n",
       "16   8.483461  14.435596  19.271200  \n",
       "17  13.312223  19.104517  24.550512  \n",
       "18  23.262562  31.169617  38.503490  \n",
       "19  25.834900  32.365593  39.108696  \n",
       "20  35.720451  39.306675  42.739048  \n",
       "21  37.646378  40.409988  41.648247  \n",
       "22  43.646309  48.332092  55.579094  \n",
       "23  41.864502  45.961647  54.410767  \n",
       "24  44.667122  45.860283  53.146549  \n",
       "25  44.088566  46.039841  51.154129  \n",
       "26  44.003593  44.212109  49.737137  \n",
       "27  45.067322  48.078381  57.933491  \n",
       "28  39.270145  40.086098  46.328804  \n",
       "29  38.867054  39.115379  43.970497  \n",
       "30  31.708794  32.004623  35.764736  \n",
       "31  29.849827  30.692434  36.598442  \n",
       "32  19.452991  19.053560  23.005661  \n",
       "33   8.081111   9.148691  13.213209  \n",
       "34   0.418085   0.609916   0.848460  \n",
       "35   0.349772   0.546725   0.760980  \n",
       "36  -0.070036   0.105111   0.138102  \n",
       "37  -0.078060   0.098680   0.129115  \n",
       "38  -0.107948   0.059892   0.057035  \n",
       "39  -0.104779   0.059453   0.054806  \n",
       "40  -0.052085   0.063366   0.031672  \n",
       "41  -0.048679   0.062427   0.029004  \n",
       "42  -0.005623   0.050088   0.000499  \n",
       "43  -0.002816   0.048175  -0.002206  \n",
       "44   0.014634   0.026801  -0.022100  \n",
       "45   0.014991   0.026887  -0.022312  \n",
       "46   0.015228   0.011181  -0.023578  \n",
       "47   0.015228   0.011181  -0.023578  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_G8 = tf.keras.Sequential([\n",
    "    layers.GRU(units=64, return_sequences=True, input_shape=[52464, 7]),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_G8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_G8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_G8.fit(np.array(Day).reshape(52464, 1, 7), np.array(Day7).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_G8 = np.squeeze(model_G8.predict(np.array(df_test).reshape(3888, 1, 7)))\n",
    "    pred_G8 = pd.DataFrame(pred_G8)\n",
    "    result_G8 = pd.concat([result_G8, pred_G8], axis=1)\n",
    "    \n",
    "result_G8[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "814/820 [============================>.] - ETA: 0s - loss: 1.5871WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 7s 6ms/step - loss: 1.5864 - val_loss: 1.6402\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.4350 - val_loss: 1.6221\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.4205 - val_loss: 1.6244\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.4077 - val_loss: 1.6067\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.4058 - val_loss: 1.6041\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3901 - val_loss: 1.6249\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3885 - val_loss: 1.6348\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.4018 - val_loss: 1.6124\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "809/820 [============================>.] - ETA: 0s - loss: 2.3192WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 7s 6ms/step - loss: 2.3191 - val_loss: 2.6130\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.2662 - val_loss: 2.5944\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.2385 - val_loss: 2.6224\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.2126 - val_loss: 2.5674\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.2268 - val_loss: 2.5610\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.2082 - val_loss: 2.6555\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.2160 - val_loss: 2.5830\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.2362 - val_loss: 2.5721\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "817/820 [============================>.] - ETA: 0s - loss: 2.6942WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 7s 6ms/step - loss: 2.6942 - val_loss: 3.0504\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6513 - val_loss: 3.0132\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6389 - val_loss: 2.9993\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6117 - val_loss: 3.0060\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6426 - val_loss: 2.9789\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6272 - val_loss: 3.0647\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6346 - val_loss: 3.0247\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6578 - val_loss: 2.9856\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 2.7931WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 10s 6ms/step - loss: 2.7932 - val_loss: 3.1515\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.7585 - val_loss: 3.1281\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.7471 - val_loss: 3.1039\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.7247 - val_loss: 3.1239\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.7582 - val_loss: 3.1044\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.7592 - val_loss: 3.1293\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "808/820 [============================>.] - ETA: 0s - loss: 2.7049WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 7s 6ms/step - loss: 2.7050 - val_loss: 3.0252\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.6726 - val_loss: 3.0012\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6590 - val_loss: 3.0005\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6446 - val_loss: 2.9990\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6737 - val_loss: 3.0116\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6822 - val_loss: 3.0138\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6785 - val_loss: 2.9988\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.7133 - val_loss: 2.9726\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6491 - val_loss: 2.9623\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.7095 - val_loss: 2.9766\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.5996 - val_loss: 2.9685\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.6077 - val_loss: 2.9845\n",
      "Epoch 00012: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "811/820 [============================>.] - ETA: 0s - loss: 2.4396WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 7s 6ms/step - loss: 2.4397 - val_loss: 2.7113\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.4101 - val_loss: 2.7079\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.4030 - val_loss: 2.7126\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.3941 - val_loss: 2.7215\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.4133 - val_loss: 2.7275\n",
      "Epoch 00005: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "819/820 [============================>.] - ETA: 0s - loss: 2.0753WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 7s 6ms/step - loss: 2.0753 - val_loss: 2.3044\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.0442 - val_loss: 2.2935\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.0394 - val_loss: 2.2995\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.0266 - val_loss: 2.2916\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.0418 - val_loss: 2.2934\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.0522 - val_loss: 2.2567\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.0320 - val_loss: 2.2638\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.0641 - val_loss: 2.2469\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.0013 - val_loss: 2.2368\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 2.0360 - val_loss: 2.2322\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9595 - val_loss: 2.2439\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9739 - val_loss: 2.2272\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9854 - val_loss: 2.2223\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9600 - val_loss: 2.2002\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9617 - val_loss: 2.2358\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9432 - val_loss: 2.1991\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9792 - val_loss: 2.2093\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9755 - val_loss: 2.1798\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9461 - val_loss: 2.1853\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9603 - val_loss: 2.1832\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9771 - val_loss: 2.1688\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9201 - val_loss: 2.1685\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9346 - val_loss: 2.1908\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9343 - val_loss: 2.1532\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.8927 - val_loss: 2.1422\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9024 - val_loss: 2.1685\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9284 - val_loss: 2.1510\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 4s 5ms/step - loss: 1.9021 - val_loss: 2.1643\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "811/820 [============================>.] - ETA: 0s - loss: 1.4104WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 7s 6ms/step - loss: 1.4105 - val_loss: 1.5840\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3872 - val_loss: 1.5840\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.4040 - val_loss: 1.5828\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3880 - val_loss: 1.6142\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3923 - val_loss: 1.5921\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3943 - val_loss: 1.5623\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3983 - val_loss: 1.5817\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.4079 - val_loss: 1.5623\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3739 - val_loss: 1.5550\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3968 - val_loss: 1.5474\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3405 - val_loss: 1.5609\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3619 - val_loss: 1.5779\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 1.3723 - val_loss: 1.5632\n",
      "Epoch 00013: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "810/820 [============================>.] - ETA: 0s - loss: 0.7718WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 7s 6ms/step - loss: 0.7718 - val_loss: 0.8913\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 0.7628 - val_loss: 0.9206\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 0.7726 - val_loss: 0.8770\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 0.7644 - val_loss: 0.9039\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 0.7634 - val_loss: 0.8950\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 0.7666 - val_loss: 0.8649\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 0.7702 - val_loss: 0.8794\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 0.7774 - val_loss: 0.8728\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 4s 5ms/step - loss: 0.7570 - val_loss: 0.8655\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_4_input'), name='lstm_4_input', description=\"created by layer 'lstm_4_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.018320</td>\n",
       "      <td>-0.009768</td>\n",
       "      <td>0.033719</td>\n",
       "      <td>-0.016597</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>0.010031</td>\n",
       "      <td>0.057112</td>\n",
       "      <td>0.095433</td>\n",
       "      <td>0.070377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.018526</td>\n",
       "      <td>-0.011063</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>-0.016425</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.011187</td>\n",
       "      <td>0.057775</td>\n",
       "      <td>0.091750</td>\n",
       "      <td>0.066787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.014829</td>\n",
       "      <td>-0.011801</td>\n",
       "      <td>0.028911</td>\n",
       "      <td>-0.018523</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>0.015231</td>\n",
       "      <td>0.073132</td>\n",
       "      <td>0.117845</td>\n",
       "      <td>0.079427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.014962</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>0.028726</td>\n",
       "      <td>-0.018262</td>\n",
       "      <td>-0.000502</td>\n",
       "      <td>0.014382</td>\n",
       "      <td>0.072208</td>\n",
       "      <td>0.120518</td>\n",
       "      <td>0.082034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.011460</td>\n",
       "      <td>-0.010032</td>\n",
       "      <td>0.024208</td>\n",
       "      <td>-0.020633</td>\n",
       "      <td>-0.002864</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.068588</td>\n",
       "      <td>0.132405</td>\n",
       "      <td>0.087032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.011484</td>\n",
       "      <td>-0.012271</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>-0.020175</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>0.071957</td>\n",
       "      <td>0.126841</td>\n",
       "      <td>0.080524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.008342</td>\n",
       "      <td>-0.012503</td>\n",
       "      <td>0.021081</td>\n",
       "      <td>-0.021591</td>\n",
       "      <td>-0.004355</td>\n",
       "      <td>0.018470</td>\n",
       "      <td>0.052656</td>\n",
       "      <td>0.114715</td>\n",
       "      <td>0.067151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.006674</td>\n",
       "      <td>-0.016360</td>\n",
       "      <td>0.024649</td>\n",
       "      <td>-0.020225</td>\n",
       "      <td>-0.005879</td>\n",
       "      <td>0.021435</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>0.106382</td>\n",
       "      <td>0.058749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.001467</td>\n",
       "      <td>-0.018113</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.012736</td>\n",
       "      <td>0.021479</td>\n",
       "      <td>0.034875</td>\n",
       "      <td>0.075055</td>\n",
       "      <td>0.041208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000328</td>\n",
       "      <td>-0.019410</td>\n",
       "      <td>0.029802</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>-0.013568</td>\n",
       "      <td>0.023139</td>\n",
       "      <td>0.043662</td>\n",
       "      <td>0.074502</td>\n",
       "      <td>0.044906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005588</td>\n",
       "      <td>-0.020199</td>\n",
       "      <td>0.030140</td>\n",
       "      <td>-0.013465</td>\n",
       "      <td>-0.022304</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>0.057197</td>\n",
       "      <td>0.098463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.005822</td>\n",
       "      <td>-0.020223</td>\n",
       "      <td>0.031851</td>\n",
       "      <td>-0.011336</td>\n",
       "      <td>-0.019646</td>\n",
       "      <td>0.023272</td>\n",
       "      <td>0.021620</td>\n",
       "      <td>0.058376</td>\n",
       "      <td>0.104822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.008677</td>\n",
       "      <td>-0.020632</td>\n",
       "      <td>0.026901</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.031019</td>\n",
       "      <td>0.017983</td>\n",
       "      <td>0.039557</td>\n",
       "      <td>0.135165</td>\n",
       "      <td>0.424844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008902</td>\n",
       "      <td>-0.020686</td>\n",
       "      <td>0.028504</td>\n",
       "      <td>-0.012695</td>\n",
       "      <td>-0.028263</td>\n",
       "      <td>0.020130</td>\n",
       "      <td>0.039709</td>\n",
       "      <td>0.135740</td>\n",
       "      <td>0.437862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.011312</td>\n",
       "      <td>-0.021489</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>-0.019241</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.226792</td>\n",
       "      <td>0.541086</td>\n",
       "      <td>1.535320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.264119</td>\n",
       "      <td>0.495739</td>\n",
       "      <td>0.674128</td>\n",
       "      <td>0.902048</td>\n",
       "      <td>1.047920</td>\n",
       "      <td>1.500841</td>\n",
       "      <td>2.034684</td>\n",
       "      <td>2.615589</td>\n",
       "      <td>5.133702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.072741</td>\n",
       "      <td>2.336011</td>\n",
       "      <td>3.130486</td>\n",
       "      <td>4.402256</td>\n",
       "      <td>5.309959</td>\n",
       "      <td>7.304523</td>\n",
       "      <td>11.356960</td>\n",
       "      <td>14.751767</td>\n",
       "      <td>23.926376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.942475</td>\n",
       "      <td>4.374655</td>\n",
       "      <td>5.956992</td>\n",
       "      <td>8.329583</td>\n",
       "      <td>9.918426</td>\n",
       "      <td>13.153889</td>\n",
       "      <td>17.420641</td>\n",
       "      <td>20.779972</td>\n",
       "      <td>30.774134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.854699</td>\n",
       "      <td>6.728202</td>\n",
       "      <td>9.450163</td>\n",
       "      <td>13.067003</td>\n",
       "      <td>15.378316</td>\n",
       "      <td>19.739746</td>\n",
       "      <td>27.678812</td>\n",
       "      <td>32.383354</td>\n",
       "      <td>45.449432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.939633</td>\n",
       "      <td>9.275344</td>\n",
       "      <td>13.170128</td>\n",
       "      <td>17.761337</td>\n",
       "      <td>20.245480</td>\n",
       "      <td>24.922413</td>\n",
       "      <td>30.494610</td>\n",
       "      <td>34.002228</td>\n",
       "      <td>46.085567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.915888</td>\n",
       "      <td>15.905630</td>\n",
       "      <td>22.824675</td>\n",
       "      <td>28.863770</td>\n",
       "      <td>30.576330</td>\n",
       "      <td>34.374794</td>\n",
       "      <td>39.488232</td>\n",
       "      <td>40.977833</td>\n",
       "      <td>49.224030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.200056</td>\n",
       "      <td>18.293543</td>\n",
       "      <td>25.990767</td>\n",
       "      <td>32.111149</td>\n",
       "      <td>32.966881</td>\n",
       "      <td>35.990849</td>\n",
       "      <td>40.117195</td>\n",
       "      <td>40.644146</td>\n",
       "      <td>45.738392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.407323</td>\n",
       "      <td>17.336258</td>\n",
       "      <td>25.192966</td>\n",
       "      <td>31.933836</td>\n",
       "      <td>34.416416</td>\n",
       "      <td>38.496105</td>\n",
       "      <td>46.924267</td>\n",
       "      <td>50.507477</td>\n",
       "      <td>61.142986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.967392</td>\n",
       "      <td>14.389499</td>\n",
       "      <td>20.967751</td>\n",
       "      <td>27.111248</td>\n",
       "      <td>29.764145</td>\n",
       "      <td>34.362354</td>\n",
       "      <td>45.002655</td>\n",
       "      <td>49.748482</td>\n",
       "      <td>62.335693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.808964</td>\n",
       "      <td>14.334302</td>\n",
       "      <td>20.897337</td>\n",
       "      <td>26.873413</td>\n",
       "      <td>29.254448</td>\n",
       "      <td>33.697369</td>\n",
       "      <td>44.851906</td>\n",
       "      <td>49.863873</td>\n",
       "      <td>61.423630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.081882</td>\n",
       "      <td>16.910042</td>\n",
       "      <td>24.446466</td>\n",
       "      <td>30.742262</td>\n",
       "      <td>32.603485</td>\n",
       "      <td>36.421150</td>\n",
       "      <td>45.349258</td>\n",
       "      <td>48.891109</td>\n",
       "      <td>58.018311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.806954</td>\n",
       "      <td>14.585413</td>\n",
       "      <td>21.218033</td>\n",
       "      <td>27.027863</td>\n",
       "      <td>29.005163</td>\n",
       "      <td>33.175629</td>\n",
       "      <td>42.217506</td>\n",
       "      <td>46.677853</td>\n",
       "      <td>56.449585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.828525</td>\n",
       "      <td>10.088699</td>\n",
       "      <td>14.839977</td>\n",
       "      <td>19.860422</td>\n",
       "      <td>22.499790</td>\n",
       "      <td>27.445484</td>\n",
       "      <td>41.701839</td>\n",
       "      <td>48.696514</td>\n",
       "      <td>60.961891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.373532</td>\n",
       "      <td>11.635606</td>\n",
       "      <td>17.027676</td>\n",
       "      <td>22.105349</td>\n",
       "      <td>24.033895</td>\n",
       "      <td>28.395443</td>\n",
       "      <td>36.105194</td>\n",
       "      <td>40.074772</td>\n",
       "      <td>49.067268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.839461</td>\n",
       "      <td>12.652713</td>\n",
       "      <td>18.372803</td>\n",
       "      <td>23.488434</td>\n",
       "      <td>25.086071</td>\n",
       "      <td>29.135933</td>\n",
       "      <td>35.702171</td>\n",
       "      <td>39.347977</td>\n",
       "      <td>47.683128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.328761</td>\n",
       "      <td>11.729706</td>\n",
       "      <td>16.993879</td>\n",
       "      <td>21.594444</td>\n",
       "      <td>22.700935</td>\n",
       "      <td>26.435886</td>\n",
       "      <td>28.733198</td>\n",
       "      <td>30.939457</td>\n",
       "      <td>36.958694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.866959</td>\n",
       "      <td>8.060787</td>\n",
       "      <td>11.837075</td>\n",
       "      <td>15.822600</td>\n",
       "      <td>17.561096</td>\n",
       "      <td>21.727287</td>\n",
       "      <td>27.373491</td>\n",
       "      <td>30.011728</td>\n",
       "      <td>37.217583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.385374</td>\n",
       "      <td>4.052506</td>\n",
       "      <td>6.014334</td>\n",
       "      <td>8.485521</td>\n",
       "      <td>9.929159</td>\n",
       "      <td>13.326152</td>\n",
       "      <td>17.573149</td>\n",
       "      <td>19.661575</td>\n",
       "      <td>25.812149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.336648</td>\n",
       "      <td>0.880131</td>\n",
       "      <td>1.302681</td>\n",
       "      <td>1.897597</td>\n",
       "      <td>2.305977</td>\n",
       "      <td>3.439399</td>\n",
       "      <td>7.480927</td>\n",
       "      <td>10.735545</td>\n",
       "      <td>16.216372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.023079</td>\n",
       "      <td>-0.019655</td>\n",
       "      <td>-0.003255</td>\n",
       "      <td>-0.012266</td>\n",
       "      <td>-0.067044</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.259501</td>\n",
       "      <td>0.341166</td>\n",
       "      <td>0.928638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.022003</td>\n",
       "      <td>-0.018988</td>\n",
       "      <td>-0.004034</td>\n",
       "      <td>-0.014074</td>\n",
       "      <td>-0.063722</td>\n",
       "      <td>0.009315</td>\n",
       "      <td>0.233116</td>\n",
       "      <td>0.314251</td>\n",
       "      <td>0.870704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.020892</td>\n",
       "      <td>-0.018374</td>\n",
       "      <td>-0.008166</td>\n",
       "      <td>-0.015656</td>\n",
       "      <td>-0.061980</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.052873</td>\n",
       "      <td>0.032155</td>\n",
       "      <td>0.213217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.020165</td>\n",
       "      <td>-0.017937</td>\n",
       "      <td>-0.008697</td>\n",
       "      <td>-0.016739</td>\n",
       "      <td>-0.059355</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.049062</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.202437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.019085</td>\n",
       "      <td>-0.017251</td>\n",
       "      <td>-0.013010</td>\n",
       "      <td>-0.018626</td>\n",
       "      <td>-0.059569</td>\n",
       "      <td>-0.006668</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.053057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.018369</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>-0.013577</td>\n",
       "      <td>-0.019679</td>\n",
       "      <td>-0.056922</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>0.050451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.017302</td>\n",
       "      <td>-0.016091</td>\n",
       "      <td>-0.018004</td>\n",
       "      <td>-0.021931</td>\n",
       "      <td>-0.058830</td>\n",
       "      <td>-0.018881</td>\n",
       "      <td>-0.006862</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.008685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.016869</td>\n",
       "      <td>-0.015987</td>\n",
       "      <td>-0.018230</td>\n",
       "      <td>-0.022325</td>\n",
       "      <td>-0.056330</td>\n",
       "      <td>-0.016591</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>0.017193</td>\n",
       "      <td>0.008185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.015837</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.022824</td>\n",
       "      <td>-0.025107</td>\n",
       "      <td>-0.060419</td>\n",
       "      <td>-0.033243</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>-0.006694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.015098</td>\n",
       "      <td>-0.014817</td>\n",
       "      <td>-0.023384</td>\n",
       "      <td>-0.026087</td>\n",
       "      <td>-0.056993</td>\n",
       "      <td>-0.029765</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.027856</td>\n",
       "      <td>-0.006112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.014058</td>\n",
       "      <td>-0.014156</td>\n",
       "      <td>-0.027935</td>\n",
       "      <td>-0.029163</td>\n",
       "      <td>-0.061582</td>\n",
       "      <td>-0.046556</td>\n",
       "      <td>0.015714</td>\n",
       "      <td>0.034597</td>\n",
       "      <td>-0.005556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.014006</td>\n",
       "      <td>-0.014239</td>\n",
       "      <td>-0.027929</td>\n",
       "      <td>-0.029134</td>\n",
       "      <td>-0.060981</td>\n",
       "      <td>-0.045462</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>0.034906</td>\n",
       "      <td>-0.005494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.013568</td>\n",
       "      <td>-0.014297</td>\n",
       "      <td>-0.031694</td>\n",
       "      <td>-0.031030</td>\n",
       "      <td>-0.067607</td>\n",
       "      <td>-0.063667</td>\n",
       "      <td>0.027830</td>\n",
       "      <td>0.040672</td>\n",
       "      <td>0.002630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.013568</td>\n",
       "      <td>-0.014297</td>\n",
       "      <td>-0.031694</td>\n",
       "      <td>-0.031030</td>\n",
       "      <td>-0.067607</td>\n",
       "      <td>-0.063667</td>\n",
       "      <td>0.027830</td>\n",
       "      <td>0.040672</td>\n",
       "      <td>0.002630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          0          0          0          0          0  \\\n",
       "0  -0.018320  -0.009768   0.033719  -0.016597  -0.000701   0.010031   \n",
       "1  -0.018526  -0.011063   0.033784  -0.016425  -0.000064   0.011187   \n",
       "2  -0.014829  -0.011801   0.028911  -0.018523  -0.000436   0.015231   \n",
       "3  -0.014962  -0.010795   0.028726  -0.018262  -0.000502   0.014382   \n",
       "4  -0.011460  -0.010032   0.024208  -0.020633  -0.002864   0.015210   \n",
       "5  -0.011484  -0.012271   0.024721  -0.020175  -0.001891   0.017570   \n",
       "6  -0.008342  -0.012503   0.021081  -0.021591  -0.004355   0.018470   \n",
       "7  -0.006674  -0.016360   0.024649  -0.020225  -0.005879   0.021435   \n",
       "8  -0.001467  -0.018113   0.025191  -0.019282  -0.012736   0.021479   \n",
       "9   0.000328  -0.019410   0.029802  -0.015309  -0.013568   0.023139   \n",
       "10  0.005588  -0.020199   0.030140  -0.013465  -0.022304   0.021243   \n",
       "11  0.005822  -0.020223   0.031851  -0.011336  -0.019646   0.023272   \n",
       "12  0.008677  -0.020632   0.026901  -0.014735  -0.031019   0.017983   \n",
       "13  0.008902  -0.020686   0.028504  -0.012695  -0.028263   0.020130   \n",
       "14  0.011312  -0.021489   0.021193  -0.019241  -0.046883   0.009940   \n",
       "15  0.264119   0.495739   0.674128   0.902048   1.047920   1.500841   \n",
       "16  1.072741   2.336011   3.130486   4.402256   5.309959   7.304523   \n",
       "17  1.942475   4.374655   5.956992   8.329583   9.918426  13.153889   \n",
       "18  2.854699   6.728202   9.450163  13.067003  15.378316  19.739746   \n",
       "19  3.939633   9.275344  13.170128  17.761337  20.245480  24.922413   \n",
       "20  6.915888  15.905630  22.824675  28.863770  30.576330  34.374794   \n",
       "21  8.200056  18.293543  25.990767  32.111149  32.966881  35.990849   \n",
       "22  7.407323  17.336258  25.192966  31.933836  34.416416  38.496105   \n",
       "23  5.967392  14.389499  20.967751  27.111248  29.764145  34.362354   \n",
       "24  5.808964  14.334302  20.897337  26.873413  29.254448  33.697369   \n",
       "25  7.081882  16.910042  24.446466  30.742262  32.603485  36.421150   \n",
       "26  5.806954  14.585413  21.218033  27.027863  29.005163  33.175629   \n",
       "27  3.828525  10.088699  14.839977  19.860422  22.499790  27.445484   \n",
       "28  4.373532  11.635606  17.027676  22.105349  24.033895  28.395443   \n",
       "29  4.839461  12.652713  18.372803  23.488434  25.086071  29.135933   \n",
       "30  4.328761  11.729706  16.993879  21.594444  22.700935  26.435886   \n",
       "31  2.866959   8.060787  11.837075  15.822600  17.561096  21.727287   \n",
       "32  1.385374   4.052506   6.014334   8.485521   9.929159  13.326152   \n",
       "33  0.336648   0.880131   1.302681   1.897597   2.305977   3.439399   \n",
       "34  0.023079  -0.019655  -0.003255  -0.012266  -0.067044   0.010050   \n",
       "35  0.022003  -0.018988  -0.004034  -0.014074  -0.063722   0.009315   \n",
       "36  0.020892  -0.018374  -0.008166  -0.015656  -0.061980   0.002859   \n",
       "37  0.020165  -0.017937  -0.008697  -0.016739  -0.059355   0.003199   \n",
       "38  0.019085  -0.017251  -0.013010  -0.018626  -0.059569  -0.006668   \n",
       "39  0.018369  -0.016789  -0.013577  -0.019679  -0.056922  -0.005747   \n",
       "40  0.017302  -0.016091  -0.018004  -0.021931  -0.058830  -0.018881   \n",
       "41  0.016869  -0.015987  -0.018230  -0.022325  -0.056330  -0.016591   \n",
       "42  0.015837  -0.015257  -0.022824  -0.025107  -0.060419  -0.033243   \n",
       "43  0.015098  -0.014817  -0.023384  -0.026087  -0.056993  -0.029765   \n",
       "44  0.014058  -0.014156  -0.027935  -0.029163  -0.061582  -0.046556   \n",
       "45  0.014006  -0.014239  -0.027929  -0.029134  -0.060981  -0.045462   \n",
       "46  0.013568  -0.014297  -0.031694  -0.031030  -0.067607  -0.063667   \n",
       "47  0.013568  -0.014297  -0.031694  -0.031030  -0.067607  -0.063667   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.057112   0.095433   0.070377  \n",
       "1    0.057775   0.091750   0.066787  \n",
       "2    0.073132   0.117845   0.079427  \n",
       "3    0.072208   0.120518   0.082034  \n",
       "4    0.068588   0.132405   0.087032  \n",
       "5    0.071957   0.126841   0.080524  \n",
       "6    0.052656   0.114715   0.067151  \n",
       "7    0.061731   0.106382   0.058749  \n",
       "8    0.034875   0.075055   0.041208  \n",
       "9    0.043662   0.074502   0.044906  \n",
       "10   0.017968   0.057197   0.098463  \n",
       "11   0.021620   0.058376   0.104822  \n",
       "12   0.039557   0.135165   0.424844  \n",
       "13   0.039709   0.135740   0.437862  \n",
       "14   0.226792   0.541086   1.535320  \n",
       "15   2.034684   2.615589   5.133702  \n",
       "16  11.356960  14.751767  23.926376  \n",
       "17  17.420641  20.779972  30.774134  \n",
       "18  27.678812  32.383354  45.449432  \n",
       "19  30.494610  34.002228  46.085567  \n",
       "20  39.488232  40.977833  49.224030  \n",
       "21  40.117195  40.644146  45.738392  \n",
       "22  46.924267  50.507477  61.142986  \n",
       "23  45.002655  49.748482  62.335693  \n",
       "24  44.851906  49.863873  61.423630  \n",
       "25  45.349258  48.891109  58.018311  \n",
       "26  42.217506  46.677853  56.449585  \n",
       "27  41.701839  48.696514  60.961891  \n",
       "28  36.105194  40.074772  49.067268  \n",
       "29  35.702171  39.347977  47.683128  \n",
       "30  28.733198  30.939457  36.958694  \n",
       "31  27.373491  30.011728  37.217583  \n",
       "32  17.573149  19.661575  25.812149  \n",
       "33   7.480927  10.735545  16.216372  \n",
       "34   0.259501   0.341166   0.928638  \n",
       "35   0.233116   0.314251   0.870704  \n",
       "36   0.052873   0.032155   0.213217  \n",
       "37   0.049062   0.030683   0.202437  \n",
       "38  -0.001219   0.002403   0.053057  \n",
       "39  -0.000059   0.004178   0.050451  \n",
       "40  -0.006862   0.015500   0.008685  \n",
       "41  -0.003742   0.017193   0.008185  \n",
       "42   0.001940   0.026257  -0.006694  \n",
       "43   0.004939   0.027856  -0.006112  \n",
       "44   0.015714   0.034597  -0.005556  \n",
       "45   0.016420   0.034906  -0.005494  \n",
       "46   0.027830   0.040672   0.002630  \n",
       "47   0.027830   0.040672   0.002630  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "model_M7 = tf.keras.Sequential([\n",
    "    layers.LSTM(units=64, return_sequences=True, input_shape=[52464, 8]),\n",
    "    layers.LSTM(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_M7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_M7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_M7.fit(np.array(Day0).reshape(52464, 1, 8), np.array(Day7).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_M7 = np.squeeze(model_M7.predict(np.array(df_test0).reshape(3888, 1, 8)))\n",
    "    pred_M7 = pd.DataFrame(pred_M7)\n",
    "    result_M7 = pd.concat([result_M7, pred_M7], axis=1)\n",
    "    \n",
    "result_M7[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "817/820 [============================>.] - ETA: 0s - loss: 1.5999WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 10s 10ms/step - loss: 1.5995 - val_loss: 1.6560\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4534 - val_loss: 1.6640\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4358 - val_loss: 1.6687\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4213 - val_loss: 1.6397\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4265 - val_loss: 1.6489\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4129 - val_loss: 1.6561\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4187 - val_loss: 1.6593\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.2\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "815/820 [============================>.] - ETA: 0s - loss: 2.4295WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 8s 6ms/step - loss: 2.4294 - val_loss: 2.7469\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.3756 - val_loss: 2.7660\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.3442 - val_loss: 2.7823\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.3249 - val_loss: 2.6883\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.3292 - val_loss: 2.7024\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.3191 - val_loss: 2.7632\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.3324 - val_loss: 2.7470\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.3\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "811/820 [============================>.] - ETA: 0s - loss: 2.8957WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 8s 7ms/step - loss: 2.8955 - val_loss: 3.2848\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8571 - val_loss: 3.2366\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8163 - val_loss: 3.2456\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8117 - val_loss: 3.1717\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8118 - val_loss: 3.1689\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.7883 - val_loss: 3.2789\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8161 - val_loss: 3.1917\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8138 - val_loss: 3.1617\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8234 - val_loss: 3.1749\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8219 - val_loss: 3.3082\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.7659 - val_loss: 3.2159\n",
      "Epoch 00011: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.4\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "814/820 [============================>.] - ETA: 0s - loss: 3.0216WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 8s 6ms/step - loss: 3.0213 - val_loss: 3.3789\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.9809 - val_loss: 3.3441\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.9546 - val_loss: 3.3441\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.9571 - val_loss: 3.3463\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 6s 8ms/step - loss: 2.9512 - val_loss: 3.3326\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 6s 8ms/step - loss: 2.9218 - val_loss: 3.3490\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 5s 7ms/step - loss: 2.9730 - val_loss: 3.3243\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 6s 8ms/step - loss: 2.9658 - val_loss: 3.3370\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.9571 - val_loss: 3.3493\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.9638 - val_loss: 3.3678\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.5\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 2.9156WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 10s 8ms/step - loss: 2.9155 - val_loss: 3.2617\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 7ms/step - loss: 2.8825 - val_loss: 3.2674\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.8661 - val_loss: 3.2432\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8680 - val_loss: 3.2592\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.8635 - val_loss: 3.2717\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8281 - val_loss: 3.2360\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8885 - val_loss: 3.2643\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8742 - val_loss: 3.2799\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.8546 - val_loss: 3.2650\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.6\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 2.6393WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 9s 7ms/step - loss: 2.6392 - val_loss: 2.9767\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.6022 - val_loss: 2.9950\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.5907 - val_loss: 2.9379\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.5915 - val_loss: 2.9791\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.5859 - val_loss: 2.9760\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.5542 - val_loss: 2.9481\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.7\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "812/820 [============================>.] - ETA: 0s - loss: 2.2165WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 9s 7ms/step - loss: 2.2163 - val_loss: 2.5245\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.1807 - val_loss: 2.5067\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.1716 - val_loss: 2.5281\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.1581 - val_loss: 2.4942\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 2.1603 - val_loss: 2.4863\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 5s 7ms/step - loss: 2.1261 - val_loss: 2.4480\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 5s 7ms/step - loss: 2.1677 - val_loss: 2.4859\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 5s 7ms/step - loss: 2.1455 - val_loss: 2.4708\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 5s 7ms/step - loss: 2.1144 - val_loss: 2.4257\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 5s 7ms/step - loss: 2.1267 - val_loss: 2.4306\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.0809 - val_loss: 2.4179\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.1035 - val_loss: 2.3894\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 6s 8ms/step - loss: 2.0881 - val_loss: 2.3777\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.0713 - val_loss: 2.3908\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.0725 - val_loss: 2.4015\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 6s 7ms/step - loss: 2.0358 - val_loss: 2.3962\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.8\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - ETA: 0s - loss: 1.5392WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 9s 7ms/step - loss: 1.5392 - val_loss: 1.7341\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.5122 - val_loss: 1.7460\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.5074 - val_loss: 1.7552\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4999 - val_loss: 1.6819\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4973 - val_loss: 1.6797\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4663 - val_loss: 1.6672\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4978 - val_loss: 1.6602\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4800 - val_loss: 1.7101\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4563 - val_loss: 1.6644\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 1.4782 - val_loss: 1.6823\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "0.9\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 0.8373WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
      "820/820 [==============================] - 8s 7ms/step - loss: 0.8372 - val_loss: 0.9282\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 0.8226 - val_loss: 0.9603\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 0.8177 - val_loss: 0.9303\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 0.8178 - val_loss: 0.9197\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 0.8125 - val_loss: 0.9304\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 0.7993 - val_loss: 0.9425\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 0.8156 - val_loss: 0.9047\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 0.8084 - val_loss: 0.9229\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 0.7971 - val_loss: 0.9065\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 5s 6ms/step - loss: 0.8090 - val_loss: 0.9354\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 8), dtype=tf.float32, name='lstm_6_input'), name='lstm_6_input', description=\"created by layer 'lstm_6_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041528</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.016066</td>\n",
       "      <td>-0.017140</td>\n",
       "      <td>-0.085209</td>\n",
       "      <td>0.030080</td>\n",
       "      <td>0.084629</td>\n",
       "      <td>0.028655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.041507</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>0.016043</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>-0.082663</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.084478</td>\n",
       "      <td>0.028239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.038472</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.020663</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>-0.074202</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>0.119472</td>\n",
       "      <td>0.033769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.038721</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.075112</td>\n",
       "      <td>0.047003</td>\n",
       "      <td>0.119001</td>\n",
       "      <td>0.033892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.035930</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.025469</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>-0.069133</td>\n",
       "      <td>0.067605</td>\n",
       "      <td>0.168484</td>\n",
       "      <td>0.038271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.035851</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>0.018096</td>\n",
       "      <td>-0.066619</td>\n",
       "      <td>0.069808</td>\n",
       "      <td>0.169381</td>\n",
       "      <td>0.037673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.033524</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.028151</td>\n",
       "      <td>0.031564</td>\n",
       "      <td>-0.059831</td>\n",
       "      <td>0.095915</td>\n",
       "      <td>0.238365</td>\n",
       "      <td>0.036981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.032639</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>0.049007</td>\n",
       "      <td>-0.058177</td>\n",
       "      <td>0.104937</td>\n",
       "      <td>0.248831</td>\n",
       "      <td>0.038750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.029885</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>0.073080</td>\n",
       "      <td>-0.052483</td>\n",
       "      <td>0.147891</td>\n",
       "      <td>0.365669</td>\n",
       "      <td>0.033603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.029366</td>\n",
       "      <td>-0.001399</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>0.028703</td>\n",
       "      <td>0.084407</td>\n",
       "      <td>-0.050417</td>\n",
       "      <td>0.156691</td>\n",
       "      <td>0.376725</td>\n",
       "      <td>0.036197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.001101</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.030941</td>\n",
       "      <td>0.106290</td>\n",
       "      <td>-0.042806</td>\n",
       "      <td>0.214321</td>\n",
       "      <td>0.555393</td>\n",
       "      <td>0.028220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.001069</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.108701</td>\n",
       "      <td>-0.040352</td>\n",
       "      <td>0.214508</td>\n",
       "      <td>0.553944</td>\n",
       "      <td>0.028364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.025513</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.031877</td>\n",
       "      <td>0.121943</td>\n",
       "      <td>-0.033318</td>\n",
       "      <td>0.285069</td>\n",
       "      <td>0.810288</td>\n",
       "      <td>0.065609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.025494</td>\n",
       "      <td>-0.000602</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.032157</td>\n",
       "      <td>0.124284</td>\n",
       "      <td>-0.031204</td>\n",
       "      <td>0.284095</td>\n",
       "      <td>0.805341</td>\n",
       "      <td>0.062729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.024244</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.005877</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.136935</td>\n",
       "      <td>-0.023819</td>\n",
       "      <td>0.399394</td>\n",
       "      <td>1.245431</td>\n",
       "      <td>0.308215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.163705</td>\n",
       "      <td>0.494363</td>\n",
       "      <td>0.601693</td>\n",
       "      <td>1.041145</td>\n",
       "      <td>1.500598</td>\n",
       "      <td>2.120004</td>\n",
       "      <td>3.790680</td>\n",
       "      <td>6.150227</td>\n",
       "      <td>8.203325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.734207</td>\n",
       "      <td>2.128567</td>\n",
       "      <td>2.832492</td>\n",
       "      <td>4.939633</td>\n",
       "      <td>6.857623</td>\n",
       "      <td>10.415841</td>\n",
       "      <td>17.162670</td>\n",
       "      <td>22.688566</td>\n",
       "      <td>27.188646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.365765</td>\n",
       "      <td>3.922879</td>\n",
       "      <td>5.464826</td>\n",
       "      <td>9.341690</td>\n",
       "      <td>12.560125</td>\n",
       "      <td>18.327480</td>\n",
       "      <td>25.028303</td>\n",
       "      <td>28.206825</td>\n",
       "      <td>31.369656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.014142</td>\n",
       "      <td>5.884316</td>\n",
       "      <td>8.633399</td>\n",
       "      <td>14.371058</td>\n",
       "      <td>18.709196</td>\n",
       "      <td>26.057796</td>\n",
       "      <td>34.140823</td>\n",
       "      <td>39.545776</td>\n",
       "      <td>44.381779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.881365</td>\n",
       "      <td>8.102743</td>\n",
       "      <td>12.145566</td>\n",
       "      <td>19.254927</td>\n",
       "      <td>23.877275</td>\n",
       "      <td>31.638023</td>\n",
       "      <td>37.019367</td>\n",
       "      <td>40.691109</td>\n",
       "      <td>44.075199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.811303</td>\n",
       "      <td>14.109083</td>\n",
       "      <td>21.780197</td>\n",
       "      <td>30.765471</td>\n",
       "      <td>34.016811</td>\n",
       "      <td>40.795864</td>\n",
       "      <td>41.763218</td>\n",
       "      <td>45.237823</td>\n",
       "      <td>46.935425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.431507</td>\n",
       "      <td>16.472395</td>\n",
       "      <td>25.372099</td>\n",
       "      <td>34.218544</td>\n",
       "      <td>36.105286</td>\n",
       "      <td>41.835892</td>\n",
       "      <td>41.249744</td>\n",
       "      <td>44.111916</td>\n",
       "      <td>44.791996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.273179</td>\n",
       "      <td>15.135388</td>\n",
       "      <td>23.796743</td>\n",
       "      <td>34.090336</td>\n",
       "      <td>38.220592</td>\n",
       "      <td>45.829922</td>\n",
       "      <td>49.055729</td>\n",
       "      <td>54.976547</td>\n",
       "      <td>59.119770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.662014</td>\n",
       "      <td>12.351225</td>\n",
       "      <td>19.290428</td>\n",
       "      <td>28.663797</td>\n",
       "      <td>33.311131</td>\n",
       "      <td>41.311726</td>\n",
       "      <td>46.520962</td>\n",
       "      <td>53.719425</td>\n",
       "      <td>59.650356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.498437</td>\n",
       "      <td>12.136342</td>\n",
       "      <td>18.993401</td>\n",
       "      <td>28.219103</td>\n",
       "      <td>32.713757</td>\n",
       "      <td>40.526493</td>\n",
       "      <td>45.483704</td>\n",
       "      <td>53.010464</td>\n",
       "      <td>59.550854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.975561</td>\n",
       "      <td>14.583498</td>\n",
       "      <td>22.823370</td>\n",
       "      <td>32.574348</td>\n",
       "      <td>36.256645</td>\n",
       "      <td>43.469482</td>\n",
       "      <td>46.375790</td>\n",
       "      <td>52.477821</td>\n",
       "      <td>56.533741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.515851</td>\n",
       "      <td>12.225721</td>\n",
       "      <td>19.160999</td>\n",
       "      <td>28.279984</td>\n",
       "      <td>32.513809</td>\n",
       "      <td>40.035805</td>\n",
       "      <td>43.463879</td>\n",
       "      <td>49.646496</td>\n",
       "      <td>55.124012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.634353</td>\n",
       "      <td>8.281919</td>\n",
       "      <td>13.020762</td>\n",
       "      <td>20.800072</td>\n",
       "      <td>25.858343</td>\n",
       "      <td>33.983849</td>\n",
       "      <td>41.909557</td>\n",
       "      <td>50.748085</td>\n",
       "      <td>59.927212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.103366</td>\n",
       "      <td>9.494406</td>\n",
       "      <td>14.946491</td>\n",
       "      <td>22.910091</td>\n",
       "      <td>27.318787</td>\n",
       "      <td>34.717789</td>\n",
       "      <td>37.884441</td>\n",
       "      <td>42.442059</td>\n",
       "      <td>47.340366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.583545</td>\n",
       "      <td>10.475468</td>\n",
       "      <td>16.401102</td>\n",
       "      <td>24.546824</td>\n",
       "      <td>28.593285</td>\n",
       "      <td>35.703087</td>\n",
       "      <td>37.639545</td>\n",
       "      <td>41.975739</td>\n",
       "      <td>46.276333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.087681</td>\n",
       "      <td>9.518919</td>\n",
       "      <td>14.901230</td>\n",
       "      <td>22.243767</td>\n",
       "      <td>25.818138</td>\n",
       "      <td>32.206387</td>\n",
       "      <td>31.293978</td>\n",
       "      <td>32.851269</td>\n",
       "      <td>35.022396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.868953</td>\n",
       "      <td>6.504136</td>\n",
       "      <td>10.268718</td>\n",
       "      <td>16.486364</td>\n",
       "      <td>20.491331</td>\n",
       "      <td>27.074900</td>\n",
       "      <td>29.371634</td>\n",
       "      <td>31.224209</td>\n",
       "      <td>34.205914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.809692</td>\n",
       "      <td>3.212546</td>\n",
       "      <td>5.106972</td>\n",
       "      <td>8.887909</td>\n",
       "      <td>11.896382</td>\n",
       "      <td>16.975452</td>\n",
       "      <td>20.083115</td>\n",
       "      <td>19.681376</td>\n",
       "      <td>21.461214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.164436</td>\n",
       "      <td>0.712153</td>\n",
       "      <td>1.096297</td>\n",
       "      <td>1.996767</td>\n",
       "      <td>2.970510</td>\n",
       "      <td>4.452457</td>\n",
       "      <td>8.914912</td>\n",
       "      <td>12.319630</td>\n",
       "      <td>17.087610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.016487</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.013321</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.051712</td>\n",
       "      <td>0.531222</td>\n",
       "      <td>1.430376</td>\n",
       "      <td>0.168569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.016617</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.023538</td>\n",
       "      <td>0.012023</td>\n",
       "      <td>0.180779</td>\n",
       "      <td>0.040885</td>\n",
       "      <td>0.499768</td>\n",
       "      <td>1.372622</td>\n",
       "      <td>0.161684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.016206</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.023962</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.164164</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.337528</td>\n",
       "      <td>0.886785</td>\n",
       "      <td>0.081286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.016296</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.023331</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.157046</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.321467</td>\n",
       "      <td>0.858932</td>\n",
       "      <td>0.078351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.015845</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.023754</td>\n",
       "      <td>-0.006658</td>\n",
       "      <td>0.139430</td>\n",
       "      <td>-0.005214</td>\n",
       "      <td>0.218347</td>\n",
       "      <td>0.566952</td>\n",
       "      <td>0.075679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.015937</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>-0.007182</td>\n",
       "      <td>0.132655</td>\n",
       "      <td>-0.010286</td>\n",
       "      <td>0.207120</td>\n",
       "      <td>0.547564</td>\n",
       "      <td>0.072850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.015441</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.023509</td>\n",
       "      <td>-0.017168</td>\n",
       "      <td>0.114629</td>\n",
       "      <td>-0.029525</td>\n",
       "      <td>0.151585</td>\n",
       "      <td>0.371098</td>\n",
       "      <td>0.078401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.015507</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.023005</td>\n",
       "      <td>-0.016919</td>\n",
       "      <td>0.111454</td>\n",
       "      <td>-0.030944</td>\n",
       "      <td>0.146305</td>\n",
       "      <td>0.361779</td>\n",
       "      <td>0.076389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.014961</td>\n",
       "      <td>-0.000951</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>-0.027308</td>\n",
       "      <td>0.093670</td>\n",
       "      <td>-0.048224</td>\n",
       "      <td>0.125090</td>\n",
       "      <td>0.254564</td>\n",
       "      <td>0.079670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.015071</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>0.022711</td>\n",
       "      <td>-0.027115</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>-0.049247</td>\n",
       "      <td>0.119385</td>\n",
       "      <td>0.244170</td>\n",
       "      <td>0.077199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.014493</td>\n",
       "      <td>-0.002407</td>\n",
       "      <td>0.023060</td>\n",
       "      <td>-0.036813</td>\n",
       "      <td>0.072113</td>\n",
       "      <td>-0.061266</td>\n",
       "      <td>0.120308</td>\n",
       "      <td>0.180855</td>\n",
       "      <td>0.081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.014511</td>\n",
       "      <td>-0.002379</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>-0.036601</td>\n",
       "      <td>0.072205</td>\n",
       "      <td>-0.060612</td>\n",
       "      <td>0.119864</td>\n",
       "      <td>0.180047</td>\n",
       "      <td>0.081042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.013833</td>\n",
       "      <td>-0.004601</td>\n",
       "      <td>0.023774</td>\n",
       "      <td>-0.045343</td>\n",
       "      <td>0.062437</td>\n",
       "      <td>-0.068863</td>\n",
       "      <td>0.133734</td>\n",
       "      <td>0.146622</td>\n",
       "      <td>0.088492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.013833</td>\n",
       "      <td>-0.004601</td>\n",
       "      <td>0.023774</td>\n",
       "      <td>-0.045343</td>\n",
       "      <td>0.062437</td>\n",
       "      <td>-0.068863</td>\n",
       "      <td>0.133734</td>\n",
       "      <td>0.146622</td>\n",
       "      <td>0.088492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          0          0          0          0          0  \\\n",
       "0  -0.041528   0.000302   0.008078   0.016066  -0.017140  -0.085209   \n",
       "1  -0.041507  -0.000265   0.007320   0.016043  -0.012500  -0.082663   \n",
       "2  -0.038472   0.000468   0.004296   0.020663   0.003450  -0.074202   \n",
       "3  -0.038721   0.001055   0.004686   0.020842  -0.000209  -0.075112   \n",
       "4  -0.035930   0.002635   0.004033   0.025469   0.010456  -0.069133   \n",
       "5  -0.035851   0.001361   0.002574   0.024645   0.018096  -0.066619   \n",
       "6  -0.033524   0.002369   0.001964   0.028151   0.031564  -0.059831   \n",
       "7  -0.032639  -0.000187   0.000117   0.026597   0.049007  -0.058177   \n",
       "8  -0.029885  -0.000476  -0.000193   0.028773   0.073080  -0.052483   \n",
       "9  -0.029366  -0.001399  -0.000954   0.028703   0.084407  -0.050417   \n",
       "10 -0.027023  -0.001101   0.000171   0.030941   0.106290  -0.042806   \n",
       "11 -0.027023  -0.001069  -0.000470   0.031179   0.108701  -0.040352   \n",
       "12 -0.025513  -0.000538   0.002177   0.031877   0.121943  -0.033318   \n",
       "13 -0.025494  -0.000602   0.001564   0.032157   0.124284  -0.031204   \n",
       "14 -0.024244  -0.000047   0.005877   0.031397   0.136935  -0.023819   \n",
       "15  0.163705   0.494363   0.601693   1.041145   1.500598   2.120004   \n",
       "16  0.734207   2.128567   2.832492   4.939633   6.857623  10.415841   \n",
       "17  1.365765   3.922879   5.464826   9.341690  12.560125  18.327480   \n",
       "18  2.014142   5.884316   8.633399  14.371058  18.709196  26.057796   \n",
       "19  2.881365   8.102743  12.145566  19.254927  23.877275  31.638023   \n",
       "20  5.811303  14.109083  21.780197  30.765471  34.016811  40.795864   \n",
       "21  7.431507  16.472395  25.372099  34.218544  36.105286  41.835892   \n",
       "22  6.273179  15.135388  23.796743  34.090336  38.220592  45.829922   \n",
       "23  4.662014  12.351225  19.290428  28.663797  33.311131  41.311726   \n",
       "24  4.498437  12.136342  18.993401  28.219103  32.713757  40.526493   \n",
       "25  5.975561  14.583498  22.823370  32.574348  36.256645  43.469482   \n",
       "26  4.515851  12.225721  19.160999  28.279984  32.513809  40.035805   \n",
       "27  2.634353   8.281919  13.020762  20.800072  25.858343  33.983849   \n",
       "28  3.103366   9.494406  14.946491  22.910091  27.318787  34.717789   \n",
       "29  3.583545  10.475468  16.401102  24.546824  28.593285  35.703087   \n",
       "30  3.087681   9.518919  14.901230  22.243767  25.818138  32.206387   \n",
       "31  1.868953   6.504136  10.268718  16.486364  20.491331  27.074900   \n",
       "32  0.809692   3.212546   5.106972   8.887909  11.896382  16.975452   \n",
       "33  0.164436   0.712153   1.096297   1.996767   2.970510   4.452457   \n",
       "34 -0.016487   0.003272   0.024401   0.013321   0.191600   0.051712   \n",
       "35 -0.016617   0.003443   0.023538   0.012023   0.180779   0.040885   \n",
       "36 -0.016206   0.002598   0.023962   0.003491   0.164164   0.021197   \n",
       "37 -0.016296   0.002733   0.023331   0.002821   0.157046   0.014822   \n",
       "38 -0.015845   0.001641   0.023754  -0.006658   0.139430  -0.005214   \n",
       "39 -0.015937   0.001803   0.023112  -0.007182   0.132655  -0.010286   \n",
       "40 -0.015441   0.000475   0.023509  -0.017168   0.114629  -0.029525   \n",
       "41 -0.015507   0.000583   0.023005  -0.016919   0.111454  -0.030944   \n",
       "42 -0.014961  -0.000951   0.023484  -0.027308   0.093670  -0.048224   \n",
       "43 -0.015071  -0.000705   0.022711  -0.027115   0.088210  -0.049247   \n",
       "44 -0.014493  -0.002407   0.023060  -0.036813   0.072113  -0.061266   \n",
       "45 -0.014511  -0.002379   0.022964  -0.036601   0.072205  -0.060612   \n",
       "46 -0.013833  -0.004601   0.023774  -0.045343   0.062437  -0.068863   \n",
       "47 -0.013833  -0.004601   0.023774  -0.045343   0.062437  -0.068863   \n",
       "\n",
       "            0          0          0  \n",
       "0    0.030080   0.084629   0.028655  \n",
       "1    0.030907   0.084478   0.028239  \n",
       "2    0.047721   0.119472   0.033769  \n",
       "3    0.047003   0.119001   0.033892  \n",
       "4    0.067605   0.168484   0.038271  \n",
       "5    0.069808   0.169381   0.037673  \n",
       "6    0.095915   0.238365   0.036981  \n",
       "7    0.104937   0.248831   0.038750  \n",
       "8    0.147891   0.365669   0.033603  \n",
       "9    0.156691   0.376725   0.036197  \n",
       "10   0.214321   0.555393   0.028220  \n",
       "11   0.214508   0.553944   0.028364  \n",
       "12   0.285069   0.810288   0.065609  \n",
       "13   0.284095   0.805341   0.062729  \n",
       "14   0.399394   1.245431   0.308215  \n",
       "15   3.790680   6.150227   8.203325  \n",
       "16  17.162670  22.688566  27.188646  \n",
       "17  25.028303  28.206825  31.369656  \n",
       "18  34.140823  39.545776  44.381779  \n",
       "19  37.019367  40.691109  44.075199  \n",
       "20  41.763218  45.237823  46.935425  \n",
       "21  41.249744  44.111916  44.791996  \n",
       "22  49.055729  54.976547  59.119770  \n",
       "23  46.520962  53.719425  59.650356  \n",
       "24  45.483704  53.010464  59.550854  \n",
       "25  46.375790  52.477821  56.533741  \n",
       "26  43.463879  49.646496  55.124012  \n",
       "27  41.909557  50.748085  59.927212  \n",
       "28  37.884441  42.442059  47.340366  \n",
       "29  37.639545  41.975739  46.276333  \n",
       "30  31.293978  32.851269  35.022396  \n",
       "31  29.371634  31.224209  34.205914  \n",
       "32  20.083115  19.681376  21.461214  \n",
       "33   8.914912  12.319630  17.087610  \n",
       "34   0.531222   1.430376   0.168569  \n",
       "35   0.499768   1.372622   0.161684  \n",
       "36   0.337528   0.886785   0.081286  \n",
       "37   0.321467   0.858932   0.078351  \n",
       "38   0.218347   0.566952   0.075679  \n",
       "39   0.207120   0.547564   0.072850  \n",
       "40   0.151585   0.371098   0.078401  \n",
       "41   0.146305   0.361779   0.076389  \n",
       "42   0.125090   0.254564   0.079670  \n",
       "43   0.119385   0.244170   0.077199  \n",
       "44   0.120308   0.180855   0.081201  \n",
       "45   0.119864   0.180047   0.081042  \n",
       "46   0.133734   0.146622   0.088492  \n",
       "47   0.133734   0.146622   0.088492  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_M8 = tf.keras.Sequential([\n",
    "    layers.LSTM(units=64, return_sequences=True, input_shape=[52464, 8]),\n",
    "    layers.LSTM(units=32),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result_M8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model_M8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model_M8.fit(np.array(Day0).reshape(52464, 1, 8), np.array(Day8).reshape(52464, 1)\n",
    "                 , epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred_M8 = np.squeeze(model_M8.predict(np.array(df_test0).reshape(3888, 1, 8)))\n",
    "    pred_M8 = pd.DataFrame(pred_M8)\n",
    "    result_M8 = pd.concat([result_M8, pred_M8], axis=1)\n",
    "    \n",
    "result_M8[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_L0 = pd.DataFrame(results_1.sort_index())\n",
    "res_L0.columns = ['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']\n",
    "res_L1 = pd.DataFrame(results_2.sort_index())\n",
    "res_L1.columns = ['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']\n",
    "\n",
    "res_D0 = pd.DataFrame(results[0].sort_index())\n",
    "res_D0.columns = ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9']\n",
    "res_D1 = pd.DataFrame(results[1].sort_index())\n",
    "res_D1.columns = ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9']\n",
    "\n",
    "res_C0 = pd.DataFrame(result7.sort_index())\n",
    "res_C0.columns = ['C00.1','C00.2','C00.3','C00.4','C00.5','C00.6','C00.7','C00.8','C00.9']\n",
    "res_C1 = pd.DataFrame(result8.sort_index())\n",
    "res_C1.columns = ['C10.1','C10.2','C10.3','C10.4','C10.5','C10.6','C10.7','C10.8','C10.9']\n",
    "\n",
    "res_G0 = pd.DataFrame(result_G7.sort_index())\n",
    "res_G0.columns = ['G00.1','G00.2','G00.3','G00.4','G00.5','G00.6','G00.7','G00.8','G00.9']\n",
    "res_G1 = pd.DataFrame(result_G8.sort_index())\n",
    "res_G1.columns = ['G10.1','G10.2','G10.3','G10.4','G10.5','G10.6','G10.7','G10.8','G10.9']\n",
    "\n",
    "res_M0 = pd.DataFrame(result_M7.sort_index())\n",
    "res_M0.columns = ['M00.1','M00.2','M00.3','M00.4','M00.5','M00.6','M00.7','M00.8','M00.9']\n",
    "res_M1 = pd.DataFrame(result_M8.sort_index())\n",
    "res_M1.columns = ['M10.1','M10.2','M10.3','M10.4','M10.5','M10.6','M10.7','M10.8','M10.9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0 = pd.DataFrame()\n",
    "res_1= pd.DataFrame()\n",
    "res_0 = pd.concat([res_L0, res_D0, res_C0, res_G0, res_M0], axis=1)\n",
    "res_1 = pd.concat([res_L1, res_D1, res_C1, res_G1, res_M1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L00.1</th>\n",
       "      <th>L00.2</th>\n",
       "      <th>L00.3</th>\n",
       "      <th>L00.4</th>\n",
       "      <th>L00.5</th>\n",
       "      <th>L00.6</th>\n",
       "      <th>L00.7</th>\n",
       "      <th>L00.8</th>\n",
       "      <th>L00.9</th>\n",
       "      <th>D00.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G00.9</th>\n",
       "      <th>M00.1</th>\n",
       "      <th>M00.2</th>\n",
       "      <th>M00.3</th>\n",
       "      <th>M00.4</th>\n",
       "      <th>M00.5</th>\n",
       "      <th>M00.6</th>\n",
       "      <th>M00.7</th>\n",
       "      <th>M00.8</th>\n",
       "      <th>M00.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.095921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059470</td>\n",
       "      <td>-0.018320</td>\n",
       "      <td>-0.009768</td>\n",
       "      <td>0.033719</td>\n",
       "      <td>-0.016597</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>0.010031</td>\n",
       "      <td>0.057112</td>\n",
       "      <td>0.095433</td>\n",
       "      <td>0.070377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.093247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059932</td>\n",
       "      <td>-0.018526</td>\n",
       "      <td>-0.011063</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>-0.016425</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.011187</td>\n",
       "      <td>0.057775</td>\n",
       "      <td>0.091750</td>\n",
       "      <td>0.066787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.092981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065703</td>\n",
       "      <td>-0.014829</td>\n",
       "      <td>-0.011801</td>\n",
       "      <td>0.028911</td>\n",
       "      <td>-0.018523</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>0.015231</td>\n",
       "      <td>0.073132</td>\n",
       "      <td>0.117845</td>\n",
       "      <td>0.079427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.096773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065018</td>\n",
       "      <td>-0.014962</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>0.028726</td>\n",
       "      <td>-0.018262</td>\n",
       "      <td>-0.000502</td>\n",
       "      <td>0.014382</td>\n",
       "      <td>0.072208</td>\n",
       "      <td>0.120518</td>\n",
       "      <td>0.082034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.100949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064344</td>\n",
       "      <td>-0.011460</td>\n",
       "      <td>-0.010032</td>\n",
       "      <td>0.024208</td>\n",
       "      <td>-0.020633</td>\n",
       "      <td>-0.002864</td>\n",
       "      <td>0.015210</td>\n",
       "      <td>0.068588</td>\n",
       "      <td>0.132405</td>\n",
       "      <td>0.087032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.102027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064828</td>\n",
       "      <td>-0.011484</td>\n",
       "      <td>-0.012271</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>-0.020175</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>0.071957</td>\n",
       "      <td>0.126841</td>\n",
       "      <td>0.080524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.111048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052623</td>\n",
       "      <td>-0.008342</td>\n",
       "      <td>-0.012503</td>\n",
       "      <td>0.021081</td>\n",
       "      <td>-0.021591</td>\n",
       "      <td>-0.004355</td>\n",
       "      <td>0.018470</td>\n",
       "      <td>0.052656</td>\n",
       "      <td>0.114715</td>\n",
       "      <td>0.067151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.109572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045988</td>\n",
       "      <td>-0.006674</td>\n",
       "      <td>-0.016360</td>\n",
       "      <td>0.024649</td>\n",
       "      <td>-0.020225</td>\n",
       "      <td>-0.005879</td>\n",
       "      <td>0.021435</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>0.106382</td>\n",
       "      <td>0.058749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.108368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>-0.001467</td>\n",
       "      <td>-0.018113</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>-0.012736</td>\n",
       "      <td>0.021479</td>\n",
       "      <td>0.034875</td>\n",
       "      <td>0.075055</td>\n",
       "      <td>0.041208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.106652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>-0.019410</td>\n",
       "      <td>0.029802</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>-0.013568</td>\n",
       "      <td>0.023139</td>\n",
       "      <td>0.043662</td>\n",
       "      <td>0.074502</td>\n",
       "      <td>0.044906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.117759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>-0.020199</td>\n",
       "      <td>0.030140</td>\n",
       "      <td>-0.013465</td>\n",
       "      <td>-0.022304</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>0.057197</td>\n",
       "      <td>0.098463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.116356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101153</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>-0.020223</td>\n",
       "      <td>0.031851</td>\n",
       "      <td>-0.011336</td>\n",
       "      <td>-0.019646</td>\n",
       "      <td>0.023272</td>\n",
       "      <td>0.021620</td>\n",
       "      <td>0.058376</td>\n",
       "      <td>0.104822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.117097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225337</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>-0.020632</td>\n",
       "      <td>0.026901</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.031019</td>\n",
       "      <td>0.017983</td>\n",
       "      <td>0.039557</td>\n",
       "      <td>0.135165</td>\n",
       "      <td>0.424844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.114960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233721</td>\n",
       "      <td>0.008902</td>\n",
       "      <td>-0.020686</td>\n",
       "      <td>0.028504</td>\n",
       "      <td>-0.012695</td>\n",
       "      <td>-0.028263</td>\n",
       "      <td>0.020130</td>\n",
       "      <td>0.039709</td>\n",
       "      <td>0.135740</td>\n",
       "      <td>0.437862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.197568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777782</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>-0.021489</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>-0.019241</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.226792</td>\n",
       "      <td>0.541086</td>\n",
       "      <td>1.535320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "      <td>-0.077121</td>\n",
       "      <td>...</td>\n",
       "      <td>3.400994</td>\n",
       "      <td>0.264119</td>\n",
       "      <td>0.495739</td>\n",
       "      <td>0.674128</td>\n",
       "      <td>0.902048</td>\n",
       "      <td>1.047920</td>\n",
       "      <td>1.500841</td>\n",
       "      <td>2.034684</td>\n",
       "      <td>2.615589</td>\n",
       "      <td>5.133702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "      <td>1.256989</td>\n",
       "      <td>...</td>\n",
       "      <td>17.002937</td>\n",
       "      <td>1.072741</td>\n",
       "      <td>2.336011</td>\n",
       "      <td>3.130486</td>\n",
       "      <td>4.402256</td>\n",
       "      <td>5.309959</td>\n",
       "      <td>7.304523</td>\n",
       "      <td>11.356960</td>\n",
       "      <td>14.751767</td>\n",
       "      <td>23.926376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "      <td>2.738707</td>\n",
       "      <td>...</td>\n",
       "      <td>23.022182</td>\n",
       "      <td>1.942475</td>\n",
       "      <td>4.374655</td>\n",
       "      <td>5.956992</td>\n",
       "      <td>8.329583</td>\n",
       "      <td>9.918426</td>\n",
       "      <td>13.153889</td>\n",
       "      <td>17.420641</td>\n",
       "      <td>20.779972</td>\n",
       "      <td>30.774134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "      <td>8.618659</td>\n",
       "      <td>...</td>\n",
       "      <td>38.536106</td>\n",
       "      <td>2.854699</td>\n",
       "      <td>6.728202</td>\n",
       "      <td>9.450163</td>\n",
       "      <td>13.067003</td>\n",
       "      <td>15.378316</td>\n",
       "      <td>19.739746</td>\n",
       "      <td>27.678812</td>\n",
       "      <td>32.383354</td>\n",
       "      <td>45.449432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "      <td>11.597038</td>\n",
       "      <td>...</td>\n",
       "      <td>39.539894</td>\n",
       "      <td>3.939633</td>\n",
       "      <td>9.275344</td>\n",
       "      <td>13.170128</td>\n",
       "      <td>17.761337</td>\n",
       "      <td>20.245480</td>\n",
       "      <td>24.922413</td>\n",
       "      <td>30.494610</td>\n",
       "      <td>34.002228</td>\n",
       "      <td>46.085567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "      <td>15.709170</td>\n",
       "      <td>...</td>\n",
       "      <td>43.025505</td>\n",
       "      <td>6.915888</td>\n",
       "      <td>15.905630</td>\n",
       "      <td>22.824675</td>\n",
       "      <td>28.863770</td>\n",
       "      <td>30.576330</td>\n",
       "      <td>34.374794</td>\n",
       "      <td>39.488232</td>\n",
       "      <td>40.977833</td>\n",
       "      <td>49.224030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "      <td>14.911612</td>\n",
       "      <td>...</td>\n",
       "      <td>41.715477</td>\n",
       "      <td>8.200056</td>\n",
       "      <td>18.293543</td>\n",
       "      <td>25.990767</td>\n",
       "      <td>32.111149</td>\n",
       "      <td>32.966881</td>\n",
       "      <td>35.990849</td>\n",
       "      <td>40.117195</td>\n",
       "      <td>40.644146</td>\n",
       "      <td>45.738392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "      <td>19.156044</td>\n",
       "      <td>...</td>\n",
       "      <td>54.869659</td>\n",
       "      <td>7.407323</td>\n",
       "      <td>17.336258</td>\n",
       "      <td>25.192966</td>\n",
       "      <td>31.933836</td>\n",
       "      <td>34.416416</td>\n",
       "      <td>38.496105</td>\n",
       "      <td>46.924267</td>\n",
       "      <td>50.507477</td>\n",
       "      <td>61.142986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "      <td>18.315992</td>\n",
       "      <td>...</td>\n",
       "      <td>54.997051</td>\n",
       "      <td>5.967392</td>\n",
       "      <td>14.389499</td>\n",
       "      <td>20.967751</td>\n",
       "      <td>27.111248</td>\n",
       "      <td>29.764145</td>\n",
       "      <td>34.362354</td>\n",
       "      <td>45.002655</td>\n",
       "      <td>49.748482</td>\n",
       "      <td>62.335693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "      <td>19.220716</td>\n",
       "      <td>...</td>\n",
       "      <td>53.178963</td>\n",
       "      <td>5.808964</td>\n",
       "      <td>14.334302</td>\n",
       "      <td>20.897337</td>\n",
       "      <td>26.873413</td>\n",
       "      <td>29.254448</td>\n",
       "      <td>33.697369</td>\n",
       "      <td>44.851906</td>\n",
       "      <td>49.863873</td>\n",
       "      <td>61.423630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "      <td>19.388786</td>\n",
       "      <td>...</td>\n",
       "      <td>51.653759</td>\n",
       "      <td>7.081882</td>\n",
       "      <td>16.910042</td>\n",
       "      <td>24.446466</td>\n",
       "      <td>30.742262</td>\n",
       "      <td>32.603485</td>\n",
       "      <td>36.421150</td>\n",
       "      <td>45.349258</td>\n",
       "      <td>48.891109</td>\n",
       "      <td>58.018311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "      <td>17.984758</td>\n",
       "      <td>...</td>\n",
       "      <td>49.190712</td>\n",
       "      <td>5.806954</td>\n",
       "      <td>14.585413</td>\n",
       "      <td>21.218033</td>\n",
       "      <td>27.027863</td>\n",
       "      <td>29.005163</td>\n",
       "      <td>33.175629</td>\n",
       "      <td>42.217506</td>\n",
       "      <td>46.677853</td>\n",
       "      <td>56.449585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "      <td>18.846518</td>\n",
       "      <td>...</td>\n",
       "      <td>56.211308</td>\n",
       "      <td>3.828525</td>\n",
       "      <td>10.088699</td>\n",
       "      <td>14.839977</td>\n",
       "      <td>19.860422</td>\n",
       "      <td>22.499790</td>\n",
       "      <td>27.445484</td>\n",
       "      <td>41.701839</td>\n",
       "      <td>48.696514</td>\n",
       "      <td>60.961891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "      <td>16.131306</td>\n",
       "      <td>...</td>\n",
       "      <td>44.686806</td>\n",
       "      <td>4.373532</td>\n",
       "      <td>11.635606</td>\n",
       "      <td>17.027676</td>\n",
       "      <td>22.105349</td>\n",
       "      <td>24.033895</td>\n",
       "      <td>28.395443</td>\n",
       "      <td>36.105194</td>\n",
       "      <td>40.074772</td>\n",
       "      <td>49.067268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "      <td>15.983805</td>\n",
       "      <td>...</td>\n",
       "      <td>41.940247</td>\n",
       "      <td>4.839461</td>\n",
       "      <td>12.652713</td>\n",
       "      <td>18.372803</td>\n",
       "      <td>23.488434</td>\n",
       "      <td>25.086071</td>\n",
       "      <td>29.135933</td>\n",
       "      <td>35.702171</td>\n",
       "      <td>39.347977</td>\n",
       "      <td>47.683128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "      <td>10.291277</td>\n",
       "      <td>...</td>\n",
       "      <td>33.889275</td>\n",
       "      <td>4.328761</td>\n",
       "      <td>11.729706</td>\n",
       "      <td>16.993879</td>\n",
       "      <td>21.594444</td>\n",
       "      <td>22.700935</td>\n",
       "      <td>26.435886</td>\n",
       "      <td>28.733198</td>\n",
       "      <td>30.939457</td>\n",
       "      <td>36.958694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "      <td>11.007571</td>\n",
       "      <td>...</td>\n",
       "      <td>37.509048</td>\n",
       "      <td>2.866959</td>\n",
       "      <td>8.060787</td>\n",
       "      <td>11.837075</td>\n",
       "      <td>15.822600</td>\n",
       "      <td>17.561096</td>\n",
       "      <td>21.727287</td>\n",
       "      <td>27.373491</td>\n",
       "      <td>30.011728</td>\n",
       "      <td>37.217583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "      <td>2.869643</td>\n",
       "      <td>...</td>\n",
       "      <td>23.327524</td>\n",
       "      <td>1.385374</td>\n",
       "      <td>4.052506</td>\n",
       "      <td>6.014334</td>\n",
       "      <td>8.485521</td>\n",
       "      <td>9.929159</td>\n",
       "      <td>13.326152</td>\n",
       "      <td>17.573149</td>\n",
       "      <td>19.661575</td>\n",
       "      <td>25.812149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "      <td>0.200522</td>\n",
       "      <td>...</td>\n",
       "      <td>12.147826</td>\n",
       "      <td>0.336648</td>\n",
       "      <td>0.880131</td>\n",
       "      <td>1.302681</td>\n",
       "      <td>1.897597</td>\n",
       "      <td>2.305977</td>\n",
       "      <td>3.439399</td>\n",
       "      <td>7.480927</td>\n",
       "      <td>10.735545</td>\n",
       "      <td>16.216372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.368900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274428</td>\n",
       "      <td>0.023079</td>\n",
       "      <td>-0.019655</td>\n",
       "      <td>-0.003255</td>\n",
       "      <td>-0.012266</td>\n",
       "      <td>-0.067044</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.259501</td>\n",
       "      <td>0.341166</td>\n",
       "      <td>0.928638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.389584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231477</td>\n",
       "      <td>0.022003</td>\n",
       "      <td>-0.018988</td>\n",
       "      <td>-0.004034</td>\n",
       "      <td>-0.014074</td>\n",
       "      <td>-0.063722</td>\n",
       "      <td>0.009315</td>\n",
       "      <td>0.233116</td>\n",
       "      <td>0.314251</td>\n",
       "      <td>0.870704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.037001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051620</td>\n",
       "      <td>0.020892</td>\n",
       "      <td>-0.018374</td>\n",
       "      <td>-0.008166</td>\n",
       "      <td>-0.015656</td>\n",
       "      <td>-0.061980</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.052873</td>\n",
       "      <td>0.032155</td>\n",
       "      <td>0.213217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.037392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048201</td>\n",
       "      <td>0.020165</td>\n",
       "      <td>-0.017937</td>\n",
       "      <td>-0.008697</td>\n",
       "      <td>-0.016739</td>\n",
       "      <td>-0.059355</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.049062</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.202437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.055406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027003</td>\n",
       "      <td>0.019085</td>\n",
       "      <td>-0.017251</td>\n",
       "      <td>-0.013010</td>\n",
       "      <td>-0.018626</td>\n",
       "      <td>-0.059569</td>\n",
       "      <td>-0.006668</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.053057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.050410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024909</td>\n",
       "      <td>0.018369</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>-0.013577</td>\n",
       "      <td>-0.019679</td>\n",
       "      <td>-0.056922</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>0.050451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.021308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013091</td>\n",
       "      <td>0.017302</td>\n",
       "      <td>-0.016091</td>\n",
       "      <td>-0.018004</td>\n",
       "      <td>-0.021931</td>\n",
       "      <td>-0.058830</td>\n",
       "      <td>-0.018881</td>\n",
       "      <td>-0.006862</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.008685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.020823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012896</td>\n",
       "      <td>0.016869</td>\n",
       "      <td>-0.015987</td>\n",
       "      <td>-0.018230</td>\n",
       "      <td>-0.022325</td>\n",
       "      <td>-0.056330</td>\n",
       "      <td>-0.016591</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>0.017193</td>\n",
       "      <td>0.008185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.018522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028403</td>\n",
       "      <td>0.015837</td>\n",
       "      <td>-0.015257</td>\n",
       "      <td>-0.022824</td>\n",
       "      <td>-0.025107</td>\n",
       "      <td>-0.060419</td>\n",
       "      <td>-0.033243</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>-0.006694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.018015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027395</td>\n",
       "      <td>0.015098</td>\n",
       "      <td>-0.014817</td>\n",
       "      <td>-0.023384</td>\n",
       "      <td>-0.026087</td>\n",
       "      <td>-0.056993</td>\n",
       "      <td>-0.029765</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.027856</td>\n",
       "      <td>-0.006112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.014617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021610</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>-0.014156</td>\n",
       "      <td>-0.027935</td>\n",
       "      <td>-0.029163</td>\n",
       "      <td>-0.061582</td>\n",
       "      <td>-0.046556</td>\n",
       "      <td>0.015714</td>\n",
       "      <td>0.034597</td>\n",
       "      <td>-0.005556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.014176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021657</td>\n",
       "      <td>0.014006</td>\n",
       "      <td>-0.014239</td>\n",
       "      <td>-0.027929</td>\n",
       "      <td>-0.029134</td>\n",
       "      <td>-0.060981</td>\n",
       "      <td>-0.045462</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>0.034906</td>\n",
       "      <td>-0.005494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.005292</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006435</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>-0.014297</td>\n",
       "      <td>-0.031694</td>\n",
       "      <td>-0.031030</td>\n",
       "      <td>-0.067607</td>\n",
       "      <td>-0.063667</td>\n",
       "      <td>0.027830</td>\n",
       "      <td>0.040672</td>\n",
       "      <td>0.002630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.005292</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006435</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>-0.014297</td>\n",
       "      <td>-0.031694</td>\n",
       "      <td>-0.031030</td>\n",
       "      <td>-0.067607</td>\n",
       "      <td>-0.063667</td>\n",
       "      <td>0.027830</td>\n",
       "      <td>0.040672</td>\n",
       "      <td>0.002630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L00.1  L00.2  L00.3  L00.4  L00.5  L00.6  L00.7  L00.8  L00.9      D00.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.095921   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.093247   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.092981   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.096773   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.100949   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.102027   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.111048   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.109572   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.108368   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.106652   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.117759   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.116356   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.117097   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.114960   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.197568   \n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45  -0.077121   \n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51   1.256989   \n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11   2.738707   \n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34   8.618659   \n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51  11.597038   \n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17  15.709170   \n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03  14.911612   \n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17  19.156044   \n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82  18.315992   \n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14  19.220716   \n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61  19.388786   \n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92  17.984758   \n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20  18.846518   \n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73  16.131306   \n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94  15.983805   \n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20  10.291277   \n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59  11.007571   \n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49   2.869643   \n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13   0.200522   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.368900   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.389584   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.037001   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.037392   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.055406   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.050410   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.021308   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.020823   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.018522   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.018015   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.014617   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.014176   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.005292   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.005292   \n",
       "\n",
       "    ...      G00.9     M00.1      M00.2      M00.3      M00.4      M00.5  \\\n",
       "0   ...  -0.059470 -0.018320  -0.009768   0.033719  -0.016597  -0.000701   \n",
       "1   ...  -0.059932 -0.018526  -0.011063   0.033784  -0.016425  -0.000064   \n",
       "2   ...  -0.065703 -0.014829  -0.011801   0.028911  -0.018523  -0.000436   \n",
       "3   ...  -0.065018 -0.014962  -0.010795   0.028726  -0.018262  -0.000502   \n",
       "4   ...  -0.064344 -0.011460  -0.010032   0.024208  -0.020633  -0.002864   \n",
       "5   ...  -0.064828 -0.011484  -0.012271   0.024721  -0.020175  -0.001891   \n",
       "6   ...  -0.052623 -0.008342  -0.012503   0.021081  -0.021591  -0.004355   \n",
       "7   ...  -0.045988 -0.006674  -0.016360   0.024649  -0.020225  -0.005879   \n",
       "8   ...   0.004021 -0.001467  -0.018113   0.025191  -0.019282  -0.012736   \n",
       "9   ...   0.012314  0.000328  -0.019410   0.029802  -0.015309  -0.013568   \n",
       "10  ...   0.100227  0.005588  -0.020199   0.030140  -0.013465  -0.022304   \n",
       "11  ...   0.101153  0.005822  -0.020223   0.031851  -0.011336  -0.019646   \n",
       "12  ...   0.225337  0.008677  -0.020632   0.026901  -0.014735  -0.031019   \n",
       "13  ...   0.233721  0.008902  -0.020686   0.028504  -0.012695  -0.028263   \n",
       "14  ...   0.777782  0.011312  -0.021489   0.021193  -0.019241  -0.046883   \n",
       "15  ...   3.400994  0.264119   0.495739   0.674128   0.902048   1.047920   \n",
       "16  ...  17.002937  1.072741   2.336011   3.130486   4.402256   5.309959   \n",
       "17  ...  23.022182  1.942475   4.374655   5.956992   8.329583   9.918426   \n",
       "18  ...  38.536106  2.854699   6.728202   9.450163  13.067003  15.378316   \n",
       "19  ...  39.539894  3.939633   9.275344  13.170128  17.761337  20.245480   \n",
       "20  ...  43.025505  6.915888  15.905630  22.824675  28.863770  30.576330   \n",
       "21  ...  41.715477  8.200056  18.293543  25.990767  32.111149  32.966881   \n",
       "22  ...  54.869659  7.407323  17.336258  25.192966  31.933836  34.416416   \n",
       "23  ...  54.997051  5.967392  14.389499  20.967751  27.111248  29.764145   \n",
       "24  ...  53.178963  5.808964  14.334302  20.897337  26.873413  29.254448   \n",
       "25  ...  51.653759  7.081882  16.910042  24.446466  30.742262  32.603485   \n",
       "26  ...  49.190712  5.806954  14.585413  21.218033  27.027863  29.005163   \n",
       "27  ...  56.211308  3.828525  10.088699  14.839977  19.860422  22.499790   \n",
       "28  ...  44.686806  4.373532  11.635606  17.027676  22.105349  24.033895   \n",
       "29  ...  41.940247  4.839461  12.652713  18.372803  23.488434  25.086071   \n",
       "30  ...  33.889275  4.328761  11.729706  16.993879  21.594444  22.700935   \n",
       "31  ...  37.509048  2.866959   8.060787  11.837075  15.822600  17.561096   \n",
       "32  ...  23.327524  1.385374   4.052506   6.014334   8.485521   9.929159   \n",
       "33  ...  12.147826  0.336648   0.880131   1.302681   1.897597   2.305977   \n",
       "34  ...   0.274428  0.023079  -0.019655  -0.003255  -0.012266  -0.067044   \n",
       "35  ...   0.231477  0.022003  -0.018988  -0.004034  -0.014074  -0.063722   \n",
       "36  ...   0.051620  0.020892  -0.018374  -0.008166  -0.015656  -0.061980   \n",
       "37  ...   0.048201  0.020165  -0.017937  -0.008697  -0.016739  -0.059355   \n",
       "38  ...   0.027003  0.019085  -0.017251  -0.013010  -0.018626  -0.059569   \n",
       "39  ...   0.024909  0.018369  -0.016789  -0.013577  -0.019679  -0.056922   \n",
       "40  ...  -0.013091  0.017302  -0.016091  -0.018004  -0.021931  -0.058830   \n",
       "41  ...  -0.012896  0.016869  -0.015987  -0.018230  -0.022325  -0.056330   \n",
       "42  ...  -0.028403  0.015837  -0.015257  -0.022824  -0.025107  -0.060419   \n",
       "43  ...  -0.027395  0.015098  -0.014817  -0.023384  -0.026087  -0.056993   \n",
       "44  ...  -0.021610  0.014058  -0.014156  -0.027935  -0.029163  -0.061582   \n",
       "45  ...  -0.021657  0.014006  -0.014239  -0.027929  -0.029134  -0.060981   \n",
       "46  ...  -0.006435  0.013568  -0.014297  -0.031694  -0.031030  -0.067607   \n",
       "47  ...  -0.006435  0.013568  -0.014297  -0.031694  -0.031030  -0.067607   \n",
       "\n",
       "        M00.6      M00.7      M00.8      M00.9  \n",
       "0    0.010031   0.057112   0.095433   0.070377  \n",
       "1    0.011187   0.057775   0.091750   0.066787  \n",
       "2    0.015231   0.073132   0.117845   0.079427  \n",
       "3    0.014382   0.072208   0.120518   0.082034  \n",
       "4    0.015210   0.068588   0.132405   0.087032  \n",
       "5    0.017570   0.071957   0.126841   0.080524  \n",
       "6    0.018470   0.052656   0.114715   0.067151  \n",
       "7    0.021435   0.061731   0.106382   0.058749  \n",
       "8    0.021479   0.034875   0.075055   0.041208  \n",
       "9    0.023139   0.043662   0.074502   0.044906  \n",
       "10   0.021243   0.017968   0.057197   0.098463  \n",
       "11   0.023272   0.021620   0.058376   0.104822  \n",
       "12   0.017983   0.039557   0.135165   0.424844  \n",
       "13   0.020130   0.039709   0.135740   0.437862  \n",
       "14   0.009940   0.226792   0.541086   1.535320  \n",
       "15   1.500841   2.034684   2.615589   5.133702  \n",
       "16   7.304523  11.356960  14.751767  23.926376  \n",
       "17  13.153889  17.420641  20.779972  30.774134  \n",
       "18  19.739746  27.678812  32.383354  45.449432  \n",
       "19  24.922413  30.494610  34.002228  46.085567  \n",
       "20  34.374794  39.488232  40.977833  49.224030  \n",
       "21  35.990849  40.117195  40.644146  45.738392  \n",
       "22  38.496105  46.924267  50.507477  61.142986  \n",
       "23  34.362354  45.002655  49.748482  62.335693  \n",
       "24  33.697369  44.851906  49.863873  61.423630  \n",
       "25  36.421150  45.349258  48.891109  58.018311  \n",
       "26  33.175629  42.217506  46.677853  56.449585  \n",
       "27  27.445484  41.701839  48.696514  60.961891  \n",
       "28  28.395443  36.105194  40.074772  49.067268  \n",
       "29  29.135933  35.702171  39.347977  47.683128  \n",
       "30  26.435886  28.733198  30.939457  36.958694  \n",
       "31  21.727287  27.373491  30.011728  37.217583  \n",
       "32  13.326152  17.573149  19.661575  25.812149  \n",
       "33   3.439399   7.480927  10.735545  16.216372  \n",
       "34   0.010050   0.259501   0.341166   0.928638  \n",
       "35   0.009315   0.233116   0.314251   0.870704  \n",
       "36   0.002859   0.052873   0.032155   0.213217  \n",
       "37   0.003199   0.049062   0.030683   0.202437  \n",
       "38  -0.006668  -0.001219   0.002403   0.053057  \n",
       "39  -0.005747  -0.000059   0.004178   0.050451  \n",
       "40  -0.018881  -0.006862   0.015500   0.008685  \n",
       "41  -0.016591  -0.003742   0.017193   0.008185  \n",
       "42  -0.033243   0.001940   0.026257  -0.006694  \n",
       "43  -0.029765   0.004939   0.027856  -0.006112  \n",
       "44  -0.046556   0.015714   0.034597  -0.005556  \n",
       "45  -0.045462   0.016420   0.034906  -0.005494  \n",
       "46  -0.063667   0.027830   0.040672   0.002630  \n",
       "47  -0.063667   0.027830   0.040672   0.002630  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_0[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L10.1</th>\n",
       "      <th>L10.2</th>\n",
       "      <th>L10.3</th>\n",
       "      <th>L10.4</th>\n",
       "      <th>L10.5</th>\n",
       "      <th>L10.6</th>\n",
       "      <th>L10.7</th>\n",
       "      <th>L10.8</th>\n",
       "      <th>L10.9</th>\n",
       "      <th>D10.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G10.9</th>\n",
       "      <th>M10.1</th>\n",
       "      <th>M10.2</th>\n",
       "      <th>M10.3</th>\n",
       "      <th>M10.4</th>\n",
       "      <th>M10.5</th>\n",
       "      <th>M10.6</th>\n",
       "      <th>M10.7</th>\n",
       "      <th>M10.8</th>\n",
       "      <th>M10.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.106918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149395</td>\n",
       "      <td>-0.041528</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.016066</td>\n",
       "      <td>-0.017140</td>\n",
       "      <td>-0.085209</td>\n",
       "      <td>0.030080</td>\n",
       "      <td>0.084629</td>\n",
       "      <td>0.028655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.104244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144378</td>\n",
       "      <td>-0.041507</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>0.016043</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>-0.082663</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.084478</td>\n",
       "      <td>0.028239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.100999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129127</td>\n",
       "      <td>-0.038472</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.020663</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>-0.074202</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>0.119472</td>\n",
       "      <td>0.033769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.104531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132215</td>\n",
       "      <td>-0.038721</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.075112</td>\n",
       "      <td>0.047003</td>\n",
       "      <td>0.119001</td>\n",
       "      <td>0.033892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.105793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137172</td>\n",
       "      <td>-0.035930</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.025469</td>\n",
       "      <td>0.010456</td>\n",
       "      <td>-0.069133</td>\n",
       "      <td>0.067605</td>\n",
       "      <td>0.168484</td>\n",
       "      <td>0.038271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.108143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135302</td>\n",
       "      <td>-0.035851</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>0.018096</td>\n",
       "      <td>-0.066619</td>\n",
       "      <td>0.069808</td>\n",
       "      <td>0.169381</td>\n",
       "      <td>0.037673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.115231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160087</td>\n",
       "      <td>-0.033524</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.028151</td>\n",
       "      <td>0.031564</td>\n",
       "      <td>-0.059831</td>\n",
       "      <td>0.095915</td>\n",
       "      <td>0.238365</td>\n",
       "      <td>0.036981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.117750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163659</td>\n",
       "      <td>-0.032639</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>0.049007</td>\n",
       "      <td>-0.058177</td>\n",
       "      <td>0.104937</td>\n",
       "      <td>0.248831</td>\n",
       "      <td>0.038750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.117665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>-0.029885</td>\n",
       "      <td>-0.000476</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>0.073080</td>\n",
       "      <td>-0.052483</td>\n",
       "      <td>0.147891</td>\n",
       "      <td>0.365669</td>\n",
       "      <td>0.033603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.118352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222411</td>\n",
       "      <td>-0.029366</td>\n",
       "      <td>-0.001399</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>0.028703</td>\n",
       "      <td>0.084407</td>\n",
       "      <td>-0.050417</td>\n",
       "      <td>0.156691</td>\n",
       "      <td>0.376725</td>\n",
       "      <td>0.036197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.129696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341714</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.001101</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.030941</td>\n",
       "      <td>0.106290</td>\n",
       "      <td>-0.042806</td>\n",
       "      <td>0.214321</td>\n",
       "      <td>0.555393</td>\n",
       "      <td>0.028220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.129130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347641</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.001069</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.108701</td>\n",
       "      <td>-0.040352</td>\n",
       "      <td>0.214508</td>\n",
       "      <td>0.553944</td>\n",
       "      <td>0.028364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.129863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621086</td>\n",
       "      <td>-0.025513</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.031877</td>\n",
       "      <td>0.121943</td>\n",
       "      <td>-0.033318</td>\n",
       "      <td>0.285069</td>\n",
       "      <td>0.810288</td>\n",
       "      <td>0.065609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.128609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643642</td>\n",
       "      <td>-0.025494</td>\n",
       "      <td>-0.000602</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.032157</td>\n",
       "      <td>0.124284</td>\n",
       "      <td>-0.031204</td>\n",
       "      <td>0.284095</td>\n",
       "      <td>0.805341</td>\n",
       "      <td>0.062729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.217106</td>\n",
       "      <td>...</td>\n",
       "      <td>1.131276</td>\n",
       "      <td>-0.024244</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.005877</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.136935</td>\n",
       "      <td>-0.023819</td>\n",
       "      <td>0.399394</td>\n",
       "      <td>1.245431</td>\n",
       "      <td>0.308215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.87</td>\n",
       "      <td>-0.112919</td>\n",
       "      <td>...</td>\n",
       "      <td>4.379683</td>\n",
       "      <td>0.163705</td>\n",
       "      <td>0.494363</td>\n",
       "      <td>0.601693</td>\n",
       "      <td>1.041145</td>\n",
       "      <td>1.500598</td>\n",
       "      <td>2.120004</td>\n",
       "      <td>3.790680</td>\n",
       "      <td>6.150227</td>\n",
       "      <td>8.203325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.92</td>\n",
       "      <td>1.116548</td>\n",
       "      <td>...</td>\n",
       "      <td>19.271200</td>\n",
       "      <td>0.734207</td>\n",
       "      <td>2.128567</td>\n",
       "      <td>2.832492</td>\n",
       "      <td>4.939633</td>\n",
       "      <td>6.857623</td>\n",
       "      <td>10.415841</td>\n",
       "      <td>17.162670</td>\n",
       "      <td>22.688566</td>\n",
       "      <td>27.188646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>18.24</td>\n",
       "      <td>25.56</td>\n",
       "      <td>2.603195</td>\n",
       "      <td>...</td>\n",
       "      <td>24.550512</td>\n",
       "      <td>1.365765</td>\n",
       "      <td>3.922879</td>\n",
       "      <td>5.464826</td>\n",
       "      <td>9.341690</td>\n",
       "      <td>12.560125</td>\n",
       "      <td>18.327480</td>\n",
       "      <td>25.028303</td>\n",
       "      <td>28.206825</td>\n",
       "      <td>31.369656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.74</td>\n",
       "      <td>14.78</td>\n",
       "      <td>19.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>22.60</td>\n",
       "      <td>20.57</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.66</td>\n",
       "      <td>32.96</td>\n",
       "      <td>8.278840</td>\n",
       "      <td>...</td>\n",
       "      <td>38.503490</td>\n",
       "      <td>2.014142</td>\n",
       "      <td>5.884316</td>\n",
       "      <td>8.633399</td>\n",
       "      <td>14.371058</td>\n",
       "      <td>18.709196</td>\n",
       "      <td>26.057796</td>\n",
       "      <td>34.140823</td>\n",
       "      <td>39.545776</td>\n",
       "      <td>44.381779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.47</td>\n",
       "      <td>19.06</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.62</td>\n",
       "      <td>20.20</td>\n",
       "      <td>17.15</td>\n",
       "      <td>22.85</td>\n",
       "      <td>33.97</td>\n",
       "      <td>10.936314</td>\n",
       "      <td>...</td>\n",
       "      <td>39.108696</td>\n",
       "      <td>2.881365</td>\n",
       "      <td>8.102743</td>\n",
       "      <td>12.145566</td>\n",
       "      <td>19.254927</td>\n",
       "      <td>23.877275</td>\n",
       "      <td>31.638023</td>\n",
       "      <td>37.019367</td>\n",
       "      <td>40.691109</td>\n",
       "      <td>44.075199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.33</td>\n",
       "      <td>24.78</td>\n",
       "      <td>24.52</td>\n",
       "      <td>23.02</td>\n",
       "      <td>28.58</td>\n",
       "      <td>28.55</td>\n",
       "      <td>25.06</td>\n",
       "      <td>26.39</td>\n",
       "      <td>27.24</td>\n",
       "      <td>14.450748</td>\n",
       "      <td>...</td>\n",
       "      <td>42.739048</td>\n",
       "      <td>5.811303</td>\n",
       "      <td>14.109083</td>\n",
       "      <td>21.780197</td>\n",
       "      <td>30.765471</td>\n",
       "      <td>34.016811</td>\n",
       "      <td>40.795864</td>\n",
       "      <td>41.763218</td>\n",
       "      <td>45.237823</td>\n",
       "      <td>46.935425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.26</td>\n",
       "      <td>26.20</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.70</td>\n",
       "      <td>29.22</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.05</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.45</td>\n",
       "      <td>13.226276</td>\n",
       "      <td>...</td>\n",
       "      <td>41.648247</td>\n",
       "      <td>7.431507</td>\n",
       "      <td>16.472395</td>\n",
       "      <td>25.372099</td>\n",
       "      <td>34.218544</td>\n",
       "      <td>36.105286</td>\n",
       "      <td>41.835892</td>\n",
       "      <td>41.249744</td>\n",
       "      <td>44.111916</td>\n",
       "      <td>44.791996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.96</td>\n",
       "      <td>29.89</td>\n",
       "      <td>33.58</td>\n",
       "      <td>33.28</td>\n",
       "      <td>38.13</td>\n",
       "      <td>35.95</td>\n",
       "      <td>34.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.78</td>\n",
       "      <td>17.926546</td>\n",
       "      <td>...</td>\n",
       "      <td>55.579094</td>\n",
       "      <td>6.273179</td>\n",
       "      <td>15.135388</td>\n",
       "      <td>23.796743</td>\n",
       "      <td>34.090336</td>\n",
       "      <td>38.220592</td>\n",
       "      <td>45.829922</td>\n",
       "      <td>49.055729</td>\n",
       "      <td>54.976547</td>\n",
       "      <td>59.119770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.24</td>\n",
       "      <td>24.45</td>\n",
       "      <td>29.77</td>\n",
       "      <td>29.36</td>\n",
       "      <td>34.56</td>\n",
       "      <td>30.81</td>\n",
       "      <td>26.23</td>\n",
       "      <td>27.65</td>\n",
       "      <td>34.53</td>\n",
       "      <td>17.436552</td>\n",
       "      <td>...</td>\n",
       "      <td>54.410767</td>\n",
       "      <td>4.662014</td>\n",
       "      <td>12.351225</td>\n",
       "      <td>19.290428</td>\n",
       "      <td>28.663797</td>\n",
       "      <td>33.311131</td>\n",
       "      <td>41.311726</td>\n",
       "      <td>46.520962</td>\n",
       "      <td>53.719425</td>\n",
       "      <td>59.650356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.30</td>\n",
       "      <td>25.22</td>\n",
       "      <td>31.45</td>\n",
       "      <td>29.95</td>\n",
       "      <td>34.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>29.10</td>\n",
       "      <td>27.16</td>\n",
       "      <td>31.12</td>\n",
       "      <td>18.304255</td>\n",
       "      <td>...</td>\n",
       "      <td>53.146549</td>\n",
       "      <td>4.498437</td>\n",
       "      <td>12.136342</td>\n",
       "      <td>18.993401</td>\n",
       "      <td>28.219103</td>\n",
       "      <td>32.713757</td>\n",
       "      <td>40.526493</td>\n",
       "      <td>45.483704</td>\n",
       "      <td>53.010464</td>\n",
       "      <td>59.550854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.03</td>\n",
       "      <td>30.02</td>\n",
       "      <td>34.72</td>\n",
       "      <td>32.49</td>\n",
       "      <td>35.12</td>\n",
       "      <td>35.82</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "      <td>18.084896</td>\n",
       "      <td>...</td>\n",
       "      <td>51.154129</td>\n",
       "      <td>5.975561</td>\n",
       "      <td>14.583498</td>\n",
       "      <td>22.823370</td>\n",
       "      <td>32.574348</td>\n",
       "      <td>36.256645</td>\n",
       "      <td>43.469482</td>\n",
       "      <td>46.375790</td>\n",
       "      <td>52.477821</td>\n",
       "      <td>56.533741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.21</td>\n",
       "      <td>27.84</td>\n",
       "      <td>33.51</td>\n",
       "      <td>32.70</td>\n",
       "      <td>37.10</td>\n",
       "      <td>34.75</td>\n",
       "      <td>29.17</td>\n",
       "      <td>27.61</td>\n",
       "      <td>29.70</td>\n",
       "      <td>16.875334</td>\n",
       "      <td>...</td>\n",
       "      <td>49.737137</td>\n",
       "      <td>4.515851</td>\n",
       "      <td>12.225721</td>\n",
       "      <td>19.160999</td>\n",
       "      <td>28.279984</td>\n",
       "      <td>32.513809</td>\n",
       "      <td>40.035805</td>\n",
       "      <td>43.463879</td>\n",
       "      <td>49.646496</td>\n",
       "      <td>55.124012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.50</td>\n",
       "      <td>22.99</td>\n",
       "      <td>30.11</td>\n",
       "      <td>31.82</td>\n",
       "      <td>32.80</td>\n",
       "      <td>33.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>36.02</td>\n",
       "      <td>44.45</td>\n",
       "      <td>18.224295</td>\n",
       "      <td>...</td>\n",
       "      <td>57.933491</td>\n",
       "      <td>2.634353</td>\n",
       "      <td>8.281919</td>\n",
       "      <td>13.020762</td>\n",
       "      <td>20.800072</td>\n",
       "      <td>25.858343</td>\n",
       "      <td>33.983849</td>\n",
       "      <td>41.909557</td>\n",
       "      <td>50.748085</td>\n",
       "      <td>59.927212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.97</td>\n",
       "      <td>21.03</td>\n",
       "      <td>25.88</td>\n",
       "      <td>24.37</td>\n",
       "      <td>28.26</td>\n",
       "      <td>26.10</td>\n",
       "      <td>21.49</td>\n",
       "      <td>20.61</td>\n",
       "      <td>26.21</td>\n",
       "      <td>15.174002</td>\n",
       "      <td>...</td>\n",
       "      <td>46.328804</td>\n",
       "      <td>3.103366</td>\n",
       "      <td>9.494406</td>\n",
       "      <td>14.946491</td>\n",
       "      <td>22.910091</td>\n",
       "      <td>27.318787</td>\n",
       "      <td>34.717789</td>\n",
       "      <td>37.884441</td>\n",
       "      <td>42.442059</td>\n",
       "      <td>47.340366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>22.61</td>\n",
       "      <td>27.12</td>\n",
       "      <td>27.53</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.98</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.92</td>\n",
       "      <td>25.82</td>\n",
       "      <td>14.869037</td>\n",
       "      <td>...</td>\n",
       "      <td>43.970497</td>\n",
       "      <td>3.583545</td>\n",
       "      <td>10.475468</td>\n",
       "      <td>16.401102</td>\n",
       "      <td>24.546824</td>\n",
       "      <td>28.593285</td>\n",
       "      <td>35.703087</td>\n",
       "      <td>37.639545</td>\n",
       "      <td>41.975739</td>\n",
       "      <td>46.276333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.57</td>\n",
       "      <td>17.69</td>\n",
       "      <td>21.43</td>\n",
       "      <td>22.13</td>\n",
       "      <td>22.79</td>\n",
       "      <td>24.23</td>\n",
       "      <td>23.32</td>\n",
       "      <td>21.47</td>\n",
       "      <td>23.67</td>\n",
       "      <td>9.054276</td>\n",
       "      <td>...</td>\n",
       "      <td>35.764736</td>\n",
       "      <td>3.087681</td>\n",
       "      <td>9.518919</td>\n",
       "      <td>14.901230</td>\n",
       "      <td>22.243767</td>\n",
       "      <td>25.818138</td>\n",
       "      <td>32.206387</td>\n",
       "      <td>31.293978</td>\n",
       "      <td>32.851269</td>\n",
       "      <td>35.022396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.19</td>\n",
       "      <td>15.84</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.86</td>\n",
       "      <td>15.93</td>\n",
       "      <td>16.22</td>\n",
       "      <td>15.55</td>\n",
       "      <td>15.47</td>\n",
       "      <td>24.20</td>\n",
       "      <td>10.094210</td>\n",
       "      <td>...</td>\n",
       "      <td>36.598442</td>\n",
       "      <td>1.868953</td>\n",
       "      <td>6.504136</td>\n",
       "      <td>10.268718</td>\n",
       "      <td>16.486364</td>\n",
       "      <td>20.491331</td>\n",
       "      <td>27.074900</td>\n",
       "      <td>29.371634</td>\n",
       "      <td>31.224209</td>\n",
       "      <td>34.205914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.95</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>15.83</td>\n",
       "      <td>2.551653</td>\n",
       "      <td>...</td>\n",
       "      <td>23.005661</td>\n",
       "      <td>0.809692</td>\n",
       "      <td>3.212546</td>\n",
       "      <td>5.106972</td>\n",
       "      <td>8.887909</td>\n",
       "      <td>11.896382</td>\n",
       "      <td>16.975452</td>\n",
       "      <td>20.083115</td>\n",
       "      <td>19.681376</td>\n",
       "      <td>21.461214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>13.80</td>\n",
       "      <td>0.176253</td>\n",
       "      <td>...</td>\n",
       "      <td>13.213209</td>\n",
       "      <td>0.164436</td>\n",
       "      <td>0.712153</td>\n",
       "      <td>1.096297</td>\n",
       "      <td>1.996767</td>\n",
       "      <td>2.970510</td>\n",
       "      <td>4.452457</td>\n",
       "      <td>8.914912</td>\n",
       "      <td>12.319630</td>\n",
       "      <td>17.087610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.337851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848460</td>\n",
       "      <td>-0.016487</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>0.013321</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.051712</td>\n",
       "      <td>0.531222</td>\n",
       "      <td>1.430376</td>\n",
       "      <td>0.168569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.356946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760980</td>\n",
       "      <td>-0.016617</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.023538</td>\n",
       "      <td>0.012023</td>\n",
       "      <td>0.180779</td>\n",
       "      <td>0.040885</td>\n",
       "      <td>0.499768</td>\n",
       "      <td>1.372622</td>\n",
       "      <td>0.161684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.032899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138102</td>\n",
       "      <td>-0.016206</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.023962</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.164164</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.337528</td>\n",
       "      <td>0.886785</td>\n",
       "      <td>0.081286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.032862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129115</td>\n",
       "      <td>-0.016296</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.023331</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.157046</td>\n",
       "      <td>0.014822</td>\n",
       "      <td>0.321467</td>\n",
       "      <td>0.858932</td>\n",
       "      <td>0.078351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.049657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057035</td>\n",
       "      <td>-0.015845</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.023754</td>\n",
       "      <td>-0.006658</td>\n",
       "      <td>0.139430</td>\n",
       "      <td>-0.005214</td>\n",
       "      <td>0.218347</td>\n",
       "      <td>0.566952</td>\n",
       "      <td>0.075679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.046683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054806</td>\n",
       "      <td>-0.015937</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>-0.007182</td>\n",
       "      <td>0.132655</td>\n",
       "      <td>-0.010286</td>\n",
       "      <td>0.207120</td>\n",
       "      <td>0.547564</td>\n",
       "      <td>0.072850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.034375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031672</td>\n",
       "      <td>-0.015441</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.023509</td>\n",
       "      <td>-0.017168</td>\n",
       "      <td>0.114629</td>\n",
       "      <td>-0.029525</td>\n",
       "      <td>0.151585</td>\n",
       "      <td>0.371098</td>\n",
       "      <td>0.078401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.033595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029004</td>\n",
       "      <td>-0.015507</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.023005</td>\n",
       "      <td>-0.016919</td>\n",
       "      <td>0.111454</td>\n",
       "      <td>-0.030944</td>\n",
       "      <td>0.146305</td>\n",
       "      <td>0.361779</td>\n",
       "      <td>0.076389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.029068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>-0.014961</td>\n",
       "      <td>-0.000951</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>-0.027308</td>\n",
       "      <td>0.093670</td>\n",
       "      <td>-0.048224</td>\n",
       "      <td>0.125090</td>\n",
       "      <td>0.254564</td>\n",
       "      <td>0.079670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.027590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002206</td>\n",
       "      <td>-0.015071</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>0.022711</td>\n",
       "      <td>-0.027115</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>-0.049247</td>\n",
       "      <td>0.119385</td>\n",
       "      <td>0.244170</td>\n",
       "      <td>0.077199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.022062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022100</td>\n",
       "      <td>-0.014493</td>\n",
       "      <td>-0.002407</td>\n",
       "      <td>0.023060</td>\n",
       "      <td>-0.036813</td>\n",
       "      <td>0.072113</td>\n",
       "      <td>-0.061266</td>\n",
       "      <td>0.120308</td>\n",
       "      <td>0.180855</td>\n",
       "      <td>0.081201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.021784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022312</td>\n",
       "      <td>-0.014511</td>\n",
       "      <td>-0.002379</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>-0.036601</td>\n",
       "      <td>0.072205</td>\n",
       "      <td>-0.060612</td>\n",
       "      <td>0.119864</td>\n",
       "      <td>0.180047</td>\n",
       "      <td>0.081042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.013247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023578</td>\n",
       "      <td>-0.013833</td>\n",
       "      <td>-0.004601</td>\n",
       "      <td>0.023774</td>\n",
       "      <td>-0.045343</td>\n",
       "      <td>0.062437</td>\n",
       "      <td>-0.068863</td>\n",
       "      <td>0.133734</td>\n",
       "      <td>0.146622</td>\n",
       "      <td>0.088492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.013247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023578</td>\n",
       "      <td>-0.013833</td>\n",
       "      <td>-0.004601</td>\n",
       "      <td>0.023774</td>\n",
       "      <td>-0.045343</td>\n",
       "      <td>0.062437</td>\n",
       "      <td>-0.068863</td>\n",
       "      <td>0.133734</td>\n",
       "      <td>0.146622</td>\n",
       "      <td>0.088492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L10.1  L10.2  L10.3  L10.4  L10.5  L10.6  L10.7  L10.8  L10.9      D10.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.106918   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.104244   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.100999   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.104531   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.105793   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.108143   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.115231   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.117750   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.117665   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.118352   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.129696   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.129130   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.129863   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.128609   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.217106   \n",
       "15   0.48   0.70   1.80   2.18   0.98   3.56   2.99   6.80   9.87  -0.112919   \n",
       "16   2.67   7.45   8.62  10.08  10.06  11.52  17.00  19.79  20.92   1.116548   \n",
       "17   3.80   8.55   8.15  10.49  10.84  12.03  15.05  18.24  25.56   2.603195   \n",
       "18   7.74  14.78  19.70  18.93  22.60  20.57  16.19  15.66  32.96   8.278840   \n",
       "19   9.47  19.06  21.52  21.87  23.62  20.20  17.15  22.85  33.97  10.936314   \n",
       "20  14.33  24.78  24.52  23.02  28.58  28.55  25.06  26.39  27.24  14.450748   \n",
       "21  15.26  26.20  24.57  24.70  29.22  31.11  31.05  31.59  32.45  13.226276   \n",
       "22  15.96  29.89  33.58  33.28  38.13  35.95  34.24  33.24  33.78  17.926546   \n",
       "23  14.24  24.45  29.77  29.36  34.56  30.81  26.23  27.65  34.53  17.436552   \n",
       "24  14.30  25.22  31.45  29.95  34.70  31.31  29.10  27.16  31.12  18.304255   \n",
       "25  16.03  30.02  34.72  32.49  35.12  35.82  33.60  32.73  34.11  18.084896   \n",
       "26  15.21  27.84  33.51  32.70  37.10  34.75  29.17  27.61  29.70  16.875334   \n",
       "27   9.50  22.99  30.11  31.82  32.80  33.88  19.43  36.02  44.45  18.224295   \n",
       "28  10.97  21.03  25.88  24.37  28.26  26.10  21.49  20.61  26.21  15.174002   \n",
       "29  12.63  22.61  27.12  27.53  31.02  29.98  27.37  22.92  25.82  14.869037   \n",
       "30   8.57  17.69  21.43  22.13  22.79  24.23  23.32  21.47  23.67   9.054276   \n",
       "31   7.19  15.84  18.83  17.86  15.93  16.22  15.55  15.47  24.20  10.094210   \n",
       "32   2.95   8.16   8.37   7.24   6.39   6.81   6.33   6.51  15.83   2.551653   \n",
       "33   1.71   4.94   4.62   3.24   3.42   1.19   3.41  -0.50  13.80   0.176253   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17  -0.337851   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17  -0.356946   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.032899   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.032862   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.049657   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.046683   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.034375   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.033595   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.029068   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.027590   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.022062   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.021784   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.013247   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  -0.013247   \n",
       "\n",
       "    ...      G10.9     M10.1      M10.2      M10.3      M10.4      M10.5  \\\n",
       "0   ...   0.149395 -0.041528   0.000302   0.008078   0.016066  -0.017140   \n",
       "1   ...   0.144378 -0.041507  -0.000265   0.007320   0.016043  -0.012500   \n",
       "2   ...   0.129127 -0.038472   0.000468   0.004296   0.020663   0.003450   \n",
       "3   ...   0.132215 -0.038721   0.001055   0.004686   0.020842  -0.000209   \n",
       "4   ...   0.137172 -0.035930   0.002635   0.004033   0.025469   0.010456   \n",
       "5   ...   0.135302 -0.035851   0.001361   0.002574   0.024645   0.018096   \n",
       "6   ...   0.160087 -0.033524   0.002369   0.001964   0.028151   0.031564   \n",
       "7   ...   0.163659 -0.032639  -0.000187   0.000117   0.026597   0.049007   \n",
       "8   ...   0.215700 -0.029885  -0.000476  -0.000193   0.028773   0.073080   \n",
       "9   ...   0.222411 -0.029366  -0.001399  -0.000954   0.028703   0.084407   \n",
       "10  ...   0.341714 -0.027023  -0.001101   0.000171   0.030941   0.106290   \n",
       "11  ...   0.347641 -0.027023  -0.001069  -0.000470   0.031179   0.108701   \n",
       "12  ...   0.621086 -0.025513  -0.000538   0.002177   0.031877   0.121943   \n",
       "13  ...   0.643642 -0.025494  -0.000602   0.001564   0.032157   0.124284   \n",
       "14  ...   1.131276 -0.024244  -0.000047   0.005877   0.031397   0.136935   \n",
       "15  ...   4.379683  0.163705   0.494363   0.601693   1.041145   1.500598   \n",
       "16  ...  19.271200  0.734207   2.128567   2.832492   4.939633   6.857623   \n",
       "17  ...  24.550512  1.365765   3.922879   5.464826   9.341690  12.560125   \n",
       "18  ...  38.503490  2.014142   5.884316   8.633399  14.371058  18.709196   \n",
       "19  ...  39.108696  2.881365   8.102743  12.145566  19.254927  23.877275   \n",
       "20  ...  42.739048  5.811303  14.109083  21.780197  30.765471  34.016811   \n",
       "21  ...  41.648247  7.431507  16.472395  25.372099  34.218544  36.105286   \n",
       "22  ...  55.579094  6.273179  15.135388  23.796743  34.090336  38.220592   \n",
       "23  ...  54.410767  4.662014  12.351225  19.290428  28.663797  33.311131   \n",
       "24  ...  53.146549  4.498437  12.136342  18.993401  28.219103  32.713757   \n",
       "25  ...  51.154129  5.975561  14.583498  22.823370  32.574348  36.256645   \n",
       "26  ...  49.737137  4.515851  12.225721  19.160999  28.279984  32.513809   \n",
       "27  ...  57.933491  2.634353   8.281919  13.020762  20.800072  25.858343   \n",
       "28  ...  46.328804  3.103366   9.494406  14.946491  22.910091  27.318787   \n",
       "29  ...  43.970497  3.583545  10.475468  16.401102  24.546824  28.593285   \n",
       "30  ...  35.764736  3.087681   9.518919  14.901230  22.243767  25.818138   \n",
       "31  ...  36.598442  1.868953   6.504136  10.268718  16.486364  20.491331   \n",
       "32  ...  23.005661  0.809692   3.212546   5.106972   8.887909  11.896382   \n",
       "33  ...  13.213209  0.164436   0.712153   1.096297   1.996767   2.970510   \n",
       "34  ...   0.848460 -0.016487   0.003272   0.024401   0.013321   0.191600   \n",
       "35  ...   0.760980 -0.016617   0.003443   0.023538   0.012023   0.180779   \n",
       "36  ...   0.138102 -0.016206   0.002598   0.023962   0.003491   0.164164   \n",
       "37  ...   0.129115 -0.016296   0.002733   0.023331   0.002821   0.157046   \n",
       "38  ...   0.057035 -0.015845   0.001641   0.023754  -0.006658   0.139430   \n",
       "39  ...   0.054806 -0.015937   0.001803   0.023112  -0.007182   0.132655   \n",
       "40  ...   0.031672 -0.015441   0.000475   0.023509  -0.017168   0.114629   \n",
       "41  ...   0.029004 -0.015507   0.000583   0.023005  -0.016919   0.111454   \n",
       "42  ...   0.000499 -0.014961  -0.000951   0.023484  -0.027308   0.093670   \n",
       "43  ...  -0.002206 -0.015071  -0.000705   0.022711  -0.027115   0.088210   \n",
       "44  ...  -0.022100 -0.014493  -0.002407   0.023060  -0.036813   0.072113   \n",
       "45  ...  -0.022312 -0.014511  -0.002379   0.022964  -0.036601   0.072205   \n",
       "46  ...  -0.023578 -0.013833  -0.004601   0.023774  -0.045343   0.062437   \n",
       "47  ...  -0.023578 -0.013833  -0.004601   0.023774  -0.045343   0.062437   \n",
       "\n",
       "        M10.6      M10.7      M10.8      M10.9  \n",
       "0   -0.085209   0.030080   0.084629   0.028655  \n",
       "1   -0.082663   0.030907   0.084478   0.028239  \n",
       "2   -0.074202   0.047721   0.119472   0.033769  \n",
       "3   -0.075112   0.047003   0.119001   0.033892  \n",
       "4   -0.069133   0.067605   0.168484   0.038271  \n",
       "5   -0.066619   0.069808   0.169381   0.037673  \n",
       "6   -0.059831   0.095915   0.238365   0.036981  \n",
       "7   -0.058177   0.104937   0.248831   0.038750  \n",
       "8   -0.052483   0.147891   0.365669   0.033603  \n",
       "9   -0.050417   0.156691   0.376725   0.036197  \n",
       "10  -0.042806   0.214321   0.555393   0.028220  \n",
       "11  -0.040352   0.214508   0.553944   0.028364  \n",
       "12  -0.033318   0.285069   0.810288   0.065609  \n",
       "13  -0.031204   0.284095   0.805341   0.062729  \n",
       "14  -0.023819   0.399394   1.245431   0.308215  \n",
       "15   2.120004   3.790680   6.150227   8.203325  \n",
       "16  10.415841  17.162670  22.688566  27.188646  \n",
       "17  18.327480  25.028303  28.206825  31.369656  \n",
       "18  26.057796  34.140823  39.545776  44.381779  \n",
       "19  31.638023  37.019367  40.691109  44.075199  \n",
       "20  40.795864  41.763218  45.237823  46.935425  \n",
       "21  41.835892  41.249744  44.111916  44.791996  \n",
       "22  45.829922  49.055729  54.976547  59.119770  \n",
       "23  41.311726  46.520962  53.719425  59.650356  \n",
       "24  40.526493  45.483704  53.010464  59.550854  \n",
       "25  43.469482  46.375790  52.477821  56.533741  \n",
       "26  40.035805  43.463879  49.646496  55.124012  \n",
       "27  33.983849  41.909557  50.748085  59.927212  \n",
       "28  34.717789  37.884441  42.442059  47.340366  \n",
       "29  35.703087  37.639545  41.975739  46.276333  \n",
       "30  32.206387  31.293978  32.851269  35.022396  \n",
       "31  27.074900  29.371634  31.224209  34.205914  \n",
       "32  16.975452  20.083115  19.681376  21.461214  \n",
       "33   4.452457   8.914912  12.319630  17.087610  \n",
       "34   0.051712   0.531222   1.430376   0.168569  \n",
       "35   0.040885   0.499768   1.372622   0.161684  \n",
       "36   0.021197   0.337528   0.886785   0.081286  \n",
       "37   0.014822   0.321467   0.858932   0.078351  \n",
       "38  -0.005214   0.218347   0.566952   0.075679  \n",
       "39  -0.010286   0.207120   0.547564   0.072850  \n",
       "40  -0.029525   0.151585   0.371098   0.078401  \n",
       "41  -0.030944   0.146305   0.361779   0.076389  \n",
       "42  -0.048224   0.125090   0.254564   0.079670  \n",
       "43  -0.049247   0.119385   0.244170   0.077199  \n",
       "44  -0.061266   0.120308   0.180855   0.081201  \n",
       "45  -0.060612   0.119864   0.180047   0.081042  \n",
       "46  -0.068863   0.133734   0.146622   0.088492  \n",
       "47  -0.068863   0.133734   0.146622   0.088492  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0.loc[res_0[res_0['L00.1'] == 0].index, ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9'\n",
    "                                            ,'C00.1','C00.2','C00.3','C00.4','C00.5','C00.6','C00.7','C00.8','C00.9'\n",
    "                                            ,'G00.1','G00.2','G00.3','G00.4','G00.5','G00.6','G00.7','G00.8','G00.9'\n",
    "                                            ,'M00.1','M00.2','M00.3','M00.4','M00.5','M00.6','M00.7','M00.8','M00.9'\n",
    "                                            ]] = 0\n",
    "res_1.loc[res_1[res_1['L10.1'] == 0].index, ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9'\n",
    "                                            ,'C10.1','C10.2','C10.3','C10.4','C10.5','C10.6','C10.7','C10.8','C10.9'\n",
    "                                            ,'G10.1','G10.2','G10.3','G10.4','G10.5','G10.6','G10.7','G10.8','G10.9'\n",
    "                                            ,'M10.1','M10.2','M10.3','M10.4','M10.5','M10.6','M10.7','M10.8','M10.9'\n",
    "                                            ]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L00.1</th>\n",
       "      <th>L00.2</th>\n",
       "      <th>L00.3</th>\n",
       "      <th>L00.4</th>\n",
       "      <th>L00.5</th>\n",
       "      <th>L00.6</th>\n",
       "      <th>L00.7</th>\n",
       "      <th>L00.8</th>\n",
       "      <th>L00.9</th>\n",
       "      <th>D00.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G00.9</th>\n",
       "      <th>M00.1</th>\n",
       "      <th>M00.2</th>\n",
       "      <th>M00.3</th>\n",
       "      <th>M00.4</th>\n",
       "      <th>M00.5</th>\n",
       "      <th>M00.6</th>\n",
       "      <th>M00.7</th>\n",
       "      <th>M00.8</th>\n",
       "      <th>M00.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.41</td>\n",
       "      <td>9.45</td>\n",
       "      <td>-0.077121</td>\n",
       "      <td>...</td>\n",
       "      <td>3.400994</td>\n",
       "      <td>0.264119</td>\n",
       "      <td>0.495739</td>\n",
       "      <td>0.674128</td>\n",
       "      <td>0.902048</td>\n",
       "      <td>1.047920</td>\n",
       "      <td>1.500841</td>\n",
       "      <td>2.034684</td>\n",
       "      <td>2.615589</td>\n",
       "      <td>5.133702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.03</td>\n",
       "      <td>5.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.84</td>\n",
       "      <td>9.94</td>\n",
       "      <td>14.36</td>\n",
       "      <td>17.51</td>\n",
       "      <td>23.51</td>\n",
       "      <td>1.256989</td>\n",
       "      <td>...</td>\n",
       "      <td>17.002937</td>\n",
       "      <td>1.072741</td>\n",
       "      <td>2.336011</td>\n",
       "      <td>3.130486</td>\n",
       "      <td>4.402256</td>\n",
       "      <td>5.309959</td>\n",
       "      <td>7.304523</td>\n",
       "      <td>11.356960</td>\n",
       "      <td>14.751767</td>\n",
       "      <td>23.926376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.36</td>\n",
       "      <td>5.12</td>\n",
       "      <td>8.09</td>\n",
       "      <td>9.77</td>\n",
       "      <td>11.39</td>\n",
       "      <td>9.42</td>\n",
       "      <td>15.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>27.11</td>\n",
       "      <td>2.738707</td>\n",
       "      <td>...</td>\n",
       "      <td>23.022182</td>\n",
       "      <td>1.942475</td>\n",
       "      <td>4.374655</td>\n",
       "      <td>5.956992</td>\n",
       "      <td>8.329583</td>\n",
       "      <td>9.918426</td>\n",
       "      <td>13.153889</td>\n",
       "      <td>17.420641</td>\n",
       "      <td>20.779972</td>\n",
       "      <td>30.774134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.67</td>\n",
       "      <td>13.71</td>\n",
       "      <td>17.53</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.56</td>\n",
       "      <td>13.35</td>\n",
       "      <td>19.91</td>\n",
       "      <td>33.34</td>\n",
       "      <td>8.618659</td>\n",
       "      <td>...</td>\n",
       "      <td>38.536106</td>\n",
       "      <td>2.854699</td>\n",
       "      <td>6.728202</td>\n",
       "      <td>9.450163</td>\n",
       "      <td>13.067003</td>\n",
       "      <td>15.378316</td>\n",
       "      <td>19.739746</td>\n",
       "      <td>27.678812</td>\n",
       "      <td>32.383354</td>\n",
       "      <td>45.449432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.49</td>\n",
       "      <td>17.88</td>\n",
       "      <td>17.84</td>\n",
       "      <td>23.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>18.34</td>\n",
       "      <td>15.95</td>\n",
       "      <td>20.77</td>\n",
       "      <td>33.51</td>\n",
       "      <td>11.597038</td>\n",
       "      <td>...</td>\n",
       "      <td>39.539894</td>\n",
       "      <td>3.939633</td>\n",
       "      <td>9.275344</td>\n",
       "      <td>13.170128</td>\n",
       "      <td>17.761337</td>\n",
       "      <td>20.245480</td>\n",
       "      <td>24.922413</td>\n",
       "      <td>30.494610</td>\n",
       "      <td>34.002228</td>\n",
       "      <td>46.085567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19.19</td>\n",
       "      <td>28.72</td>\n",
       "      <td>29.51</td>\n",
       "      <td>33.64</td>\n",
       "      <td>33.36</td>\n",
       "      <td>33.41</td>\n",
       "      <td>29.83</td>\n",
       "      <td>26.87</td>\n",
       "      <td>33.17</td>\n",
       "      <td>15.709170</td>\n",
       "      <td>...</td>\n",
       "      <td>43.025505</td>\n",
       "      <td>6.915888</td>\n",
       "      <td>15.905630</td>\n",
       "      <td>22.824675</td>\n",
       "      <td>28.863770</td>\n",
       "      <td>30.576330</td>\n",
       "      <td>34.374794</td>\n",
       "      <td>39.488232</td>\n",
       "      <td>40.977833</td>\n",
       "      <td>49.224030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.91</td>\n",
       "      <td>28.99</td>\n",
       "      <td>31.83</td>\n",
       "      <td>33.64</td>\n",
       "      <td>34.77</td>\n",
       "      <td>33.47</td>\n",
       "      <td>34.75</td>\n",
       "      <td>33.51</td>\n",
       "      <td>34.03</td>\n",
       "      <td>14.911612</td>\n",
       "      <td>...</td>\n",
       "      <td>41.715477</td>\n",
       "      <td>8.200056</td>\n",
       "      <td>18.293543</td>\n",
       "      <td>25.990767</td>\n",
       "      <td>32.111149</td>\n",
       "      <td>32.966881</td>\n",
       "      <td>35.990849</td>\n",
       "      <td>40.117195</td>\n",
       "      <td>40.644146</td>\n",
       "      <td>45.738392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.59</td>\n",
       "      <td>32.35</td>\n",
       "      <td>38.64</td>\n",
       "      <td>38.07</td>\n",
       "      <td>40.10</td>\n",
       "      <td>37.68</td>\n",
       "      <td>36.54</td>\n",
       "      <td>34.51</td>\n",
       "      <td>37.17</td>\n",
       "      <td>19.156044</td>\n",
       "      <td>...</td>\n",
       "      <td>54.869659</td>\n",
       "      <td>7.407323</td>\n",
       "      <td>17.336258</td>\n",
       "      <td>25.192966</td>\n",
       "      <td>31.933836</td>\n",
       "      <td>34.416416</td>\n",
       "      <td>38.496105</td>\n",
       "      <td>46.924267</td>\n",
       "      <td>50.507477</td>\n",
       "      <td>61.142986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.74</td>\n",
       "      <td>26.45</td>\n",
       "      <td>30.28</td>\n",
       "      <td>32.03</td>\n",
       "      <td>31.79</td>\n",
       "      <td>31.78</td>\n",
       "      <td>37.92</td>\n",
       "      <td>29.01</td>\n",
       "      <td>39.82</td>\n",
       "      <td>18.315992</td>\n",
       "      <td>...</td>\n",
       "      <td>54.997051</td>\n",
       "      <td>5.967392</td>\n",
       "      <td>14.389499</td>\n",
       "      <td>20.967751</td>\n",
       "      <td>27.111248</td>\n",
       "      <td>29.764145</td>\n",
       "      <td>34.362354</td>\n",
       "      <td>45.002655</td>\n",
       "      <td>49.748482</td>\n",
       "      <td>62.335693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.83</td>\n",
       "      <td>27.60</td>\n",
       "      <td>32.25</td>\n",
       "      <td>31.56</td>\n",
       "      <td>35.36</td>\n",
       "      <td>30.49</td>\n",
       "      <td>28.35</td>\n",
       "      <td>27.94</td>\n",
       "      <td>34.14</td>\n",
       "      <td>19.220716</td>\n",
       "      <td>...</td>\n",
       "      <td>53.178963</td>\n",
       "      <td>5.808964</td>\n",
       "      <td>14.334302</td>\n",
       "      <td>20.897337</td>\n",
       "      <td>26.873413</td>\n",
       "      <td>29.254448</td>\n",
       "      <td>33.697369</td>\n",
       "      <td>44.851906</td>\n",
       "      <td>49.863873</td>\n",
       "      <td>61.423630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>37.03</td>\n",
       "      <td>35.25</td>\n",
       "      <td>34.57</td>\n",
       "      <td>34.98</td>\n",
       "      <td>34.00</td>\n",
       "      <td>32.01</td>\n",
       "      <td>36.61</td>\n",
       "      <td>19.388786</td>\n",
       "      <td>...</td>\n",
       "      <td>51.653759</td>\n",
       "      <td>7.081882</td>\n",
       "      <td>16.910042</td>\n",
       "      <td>24.446466</td>\n",
       "      <td>30.742262</td>\n",
       "      <td>32.603485</td>\n",
       "      <td>36.421150</td>\n",
       "      <td>45.349258</td>\n",
       "      <td>48.891109</td>\n",
       "      <td>58.018311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.95</td>\n",
       "      <td>24.76</td>\n",
       "      <td>30.71</td>\n",
       "      <td>31.57</td>\n",
       "      <td>35.26</td>\n",
       "      <td>32.61</td>\n",
       "      <td>30.09</td>\n",
       "      <td>29.14</td>\n",
       "      <td>31.92</td>\n",
       "      <td>17.984758</td>\n",
       "      <td>...</td>\n",
       "      <td>49.190712</td>\n",
       "      <td>5.806954</td>\n",
       "      <td>14.585413</td>\n",
       "      <td>21.218033</td>\n",
       "      <td>27.027863</td>\n",
       "      <td>29.005163</td>\n",
       "      <td>33.175629</td>\n",
       "      <td>42.217506</td>\n",
       "      <td>46.677853</td>\n",
       "      <td>56.449585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.43</td>\n",
       "      <td>22.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.85</td>\n",
       "      <td>30.07</td>\n",
       "      <td>28.84</td>\n",
       "      <td>26.37</td>\n",
       "      <td>27.15</td>\n",
       "      <td>40.20</td>\n",
       "      <td>18.846518</td>\n",
       "      <td>...</td>\n",
       "      <td>56.211308</td>\n",
       "      <td>3.828525</td>\n",
       "      <td>10.088699</td>\n",
       "      <td>14.839977</td>\n",
       "      <td>19.860422</td>\n",
       "      <td>22.499790</td>\n",
       "      <td>27.445484</td>\n",
       "      <td>41.701839</td>\n",
       "      <td>48.696514</td>\n",
       "      <td>60.961891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.84</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.63</td>\n",
       "      <td>24.55</td>\n",
       "      <td>24.77</td>\n",
       "      <td>25.14</td>\n",
       "      <td>20.21</td>\n",
       "      <td>21.91</td>\n",
       "      <td>28.73</td>\n",
       "      <td>16.131306</td>\n",
       "      <td>...</td>\n",
       "      <td>44.686806</td>\n",
       "      <td>4.373532</td>\n",
       "      <td>11.635606</td>\n",
       "      <td>17.027676</td>\n",
       "      <td>22.105349</td>\n",
       "      <td>24.033895</td>\n",
       "      <td>28.395443</td>\n",
       "      <td>36.105194</td>\n",
       "      <td>40.074772</td>\n",
       "      <td>49.067268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>21.55</td>\n",
       "      <td>24.89</td>\n",
       "      <td>23.92</td>\n",
       "      <td>22.82</td>\n",
       "      <td>21.92</td>\n",
       "      <td>22.58</td>\n",
       "      <td>22.83</td>\n",
       "      <td>26.94</td>\n",
       "      <td>15.983805</td>\n",
       "      <td>...</td>\n",
       "      <td>41.940247</td>\n",
       "      <td>4.839461</td>\n",
       "      <td>12.652713</td>\n",
       "      <td>18.372803</td>\n",
       "      <td>23.488434</td>\n",
       "      <td>25.086071</td>\n",
       "      <td>29.135933</td>\n",
       "      <td>35.702171</td>\n",
       "      <td>39.347977</td>\n",
       "      <td>47.683128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.12</td>\n",
       "      <td>15.49</td>\n",
       "      <td>19.89</td>\n",
       "      <td>17.63</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.36</td>\n",
       "      <td>20.60</td>\n",
       "      <td>22.43</td>\n",
       "      <td>25.20</td>\n",
       "      <td>10.291277</td>\n",
       "      <td>...</td>\n",
       "      <td>33.889275</td>\n",
       "      <td>4.328761</td>\n",
       "      <td>11.729706</td>\n",
       "      <td>16.993879</td>\n",
       "      <td>21.594444</td>\n",
       "      <td>22.700935</td>\n",
       "      <td>26.435886</td>\n",
       "      <td>28.733198</td>\n",
       "      <td>30.939457</td>\n",
       "      <td>36.958694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.91</td>\n",
       "      <td>15.07</td>\n",
       "      <td>13.25</td>\n",
       "      <td>13.46</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.69</td>\n",
       "      <td>14.96</td>\n",
       "      <td>15.84</td>\n",
       "      <td>22.59</td>\n",
       "      <td>11.007571</td>\n",
       "      <td>...</td>\n",
       "      <td>37.509048</td>\n",
       "      <td>2.866959</td>\n",
       "      <td>8.060787</td>\n",
       "      <td>11.837075</td>\n",
       "      <td>15.822600</td>\n",
       "      <td>17.561096</td>\n",
       "      <td>21.727287</td>\n",
       "      <td>27.373491</td>\n",
       "      <td>30.011728</td>\n",
       "      <td>37.217583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.51</td>\n",
       "      <td>8.03</td>\n",
       "      <td>8.19</td>\n",
       "      <td>8.28</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.76</td>\n",
       "      <td>7.55</td>\n",
       "      <td>10.98</td>\n",
       "      <td>15.49</td>\n",
       "      <td>2.869643</td>\n",
       "      <td>...</td>\n",
       "      <td>23.327524</td>\n",
       "      <td>1.385374</td>\n",
       "      <td>4.052506</td>\n",
       "      <td>6.014334</td>\n",
       "      <td>8.485521</td>\n",
       "      <td>9.929159</td>\n",
       "      <td>13.326152</td>\n",
       "      <td>17.573149</td>\n",
       "      <td>19.661575</td>\n",
       "      <td>25.812149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.45</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>17.13</td>\n",
       "      <td>0.200522</td>\n",
       "      <td>...</td>\n",
       "      <td>12.147826</td>\n",
       "      <td>0.336648</td>\n",
       "      <td>0.880131</td>\n",
       "      <td>1.302681</td>\n",
       "      <td>1.897597</td>\n",
       "      <td>2.305977</td>\n",
       "      <td>3.439399</td>\n",
       "      <td>7.480927</td>\n",
       "      <td>10.735545</td>\n",
       "      <td>16.216372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L00.1  L00.2  L00.3  L00.4  L00.5  L00.6  L00.7  L00.8  L00.9      D00.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "15   0.94   1.71   1.25   1.66   4.10   3.29   7.04   7.41   9.45  -0.077121   \n",
       "16   3.03   5.28   6.82   8.90  10.84   9.94  14.36  17.51  23.51   1.256989   \n",
       "17   3.36   5.12   8.09   9.77  11.39   9.42  15.38  20.10  27.11   2.738707   \n",
       "18   8.67  13.71  17.53  19.96  20.85  17.56  13.35  19.91  33.34   8.618659   \n",
       "19  11.49  17.88  17.84  23.58  24.26  18.34  15.95  20.77  33.51  11.597038   \n",
       "20  19.19  28.72  29.51  33.64  33.36  33.41  29.83  26.87  33.17  15.709170   \n",
       "21  19.91  28.99  31.83  33.64  34.77  33.47  34.75  33.51  34.03  14.911612   \n",
       "22  22.59  32.35  38.64  38.07  40.10  37.68  36.54  34.51  37.17  19.156044   \n",
       "23  18.74  26.45  30.28  32.03  31.79  31.78  37.92  29.01  39.82  18.315992   \n",
       "24  19.83  27.60  32.25  31.56  35.36  30.49  28.35  27.94  34.14  19.220716   \n",
       "25  21.30  31.39  37.03  35.25  34.57  34.98  34.00  32.01  36.61  19.388786   \n",
       "26  18.95  24.76  30.71  31.57  35.26  32.61  30.09  29.14  31.92  17.984758   \n",
       "27  13.43  22.44  28.00  29.85  30.07  28.84  26.37  27.15  40.20  18.846518   \n",
       "28  11.84  19.29  23.63  24.55  24.77  25.14  20.21  21.91  28.73  16.131306   \n",
       "29  12.63  21.55  24.89  23.92  22.82  21.92  22.58  22.83  26.94  15.983805   \n",
       "30   8.12  15.49  19.89  17.63  18.69  17.36  20.60  22.43  25.20  10.291277   \n",
       "31   6.91  15.07  13.25  13.46  12.59  13.69  14.96  15.84  22.59  11.007571   \n",
       "32   4.51   8.03   8.19   8.28   9.75   8.76   7.55  10.98  15.49   2.869643   \n",
       "33   1.45   4.40   3.26   2.88   2.95   4.12   4.36   4.33  17.13   0.200522   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "\n",
       "    ...      G00.9     M00.1      M00.2      M00.3      M00.4      M00.5  \\\n",
       "0   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15  ...   3.400994  0.264119   0.495739   0.674128   0.902048   1.047920   \n",
       "16  ...  17.002937  1.072741   2.336011   3.130486   4.402256   5.309959   \n",
       "17  ...  23.022182  1.942475   4.374655   5.956992   8.329583   9.918426   \n",
       "18  ...  38.536106  2.854699   6.728202   9.450163  13.067003  15.378316   \n",
       "19  ...  39.539894  3.939633   9.275344  13.170128  17.761337  20.245480   \n",
       "20  ...  43.025505  6.915888  15.905630  22.824675  28.863770  30.576330   \n",
       "21  ...  41.715477  8.200056  18.293543  25.990767  32.111149  32.966881   \n",
       "22  ...  54.869659  7.407323  17.336258  25.192966  31.933836  34.416416   \n",
       "23  ...  54.997051  5.967392  14.389499  20.967751  27.111248  29.764145   \n",
       "24  ...  53.178963  5.808964  14.334302  20.897337  26.873413  29.254448   \n",
       "25  ...  51.653759  7.081882  16.910042  24.446466  30.742262  32.603485   \n",
       "26  ...  49.190712  5.806954  14.585413  21.218033  27.027863  29.005163   \n",
       "27  ...  56.211308  3.828525  10.088699  14.839977  19.860422  22.499790   \n",
       "28  ...  44.686806  4.373532  11.635606  17.027676  22.105349  24.033895   \n",
       "29  ...  41.940247  4.839461  12.652713  18.372803  23.488434  25.086071   \n",
       "30  ...  33.889275  4.328761  11.729706  16.993879  21.594444  22.700935   \n",
       "31  ...  37.509048  2.866959   8.060787  11.837075  15.822600  17.561096   \n",
       "32  ...  23.327524  1.385374   4.052506   6.014334   8.485521   9.929159   \n",
       "33  ...  12.147826  0.336648   0.880131   1.302681   1.897597   2.305977   \n",
       "34  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        M00.6      M00.7      M00.8      M00.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   1.500841   2.034684   2.615589   5.133702  \n",
       "16   7.304523  11.356960  14.751767  23.926376  \n",
       "17  13.153889  17.420641  20.779972  30.774134  \n",
       "18  19.739746  27.678812  32.383354  45.449432  \n",
       "19  24.922413  30.494610  34.002228  46.085567  \n",
       "20  34.374794  39.488232  40.977833  49.224030  \n",
       "21  35.990849  40.117195  40.644146  45.738392  \n",
       "22  38.496105  46.924267  50.507477  61.142986  \n",
       "23  34.362354  45.002655  49.748482  62.335693  \n",
       "24  33.697369  44.851906  49.863873  61.423630  \n",
       "25  36.421150  45.349258  48.891109  58.018311  \n",
       "26  33.175629  42.217506  46.677853  56.449585  \n",
       "27  27.445484  41.701839  48.696514  60.961891  \n",
       "28  28.395443  36.105194  40.074772  49.067268  \n",
       "29  29.135933  35.702171  39.347977  47.683128  \n",
       "30  26.435886  28.733198  30.939457  36.958694  \n",
       "31  21.727287  27.373491  30.011728  37.217583  \n",
       "32  13.326152  17.573149  19.661575  25.812149  \n",
       "33   3.439399   7.480927  10.735545  16.216372  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_0[:48]#.to_csv('0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L10.1</th>\n",
       "      <th>L10.2</th>\n",
       "      <th>L10.3</th>\n",
       "      <th>L10.4</th>\n",
       "      <th>L10.5</th>\n",
       "      <th>L10.6</th>\n",
       "      <th>L10.7</th>\n",
       "      <th>L10.8</th>\n",
       "      <th>L10.9</th>\n",
       "      <th>D10.1</th>\n",
       "      <th>...</th>\n",
       "      <th>G10.9</th>\n",
       "      <th>M10.1</th>\n",
       "      <th>M10.2</th>\n",
       "      <th>M10.3</th>\n",
       "      <th>M10.4</th>\n",
       "      <th>M10.5</th>\n",
       "      <th>M10.6</th>\n",
       "      <th>M10.7</th>\n",
       "      <th>M10.8</th>\n",
       "      <th>M10.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.80</td>\n",
       "      <td>9.87</td>\n",
       "      <td>-0.112919</td>\n",
       "      <td>...</td>\n",
       "      <td>4.379683</td>\n",
       "      <td>0.163705</td>\n",
       "      <td>0.494363</td>\n",
       "      <td>0.601693</td>\n",
       "      <td>1.041145</td>\n",
       "      <td>1.500598</td>\n",
       "      <td>2.120004</td>\n",
       "      <td>3.790680</td>\n",
       "      <td>6.150227</td>\n",
       "      <td>8.203325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.67</td>\n",
       "      <td>7.45</td>\n",
       "      <td>8.62</td>\n",
       "      <td>10.08</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.52</td>\n",
       "      <td>17.00</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.92</td>\n",
       "      <td>1.116548</td>\n",
       "      <td>...</td>\n",
       "      <td>19.271200</td>\n",
       "      <td>0.734207</td>\n",
       "      <td>2.128567</td>\n",
       "      <td>2.832492</td>\n",
       "      <td>4.939633</td>\n",
       "      <td>6.857623</td>\n",
       "      <td>10.415841</td>\n",
       "      <td>17.162670</td>\n",
       "      <td>22.688566</td>\n",
       "      <td>27.188646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.80</td>\n",
       "      <td>8.55</td>\n",
       "      <td>8.15</td>\n",
       "      <td>10.49</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.03</td>\n",
       "      <td>15.05</td>\n",
       "      <td>18.24</td>\n",
       "      <td>25.56</td>\n",
       "      <td>2.603195</td>\n",
       "      <td>...</td>\n",
       "      <td>24.550512</td>\n",
       "      <td>1.365765</td>\n",
       "      <td>3.922879</td>\n",
       "      <td>5.464826</td>\n",
       "      <td>9.341690</td>\n",
       "      <td>12.560125</td>\n",
       "      <td>18.327480</td>\n",
       "      <td>25.028303</td>\n",
       "      <td>28.206825</td>\n",
       "      <td>31.369656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.74</td>\n",
       "      <td>14.78</td>\n",
       "      <td>19.70</td>\n",
       "      <td>18.93</td>\n",
       "      <td>22.60</td>\n",
       "      <td>20.57</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.66</td>\n",
       "      <td>32.96</td>\n",
       "      <td>8.278840</td>\n",
       "      <td>...</td>\n",
       "      <td>38.503490</td>\n",
       "      <td>2.014142</td>\n",
       "      <td>5.884316</td>\n",
       "      <td>8.633399</td>\n",
       "      <td>14.371058</td>\n",
       "      <td>18.709196</td>\n",
       "      <td>26.057796</td>\n",
       "      <td>34.140823</td>\n",
       "      <td>39.545776</td>\n",
       "      <td>44.381779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.47</td>\n",
       "      <td>19.06</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.62</td>\n",
       "      <td>20.20</td>\n",
       "      <td>17.15</td>\n",
       "      <td>22.85</td>\n",
       "      <td>33.97</td>\n",
       "      <td>10.936314</td>\n",
       "      <td>...</td>\n",
       "      <td>39.108696</td>\n",
       "      <td>2.881365</td>\n",
       "      <td>8.102743</td>\n",
       "      <td>12.145566</td>\n",
       "      <td>19.254927</td>\n",
       "      <td>23.877275</td>\n",
       "      <td>31.638023</td>\n",
       "      <td>37.019367</td>\n",
       "      <td>40.691109</td>\n",
       "      <td>44.075199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.33</td>\n",
       "      <td>24.78</td>\n",
       "      <td>24.52</td>\n",
       "      <td>23.02</td>\n",
       "      <td>28.58</td>\n",
       "      <td>28.55</td>\n",
       "      <td>25.06</td>\n",
       "      <td>26.39</td>\n",
       "      <td>27.24</td>\n",
       "      <td>14.450748</td>\n",
       "      <td>...</td>\n",
       "      <td>42.739048</td>\n",
       "      <td>5.811303</td>\n",
       "      <td>14.109083</td>\n",
       "      <td>21.780197</td>\n",
       "      <td>30.765471</td>\n",
       "      <td>34.016811</td>\n",
       "      <td>40.795864</td>\n",
       "      <td>41.763218</td>\n",
       "      <td>45.237823</td>\n",
       "      <td>46.935425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.26</td>\n",
       "      <td>26.20</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.70</td>\n",
       "      <td>29.22</td>\n",
       "      <td>31.11</td>\n",
       "      <td>31.05</td>\n",
       "      <td>31.59</td>\n",
       "      <td>32.45</td>\n",
       "      <td>13.226276</td>\n",
       "      <td>...</td>\n",
       "      <td>41.648247</td>\n",
       "      <td>7.431507</td>\n",
       "      <td>16.472395</td>\n",
       "      <td>25.372099</td>\n",
       "      <td>34.218544</td>\n",
       "      <td>36.105286</td>\n",
       "      <td>41.835892</td>\n",
       "      <td>41.249744</td>\n",
       "      <td>44.111916</td>\n",
       "      <td>44.791996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.96</td>\n",
       "      <td>29.89</td>\n",
       "      <td>33.58</td>\n",
       "      <td>33.28</td>\n",
       "      <td>38.13</td>\n",
       "      <td>35.95</td>\n",
       "      <td>34.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.78</td>\n",
       "      <td>17.926546</td>\n",
       "      <td>...</td>\n",
       "      <td>55.579094</td>\n",
       "      <td>6.273179</td>\n",
       "      <td>15.135388</td>\n",
       "      <td>23.796743</td>\n",
       "      <td>34.090336</td>\n",
       "      <td>38.220592</td>\n",
       "      <td>45.829922</td>\n",
       "      <td>49.055729</td>\n",
       "      <td>54.976547</td>\n",
       "      <td>59.119770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.24</td>\n",
       "      <td>24.45</td>\n",
       "      <td>29.77</td>\n",
       "      <td>29.36</td>\n",
       "      <td>34.56</td>\n",
       "      <td>30.81</td>\n",
       "      <td>26.23</td>\n",
       "      <td>27.65</td>\n",
       "      <td>34.53</td>\n",
       "      <td>17.436552</td>\n",
       "      <td>...</td>\n",
       "      <td>54.410767</td>\n",
       "      <td>4.662014</td>\n",
       "      <td>12.351225</td>\n",
       "      <td>19.290428</td>\n",
       "      <td>28.663797</td>\n",
       "      <td>33.311131</td>\n",
       "      <td>41.311726</td>\n",
       "      <td>46.520962</td>\n",
       "      <td>53.719425</td>\n",
       "      <td>59.650356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.30</td>\n",
       "      <td>25.22</td>\n",
       "      <td>31.45</td>\n",
       "      <td>29.95</td>\n",
       "      <td>34.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>29.10</td>\n",
       "      <td>27.16</td>\n",
       "      <td>31.12</td>\n",
       "      <td>18.304255</td>\n",
       "      <td>...</td>\n",
       "      <td>53.146549</td>\n",
       "      <td>4.498437</td>\n",
       "      <td>12.136342</td>\n",
       "      <td>18.993401</td>\n",
       "      <td>28.219103</td>\n",
       "      <td>32.713757</td>\n",
       "      <td>40.526493</td>\n",
       "      <td>45.483704</td>\n",
       "      <td>53.010464</td>\n",
       "      <td>59.550854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.03</td>\n",
       "      <td>30.02</td>\n",
       "      <td>34.72</td>\n",
       "      <td>32.49</td>\n",
       "      <td>35.12</td>\n",
       "      <td>35.82</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "      <td>18.084896</td>\n",
       "      <td>...</td>\n",
       "      <td>51.154129</td>\n",
       "      <td>5.975561</td>\n",
       "      <td>14.583498</td>\n",
       "      <td>22.823370</td>\n",
       "      <td>32.574348</td>\n",
       "      <td>36.256645</td>\n",
       "      <td>43.469482</td>\n",
       "      <td>46.375790</td>\n",
       "      <td>52.477821</td>\n",
       "      <td>56.533741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.21</td>\n",
       "      <td>27.84</td>\n",
       "      <td>33.51</td>\n",
       "      <td>32.70</td>\n",
       "      <td>37.10</td>\n",
       "      <td>34.75</td>\n",
       "      <td>29.17</td>\n",
       "      <td>27.61</td>\n",
       "      <td>29.70</td>\n",
       "      <td>16.875334</td>\n",
       "      <td>...</td>\n",
       "      <td>49.737137</td>\n",
       "      <td>4.515851</td>\n",
       "      <td>12.225721</td>\n",
       "      <td>19.160999</td>\n",
       "      <td>28.279984</td>\n",
       "      <td>32.513809</td>\n",
       "      <td>40.035805</td>\n",
       "      <td>43.463879</td>\n",
       "      <td>49.646496</td>\n",
       "      <td>55.124012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.50</td>\n",
       "      <td>22.99</td>\n",
       "      <td>30.11</td>\n",
       "      <td>31.82</td>\n",
       "      <td>32.80</td>\n",
       "      <td>33.88</td>\n",
       "      <td>19.43</td>\n",
       "      <td>36.02</td>\n",
       "      <td>44.45</td>\n",
       "      <td>18.224295</td>\n",
       "      <td>...</td>\n",
       "      <td>57.933491</td>\n",
       "      <td>2.634353</td>\n",
       "      <td>8.281919</td>\n",
       "      <td>13.020762</td>\n",
       "      <td>20.800072</td>\n",
       "      <td>25.858343</td>\n",
       "      <td>33.983849</td>\n",
       "      <td>41.909557</td>\n",
       "      <td>50.748085</td>\n",
       "      <td>59.927212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.97</td>\n",
       "      <td>21.03</td>\n",
       "      <td>25.88</td>\n",
       "      <td>24.37</td>\n",
       "      <td>28.26</td>\n",
       "      <td>26.10</td>\n",
       "      <td>21.49</td>\n",
       "      <td>20.61</td>\n",
       "      <td>26.21</td>\n",
       "      <td>15.174002</td>\n",
       "      <td>...</td>\n",
       "      <td>46.328804</td>\n",
       "      <td>3.103366</td>\n",
       "      <td>9.494406</td>\n",
       "      <td>14.946491</td>\n",
       "      <td>22.910091</td>\n",
       "      <td>27.318787</td>\n",
       "      <td>34.717789</td>\n",
       "      <td>37.884441</td>\n",
       "      <td>42.442059</td>\n",
       "      <td>47.340366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.63</td>\n",
       "      <td>22.61</td>\n",
       "      <td>27.12</td>\n",
       "      <td>27.53</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.98</td>\n",
       "      <td>27.37</td>\n",
       "      <td>22.92</td>\n",
       "      <td>25.82</td>\n",
       "      <td>14.869037</td>\n",
       "      <td>...</td>\n",
       "      <td>43.970497</td>\n",
       "      <td>3.583545</td>\n",
       "      <td>10.475468</td>\n",
       "      <td>16.401102</td>\n",
       "      <td>24.546824</td>\n",
       "      <td>28.593285</td>\n",
       "      <td>35.703087</td>\n",
       "      <td>37.639545</td>\n",
       "      <td>41.975739</td>\n",
       "      <td>46.276333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.57</td>\n",
       "      <td>17.69</td>\n",
       "      <td>21.43</td>\n",
       "      <td>22.13</td>\n",
       "      <td>22.79</td>\n",
       "      <td>24.23</td>\n",
       "      <td>23.32</td>\n",
       "      <td>21.47</td>\n",
       "      <td>23.67</td>\n",
       "      <td>9.054276</td>\n",
       "      <td>...</td>\n",
       "      <td>35.764736</td>\n",
       "      <td>3.087681</td>\n",
       "      <td>9.518919</td>\n",
       "      <td>14.901230</td>\n",
       "      <td>22.243767</td>\n",
       "      <td>25.818138</td>\n",
       "      <td>32.206387</td>\n",
       "      <td>31.293978</td>\n",
       "      <td>32.851269</td>\n",
       "      <td>35.022396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.19</td>\n",
       "      <td>15.84</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.86</td>\n",
       "      <td>15.93</td>\n",
       "      <td>16.22</td>\n",
       "      <td>15.55</td>\n",
       "      <td>15.47</td>\n",
       "      <td>24.20</td>\n",
       "      <td>10.094210</td>\n",
       "      <td>...</td>\n",
       "      <td>36.598442</td>\n",
       "      <td>1.868953</td>\n",
       "      <td>6.504136</td>\n",
       "      <td>10.268718</td>\n",
       "      <td>16.486364</td>\n",
       "      <td>20.491331</td>\n",
       "      <td>27.074900</td>\n",
       "      <td>29.371634</td>\n",
       "      <td>31.224209</td>\n",
       "      <td>34.205914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.95</td>\n",
       "      <td>8.16</td>\n",
       "      <td>8.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.39</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.33</td>\n",
       "      <td>6.51</td>\n",
       "      <td>15.83</td>\n",
       "      <td>2.551653</td>\n",
       "      <td>...</td>\n",
       "      <td>23.005661</td>\n",
       "      <td>0.809692</td>\n",
       "      <td>3.212546</td>\n",
       "      <td>5.106972</td>\n",
       "      <td>8.887909</td>\n",
       "      <td>11.896382</td>\n",
       "      <td>16.975452</td>\n",
       "      <td>20.083115</td>\n",
       "      <td>19.681376</td>\n",
       "      <td>21.461214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.71</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>13.80</td>\n",
       "      <td>0.176253</td>\n",
       "      <td>...</td>\n",
       "      <td>13.213209</td>\n",
       "      <td>0.164436</td>\n",
       "      <td>0.712153</td>\n",
       "      <td>1.096297</td>\n",
       "      <td>1.996767</td>\n",
       "      <td>2.970510</td>\n",
       "      <td>4.452457</td>\n",
       "      <td>8.914912</td>\n",
       "      <td>12.319630</td>\n",
       "      <td>17.087610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    L10.1  L10.2  L10.3  L10.4  L10.5  L10.6  L10.7  L10.8  L10.9      D10.1  \\\n",
       "0    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "1    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "2    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "3    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "4    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "5    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "6    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "7    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "8    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "9    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "10   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "11   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "12   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "13   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "14   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "15   0.48   0.70   1.80   2.18   0.98   3.56   2.99   6.80   9.87  -0.112919   \n",
       "16   2.67   7.45   8.62  10.08  10.06  11.52  17.00  19.79  20.92   1.116548   \n",
       "17   3.80   8.55   8.15  10.49  10.84  12.03  15.05  18.24  25.56   2.603195   \n",
       "18   7.74  14.78  19.70  18.93  22.60  20.57  16.19  15.66  32.96   8.278840   \n",
       "19   9.47  19.06  21.52  21.87  23.62  20.20  17.15  22.85  33.97  10.936314   \n",
       "20  14.33  24.78  24.52  23.02  28.58  28.55  25.06  26.39  27.24  14.450748   \n",
       "21  15.26  26.20  24.57  24.70  29.22  31.11  31.05  31.59  32.45  13.226276   \n",
       "22  15.96  29.89  33.58  33.28  38.13  35.95  34.24  33.24  33.78  17.926546   \n",
       "23  14.24  24.45  29.77  29.36  34.56  30.81  26.23  27.65  34.53  17.436552   \n",
       "24  14.30  25.22  31.45  29.95  34.70  31.31  29.10  27.16  31.12  18.304255   \n",
       "25  16.03  30.02  34.72  32.49  35.12  35.82  33.60  32.73  34.11  18.084896   \n",
       "26  15.21  27.84  33.51  32.70  37.10  34.75  29.17  27.61  29.70  16.875334   \n",
       "27   9.50  22.99  30.11  31.82  32.80  33.88  19.43  36.02  44.45  18.224295   \n",
       "28  10.97  21.03  25.88  24.37  28.26  26.10  21.49  20.61  26.21  15.174002   \n",
       "29  12.63  22.61  27.12  27.53  31.02  29.98  27.37  22.92  25.82  14.869037   \n",
       "30   8.57  17.69  21.43  22.13  22.79  24.23  23.32  21.47  23.67   9.054276   \n",
       "31   7.19  15.84  18.83  17.86  15.93  16.22  15.55  15.47  24.20  10.094210   \n",
       "32   2.95   8.16   8.37   7.24   6.39   6.81   6.33   6.51  15.83   2.551653   \n",
       "33   1.71   4.94   4.62   3.24   3.42   1.19   3.41  -0.50  13.80   0.176253   \n",
       "34   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17   0.000000   \n",
       "35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.17   0.000000   \n",
       "36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "37   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "38   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "39   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "40   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "41   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "42   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "43   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "44   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "45   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "46   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.000000   \n",
       "\n",
       "    ...      G10.9     M10.1      M10.2      M10.3      M10.4      M10.5  \\\n",
       "0   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9   ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15  ...   4.379683  0.163705   0.494363   0.601693   1.041145   1.500598   \n",
       "16  ...  19.271200  0.734207   2.128567   2.832492   4.939633   6.857623   \n",
       "17  ...  24.550512  1.365765   3.922879   5.464826   9.341690  12.560125   \n",
       "18  ...  38.503490  2.014142   5.884316   8.633399  14.371058  18.709196   \n",
       "19  ...  39.108696  2.881365   8.102743  12.145566  19.254927  23.877275   \n",
       "20  ...  42.739048  5.811303  14.109083  21.780197  30.765471  34.016811   \n",
       "21  ...  41.648247  7.431507  16.472395  25.372099  34.218544  36.105286   \n",
       "22  ...  55.579094  6.273179  15.135388  23.796743  34.090336  38.220592   \n",
       "23  ...  54.410767  4.662014  12.351225  19.290428  28.663797  33.311131   \n",
       "24  ...  53.146549  4.498437  12.136342  18.993401  28.219103  32.713757   \n",
       "25  ...  51.154129  5.975561  14.583498  22.823370  32.574348  36.256645   \n",
       "26  ...  49.737137  4.515851  12.225721  19.160999  28.279984  32.513809   \n",
       "27  ...  57.933491  2.634353   8.281919  13.020762  20.800072  25.858343   \n",
       "28  ...  46.328804  3.103366   9.494406  14.946491  22.910091  27.318787   \n",
       "29  ...  43.970497  3.583545  10.475468  16.401102  24.546824  28.593285   \n",
       "30  ...  35.764736  3.087681   9.518919  14.901230  22.243767  25.818138   \n",
       "31  ...  36.598442  1.868953   6.504136  10.268718  16.486364  20.491331   \n",
       "32  ...  23.005661  0.809692   3.212546   5.106972   8.887909  11.896382   \n",
       "33  ...  13.213209  0.164436   0.712153   1.096297   1.996767   2.970510   \n",
       "34  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  ...   0.000000  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        M10.6      M10.7      M10.8      M10.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   2.120004   3.790680   6.150227   8.203325  \n",
       "16  10.415841  17.162670  22.688566  27.188646  \n",
       "17  18.327480  25.028303  28.206825  31.369656  \n",
       "18  26.057796  34.140823  39.545776  44.381779  \n",
       "19  31.638023  37.019367  40.691109  44.075199  \n",
       "20  40.795864  41.763218  45.237823  46.935425  \n",
       "21  41.835892  41.249744  44.111916  44.791996  \n",
       "22  45.829922  49.055729  54.976547  59.119770  \n",
       "23  41.311726  46.520962  53.719425  59.650356  \n",
       "24  40.526493  45.483704  53.010464  59.550854  \n",
       "25  43.469482  46.375790  52.477821  56.533741  \n",
       "26  40.035805  43.463879  49.646496  55.124012  \n",
       "27  33.983849  41.909557  50.748085  59.927212  \n",
       "28  34.717789  37.884441  42.442059  47.340366  \n",
       "29  35.703087  37.639545  41.975739  46.276333  \n",
       "30  32.206387  31.293978  32.851269  35.022396  \n",
       "31  27.074900  29.371634  31.224209  34.205914  \n",
       "32  16.975452  20.083115  19.681376  21.461214  \n",
       "33   4.452457   8.914912  12.319630  17.087610  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  \n",
       "\n",
       "[48 rows x 45 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1[:48]#.to_csv('0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    res_0[\"L00.\"+str(i)] = (res_0[\"L00.\"+str(i)] + res_0[\"D00.\"+str(i)] + res_0[\"C00.\"+str(i)] + res_0[\"G00.\"+str(i)] + res_0[\"M00.\"+str(i)])/5\n",
    "    res_1[\"L10.\"+str(i)] = (res_1[\"L10.\"+str(i)] + res_1[\"D10.\"+str(i)] + res_1[\"C10.\"+str(i)] + res_1[\"G10.\"+str(i)] + res_1[\"M10.\"+str(i)])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.csv_Day7_2h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.csv_Day7_3h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.csv_Day7_3h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.csv_Day7_4h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.csv_Day7_4h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.csv_Day7_5h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.csv_Day7_5h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.csv_Day7_6h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.csv_Day7_6h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.csv_Day7_7h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.csv_Day7_7h30m</td>\n",
       "      <td>0.185817</td>\n",
       "      <td>0.363852</td>\n",
       "      <td>0.545528</td>\n",
       "      <td>0.781844</td>\n",
       "      <td>1.264862</td>\n",
       "      <td>1.393540</td>\n",
       "      <td>1.961227</td>\n",
       "      <td>2.813347</td>\n",
       "      <td>5.296004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.csv_Day7_8h00m</td>\n",
       "      <td>1.299114</td>\n",
       "      <td>2.522978</td>\n",
       "      <td>3.821571</td>\n",
       "      <td>4.857165</td>\n",
       "      <td>6.782994</td>\n",
       "      <td>8.371122</td>\n",
       "      <td>10.363585</td>\n",
       "      <td>14.039038</td>\n",
       "      <td>20.090826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.csv_Day7_8h30m</td>\n",
       "      <td>2.660473</td>\n",
       "      <td>5.028236</td>\n",
       "      <td>7.320207</td>\n",
       "      <td>8.627370</td>\n",
       "      <td>11.197134</td>\n",
       "      <td>13.131270</td>\n",
       "      <td>15.317370</td>\n",
       "      <td>18.912939</td>\n",
       "      <td>25.129999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.csv_Day7_9h00m</td>\n",
       "      <td>5.447591</td>\n",
       "      <td>10.046474</td>\n",
       "      <td>13.804118</td>\n",
       "      <td>15.603951</td>\n",
       "      <td>19.749237</td>\n",
       "      <td>22.582904</td>\n",
       "      <td>25.968045</td>\n",
       "      <td>30.307746</td>\n",
       "      <td>38.372702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.csv_Day7_9h30m</td>\n",
       "      <td>7.353027</td>\n",
       "      <td>13.054789</td>\n",
       "      <td>16.892877</td>\n",
       "      <td>18.913115</td>\n",
       "      <td>22.307037</td>\n",
       "      <td>25.464379</td>\n",
       "      <td>27.794313</td>\n",
       "      <td>31.081791</td>\n",
       "      <td>38.145456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.csv_Day7_10h00m</td>\n",
       "      <td>11.795020</td>\n",
       "      <td>20.463100</td>\n",
       "      <td>25.524155</td>\n",
       "      <td>28.139260</td>\n",
       "      <td>31.563166</td>\n",
       "      <td>34.877855</td>\n",
       "      <td>36.331433</td>\n",
       "      <td>36.592887</td>\n",
       "      <td>40.305685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.csv_Day7_10h30m</td>\n",
       "      <td>12.725662</td>\n",
       "      <td>21.944979</td>\n",
       "      <td>27.092552</td>\n",
       "      <td>30.198556</td>\n",
       "      <td>33.264629</td>\n",
       "      <td>36.283340</td>\n",
       "      <td>38.070183</td>\n",
       "      <td>38.143927</td>\n",
       "      <td>40.308466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.csv_Day7_11h00m</td>\n",
       "      <td>13.761231</td>\n",
       "      <td>23.976542</td>\n",
       "      <td>30.563281</td>\n",
       "      <td>33.265128</td>\n",
       "      <td>38.672901</td>\n",
       "      <td>41.444053</td>\n",
       "      <td>44.494247</td>\n",
       "      <td>46.770600</td>\n",
       "      <td>51.663754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.csv_Day7_11h30m</td>\n",
       "      <td>11.989256</td>\n",
       "      <td>21.319746</td>\n",
       "      <td>27.140757</td>\n",
       "      <td>29.806807</td>\n",
       "      <td>35.351345</td>\n",
       "      <td>38.493510</td>\n",
       "      <td>42.500312</td>\n",
       "      <td>44.680337</td>\n",
       "      <td>51.538718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.csv_Day7_12h00m</td>\n",
       "      <td>12.427929</td>\n",
       "      <td>21.053785</td>\n",
       "      <td>27.002057</td>\n",
       "      <td>29.599555</td>\n",
       "      <td>34.954665</td>\n",
       "      <td>38.063557</td>\n",
       "      <td>41.556615</td>\n",
       "      <td>43.806406</td>\n",
       "      <td>49.403909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.csv_Day7_12h30m</td>\n",
       "      <td>13.662717</td>\n",
       "      <td>22.813055</td>\n",
       "      <td>28.867277</td>\n",
       "      <td>31.820636</td>\n",
       "      <td>36.132842</td>\n",
       "      <td>39.367235</td>\n",
       "      <td>42.352715</td>\n",
       "      <td>44.143047</td>\n",
       "      <td>47.558968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.csv_Day7_13h00m</td>\n",
       "      <td>12.213417</td>\n",
       "      <td>20.287006</td>\n",
       "      <td>25.487246</td>\n",
       "      <td>28.329032</td>\n",
       "      <td>32.961534</td>\n",
       "      <td>36.492861</td>\n",
       "      <td>39.381068</td>\n",
       "      <td>41.014776</td>\n",
       "      <td>45.025295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.csv_Day7_13h30m</td>\n",
       "      <td>10.393670</td>\n",
       "      <td>17.387978</td>\n",
       "      <td>22.208781</td>\n",
       "      <td>24.696652</td>\n",
       "      <td>30.011204</td>\n",
       "      <td>34.232865</td>\n",
       "      <td>39.167010</td>\n",
       "      <td>42.226905</td>\n",
       "      <td>49.108548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.csv_Day7_14h00m</td>\n",
       "      <td>10.169520</td>\n",
       "      <td>17.084926</td>\n",
       "      <td>21.101204</td>\n",
       "      <td>23.007008</td>\n",
       "      <td>27.745229</td>\n",
       "      <td>31.289501</td>\n",
       "      <td>33.777180</td>\n",
       "      <td>35.187942</td>\n",
       "      <td>39.245539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.csv_Day7_14h30m</td>\n",
       "      <td>10.583743</td>\n",
       "      <td>17.763587</td>\n",
       "      <td>21.681285</td>\n",
       "      <td>23.825459</td>\n",
       "      <td>28.425690</td>\n",
       "      <td>31.506177</td>\n",
       "      <td>33.820998</td>\n",
       "      <td>34.824745</td>\n",
       "      <td>38.176248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.csv_Day7_15h00m</td>\n",
       "      <td>8.384550</td>\n",
       "      <td>15.604373</td>\n",
       "      <td>18.321465</td>\n",
       "      <td>20.855038</td>\n",
       "      <td>24.984164</td>\n",
       "      <td>26.946173</td>\n",
       "      <td>27.869348</td>\n",
       "      <td>28.671082</td>\n",
       "      <td>30.973555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.csv_Day7_15h30m</td>\n",
       "      <td>7.128032</td>\n",
       "      <td>12.446599</td>\n",
       "      <td>15.135545</td>\n",
       "      <td>16.583525</td>\n",
       "      <td>20.283015</td>\n",
       "      <td>23.052037</td>\n",
       "      <td>24.709997</td>\n",
       "      <td>26.261571</td>\n",
       "      <td>30.208102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.csv_Day7_16h00m</td>\n",
       "      <td>3.189356</td>\n",
       "      <td>5.377254</td>\n",
       "      <td>7.300349</td>\n",
       "      <td>8.633982</td>\n",
       "      <td>11.354072</td>\n",
       "      <td>12.764828</td>\n",
       "      <td>13.932283</td>\n",
       "      <td>15.766803</td>\n",
       "      <td>18.927409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.csv_Day7_16h30m</td>\n",
       "      <td>0.697887</td>\n",
       "      <td>1.195986</td>\n",
       "      <td>1.508474</td>\n",
       "      <td>1.881144</td>\n",
       "      <td>2.797929</td>\n",
       "      <td>3.720667</td>\n",
       "      <td>5.248474</td>\n",
       "      <td>8.107316</td>\n",
       "      <td>11.643548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.csv_Day7_17h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.csv_Day7_17h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.csv_Day7_18h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.csv_Day7_18h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.csv_Day7_19h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.csv_Day7_19h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.csv_Day7_20h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.csv_Day7_20h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.csv_Day7_21h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.csv_Day7_21h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.csv_Day7_22h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.csv_Day7_22h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.csv_Day7_23h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.csv_Day7_23h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id      q_0.1      q_0.2      q_0.3      q_0.4      q_0.5  \\\n",
       "0    0.csv_Day7_0h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1    0.csv_Day7_0h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2    0.csv_Day7_1h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3    0.csv_Day7_1h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4    0.csv_Day7_2h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5    0.csv_Day7_2h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6    0.csv_Day7_3h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7    0.csv_Day7_3h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8    0.csv_Day7_4h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9    0.csv_Day7_4h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10   0.csv_Day7_5h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11   0.csv_Day7_5h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12   0.csv_Day7_6h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13   0.csv_Day7_6h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14   0.csv_Day7_7h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15   0.csv_Day7_7h30m   0.185817   0.363852   0.545528   0.781844   1.264862   \n",
       "16   0.csv_Day7_8h00m   1.299114   2.522978   3.821571   4.857165   6.782994   \n",
       "17   0.csv_Day7_8h30m   2.660473   5.028236   7.320207   8.627370  11.197134   \n",
       "18   0.csv_Day7_9h00m   5.447591  10.046474  13.804118  15.603951  19.749237   \n",
       "19   0.csv_Day7_9h30m   7.353027  13.054789  16.892877  18.913115  22.307037   \n",
       "20  0.csv_Day7_10h00m  11.795020  20.463100  25.524155  28.139260  31.563166   \n",
       "21  0.csv_Day7_10h30m  12.725662  21.944979  27.092552  30.198556  33.264629   \n",
       "22  0.csv_Day7_11h00m  13.761231  23.976542  30.563281  33.265128  38.672901   \n",
       "23  0.csv_Day7_11h30m  11.989256  21.319746  27.140757  29.806807  35.351345   \n",
       "24  0.csv_Day7_12h00m  12.427929  21.053785  27.002057  29.599555  34.954665   \n",
       "25  0.csv_Day7_12h30m  13.662717  22.813055  28.867277  31.820636  36.132842   \n",
       "26  0.csv_Day7_13h00m  12.213417  20.287006  25.487246  28.329032  32.961534   \n",
       "27  0.csv_Day7_13h30m  10.393670  17.387978  22.208781  24.696652  30.011204   \n",
       "28  0.csv_Day7_14h00m  10.169520  17.084926  21.101204  23.007008  27.745229   \n",
       "29  0.csv_Day7_14h30m  10.583743  17.763587  21.681285  23.825459  28.425690   \n",
       "30  0.csv_Day7_15h00m   8.384550  15.604373  18.321465  20.855038  24.984164   \n",
       "31  0.csv_Day7_15h30m   7.128032  12.446599  15.135545  16.583525  20.283015   \n",
       "32  0.csv_Day7_16h00m   3.189356   5.377254   7.300349   8.633982  11.354072   \n",
       "33  0.csv_Day7_16h30m   0.697887   1.195986   1.508474   1.881144   2.797929   \n",
       "34  0.csv_Day7_17h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  0.csv_Day7_17h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  0.csv_Day7_18h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  0.csv_Day7_18h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  0.csv_Day7_19h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  0.csv_Day7_19h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  0.csv_Day7_20h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  0.csv_Day7_20h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  0.csv_Day7_21h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  0.csv_Day7_21h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  0.csv_Day7_22h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  0.csv_Day7_22h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  0.csv_Day7_23h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  0.csv_Day7_23h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        q_0.6      q_0.7      q_0.8      q_0.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   1.393540   1.961227   2.813347   5.296004  \n",
       "16   8.371122  10.363585  14.039038  20.090826  \n",
       "17  13.131270  15.317370  18.912939  25.129999  \n",
       "18  22.582904  25.968045  30.307746  38.372702  \n",
       "19  25.464379  27.794313  31.081791  38.145456  \n",
       "20  34.877855  36.331433  36.592887  40.305685  \n",
       "21  36.283340  38.070183  38.143927  40.308466  \n",
       "22  41.444053  44.494247  46.770600  51.663754  \n",
       "23  38.493510  42.500312  44.680337  51.538718  \n",
       "24  38.063557  41.556615  43.806406  49.403909  \n",
       "25  39.367235  42.352715  44.143047  47.558968  \n",
       "26  36.492861  39.381068  41.014776  45.025295  \n",
       "27  34.232865  39.167010  42.226905  49.108548  \n",
       "28  31.289501  33.777180  35.187942  39.245539  \n",
       "29  31.506177  33.820998  34.824745  38.176248  \n",
       "30  26.946173  27.869348  28.671082  30.973555  \n",
       "31  23.052037  24.709997  26.261571  30.208102  \n",
       "32  12.764828  13.932283  15.766803  18.927409  \n",
       "33   3.720667   5.248474   8.107316  11.643548  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = res_0[['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']].values\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = res_1[['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']].values\n",
    "submission[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission/submission_20210124-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
