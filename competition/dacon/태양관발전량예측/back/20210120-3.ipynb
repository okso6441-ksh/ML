{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, is_train=True):\n",
    "    \n",
    "    temp = data.copy()\n",
    "    temp = temp[['Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "\n",
    "    if is_train==True:          \n",
    "    \n",
    "        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')\n",
    "        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill')\n",
    "        temp = temp.dropna()\n",
    "        \n",
    "        return temp.iloc[:-96]\n",
    "\n",
    "    elif is_train==False:\n",
    "        \n",
    "        temp = temp[['Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n",
    "                              \n",
    "        return temp.iloc[-48:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train/train.csv')\n",
    "\n",
    "test = []\n",
    "\n",
    "for i in range(81):\n",
    "    file_path = './data/test/' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(file_path)\n",
    "    temp = preprocess_data(temp, is_train=False)\n",
    "    test.append(temp)\n",
    "\n",
    "df_test = pd.concat(test)\n",
    "\n",
    "submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52464, 9), (3888, 7))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = preprocess_data(train)\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = df_train[['Hour','DHI','DNI','WS','RH','T']].min()\n",
    "max  = df_train[['Hour','DHI','DNI','WS','RH','T']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(['Hour','DHI','DNI','WS','RH','T']):\n",
    "    df_train[col] = (df_train[col] - min[i]) / (max[i] - min[i])\n",
    "    df_test[col] = (df_test[col] - min[i]) / (max[i] - min[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Day  = df_train.iloc[:, :-2]\n",
    "Day7 = df_train.iloc[:, -2]\n",
    "Day8 = df_train.iloc[:, -1]\n",
    "Day78 = df_train.iloc[:, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(q, y, pred):\n",
    "    err = (y-pred)\n",
    "    return mean(maximum(q*err, (q-1)*err), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_lst = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39348, 7), (13116, 7), (39348, 2), (13116, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(Day, Day78, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train_1, X_valid_1, Y_train_1, Y_valid_1 = train_test_split(Day, Day7, test_size=0.25, random_state=42)\n",
    "X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(Day, Day8, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import mean, maximum\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(Day.shape)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "581/615 [===========================>..] - ETA: 0s - loss: 378.3369WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 370.4143 - val_loss: 175.5633\n",
      "Epoch 2/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 169.9044 - val_loss: 157.3284\n",
      "Epoch 3/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 152.4468 - val_loss: 149.5892\n",
      "Epoch 4/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 153.5761 - val_loss: 147.8619\n",
      "Epoch 5/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 147.7484 - val_loss: 147.8122\n",
      "Epoch 6/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 149.6038 - val_loss: 151.3657\n",
      "Epoch 7/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 149.6172 - val_loss: 147.0415\n",
      "Epoch 8/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 142.2509 - val_loss: 152.3711\n",
      "Epoch 9/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 149.2954 - val_loss: 147.6583\n",
      "Epoch 10/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 148.8397 - val_loss: 142.7948\n",
      "Epoch 11/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 152.9332 - val_loss: 147.6022\n",
      "Epoch 12/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 145.1340 - val_loss: 142.4872\n",
      "Epoch 13/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 143.8952 - val_loss: 140.3544\n",
      "Epoch 14/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 145.1227 - val_loss: 139.6483\n",
      "Epoch 15/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 138.3808 - val_loss: 139.4052\n",
      "Epoch 16/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 144.1668 - val_loss: 138.1532\n",
      "Epoch 17/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 145.2854 - val_loss: 139.9868\n",
      "Epoch 18/100\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 138.7452 - val_loss: 138.5823\n",
      "Epoch 19/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 140.9201 - val_loss: 135.6353\n",
      "Epoch 20/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 134.4722 - val_loss: 136.5116\n",
      "Epoch 21/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 138.2686 - val_loss: 146.1366\n",
      "Epoch 22/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 140.0425 - val_loss: 135.1114\n",
      "Epoch 23/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 138.0929 - val_loss: 143.9767\n",
      "Epoch 24/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 137.3309 - val_loss: 132.7405\n",
      "Epoch 25/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 134.1046 - val_loss: 132.9609\n",
      "Epoch 26/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 138.0068 - val_loss: 137.5400\n",
      "Epoch 27/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 139.6765 - val_loss: 131.6952\n",
      "Epoch 28/100\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 133.0996 - val_loss: 134.0677\n",
      "Epoch 29/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 136.2175 - val_loss: 132.8486\n",
      "Epoch 30/100\n",
      "615/615 [==============================] - 1s 1ms/step - loss: 135.3982 - val_loss: 138.0644\n",
      "Epoch 00030: early stopping\n",
      "410/410 [==============================] - 0s 749us/step - loss: 136.3020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136.302001953125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam')\n",
    "hist = model.fit(X_train, Y_train, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "model.evaluate(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'], 'b-', label='loss')\n",
    "plt.plot(hist.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "797/820 [============================>.] - ETA: 0s - loss: 1.4493WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 1.4483 - val_loss: 1.5774\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.4007 - val_loss: 1.5916\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3897 - val_loss: 1.5876\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3827 - val_loss: 1.5768\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3955 - val_loss: 1.5852\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3861 - val_loss: 1.5943\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3928 - val_loss: 1.5911\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "785/820 [===========================>..] - ETA: 0s - loss: 2.2657WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.2653 - val_loss: 2.5861\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2413 - val_loss: 2.5794\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2416 - val_loss: 2.5821\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2297 - val_loss: 2.5598\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2480 - val_loss: 2.5730\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2415 - val_loss: 2.5461\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2533 - val_loss: 2.5711\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2619 - val_loss: 2.6094\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.2419 - val_loss: 2.5618\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "772/820 [===========================>..] - ETA: 0s - loss: 2.6851WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.6842 - val_loss: 3.0570\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6568 - val_loss: 3.0701\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6566 - val_loss: 3.0830\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6403 - val_loss: 3.0327\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6642 - val_loss: 3.0904\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6550 - val_loss: 3.0508\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6795 - val_loss: 3.0299\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6899 - val_loss: 3.0605\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6647 - val_loss: 3.0359\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6957 - val_loss: 3.0107\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6120 - val_loss: 3.0242\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6271 - val_loss: 3.0407\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6516 - val_loss: 3.0984\n",
      "Epoch 00013: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "776/820 [===========================>..] - ETA: 0s - loss: 2.7868WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.7859 - val_loss: 3.1404\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7428 - val_loss: 3.1617\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7569 - val_loss: 3.1800\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 1s 1ms/step - loss: 2.7427 - val_loss: 3.1560\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "789/820 [===========================>..] - ETA: 0s - loss: 2.6505WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 2s 1ms/step - loss: 2.6503 - val_loss: 3.0179\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6172 - val_loss: 3.0192\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6241 - val_loss: 2.9968\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6248 - val_loss: 3.0859\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6254 - val_loss: 3.0436\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.6226 - val_loss: 2.9981\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "816/820 [============================>.] - ETA: 0s - loss: 2.3403WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3403 - val_loss: 2.6802\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3168 - val_loss: 2.6758\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3281 - val_loss: 2.7223\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3261 - val_loss: 2.7081\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3309 - val_loss: 2.6713\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3237 - val_loss: 2.6847\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3566 - val_loss: 2.7002\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 2.3766 - val_loss: 2.6867\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "771/820 [===========================>..] - ETA: 0s - loss: 1.9129WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9130 - val_loss: 2.2169\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.8985 - val_loss: 2.2082\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.8985 - val_loss: 2.1915\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9067 - val_loss: 2.2184\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9036 - val_loss: 2.1809\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.8961 - val_loss: 2.1545\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9259 - val_loss: 2.2083\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.9385 - val_loss: 2.2027\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.8867 - val_loss: 2.2048\n",
      "Epoch 00009: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "813/820 [============================>.] - ETA: 0s - loss: 1.3877WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3878 - val_loss: 1.5924\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3854 - val_loss: 1.6155\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3834 - val_loss: 1.5800\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3840 - val_loss: 1.5952\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3817 - val_loss: 1.6248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 1.3809 - val_loss: 1.5889\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "814/820 [============================>.] - ETA: 0s - loss: 0.7780WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7780 - val_loss: 0.8960\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7671 - val_loss: 0.9577\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7748 - val_loss: 0.8861\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7689 - val_loss: 0.9138\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7690 - val_loss: 0.8798\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7647 - val_loss: 0.9059\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7790 - val_loss: 0.9289\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 1ms/step - loss: 0.7827 - val_loss: 0.8960\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 52464, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52464, 7), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3888, 18)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    model.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model.fit(Day, Day78, epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred = pd.DataFrame(model.predict(df_test))\n",
    "    results = pd.concat([results, pred], axis=1)\n",
    "\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34889\n",
      "Early stopping, best iteration is:\n",
      "[418]\tvalid_0's quantile: 1.34812\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.14466\n",
      "[1000]\tvalid_0's quantile: 2.13764\n",
      "[1500]\tvalid_0's quantile: 2.13582\n",
      "[2000]\tvalid_0's quantile: 2.1334\n",
      "Early stopping, best iteration is:\n",
      "[1749]\tvalid_0's quantile: 2.13312\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.53565\n",
      "[1000]\tvalid_0's quantile: 2.50726\n",
      "[1500]\tvalid_0's quantile: 2.49215\n",
      "Early stopping, best iteration is:\n",
      "[1604]\tvalid_0's quantile: 2.48959\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.66191\n",
      "[1000]\tvalid_0's quantile: 2.62846\n",
      "[1500]\tvalid_0's quantile: 2.61266\n",
      "[2000]\tvalid_0's quantile: 2.6059\n",
      "[2500]\tvalid_0's quantile: 2.59923\n",
      "[3000]\tvalid_0's quantile: 2.59644\n",
      "Early stopping, best iteration is:\n",
      "[2707]\tvalid_0's quantile: 2.59598\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.55537\n",
      "[1000]\tvalid_0's quantile: 2.54183\n",
      "[1500]\tvalid_0's quantile: 2.52395\n",
      "[2000]\tvalid_0's quantile: 2.5191\n",
      "[2500]\tvalid_0's quantile: 2.51606\n",
      "[3000]\tvalid_0's quantile: 2.51386\n",
      "[3500]\tvalid_0's quantile: 2.5086\n",
      "[4000]\tvalid_0's quantile: 2.50447\n",
      "[4500]\tvalid_0's quantile: 2.50257\n",
      "[5000]\tvalid_0's quantile: 2.50037\n",
      "[5500]\tvalid_0's quantile: 2.49801\n",
      "[6000]\tvalid_0's quantile: 2.49694\n",
      "[6500]\tvalid_0's quantile: 2.49596\n",
      "Early stopping, best iteration is:\n",
      "[6563]\tvalid_0's quantile: 2.49582\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.28838\n",
      "[1000]\tvalid_0's quantile: 2.26798\n",
      "[1500]\tvalid_0's quantile: 2.25775\n",
      "[2000]\tvalid_0's quantile: 2.25273\n",
      "[2500]\tvalid_0's quantile: 2.24936\n",
      "[3000]\tvalid_0's quantile: 2.24642\n",
      "[3500]\tvalid_0's quantile: 2.24451\n",
      "[4000]\tvalid_0's quantile: 2.24427\n",
      "Early stopping, best iteration is:\n",
      "[3735]\tvalid_0's quantile: 2.24391\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.87856\n",
      "[1000]\tvalid_0's quantile: 1.86236\n",
      "[1500]\tvalid_0's quantile: 1.85737\n",
      "[2000]\tvalid_0's quantile: 1.85491\n",
      "Early stopping, best iteration is:\n",
      "[1840]\tvalid_0's quantile: 1.85404\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.34949\n",
      "[1000]\tvalid_0's quantile: 1.34333\n",
      "Early stopping, best iteration is:\n",
      "[898]\tvalid_0's quantile: 1.34271\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.748569\n",
      "[1000]\tvalid_0's quantile: 0.746233\n",
      "[1500]\tvalid_0's quantile: 0.74548\n",
      "Early stopping, best iteration is:\n",
      "[1284]\tvalid_0's quantile: 0.745153\n",
      "0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.39392\n",
      "Early stopping, best iteration is:\n",
      "[331]\tvalid_0's quantile: 1.3933\n",
      "0.2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.19349\n",
      "Early stopping, best iteration is:\n",
      "[616]\tvalid_0's quantile: 2.18577\n",
      "0.3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.61892\n",
      "[1000]\tvalid_0's quantile: 2.57944\n",
      "[1500]\tvalid_0's quantile: 2.578\n",
      "Early stopping, best iteration is:\n",
      "[1380]\tvalid_0's quantile: 2.57785\n",
      "0.4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.7433\n",
      "[1000]\tvalid_0's quantile: 2.70695\n",
      "[1500]\tvalid_0's quantile: 2.70092\n",
      "[2000]\tvalid_0's quantile: 2.69464\n",
      "[2500]\tvalid_0's quantile: 2.68927\n",
      "[3000]\tvalid_0's quantile: 2.68204\n",
      "[3500]\tvalid_0's quantile: 2.67218\n",
      "[4000]\tvalid_0's quantile: 2.66515\n",
      "[4500]\tvalid_0's quantile: 2.66079\n",
      "Early stopping, best iteration is:\n",
      "[4475]\tvalid_0's quantile: 2.66079\n",
      "0.5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.6458\n",
      "[1000]\tvalid_0's quantile: 2.62814\n",
      "[1500]\tvalid_0's quantile: 2.61626\n",
      "[2000]\tvalid_0's quantile: 2.60023\n",
      "[2500]\tvalid_0's quantile: 2.58706\n",
      "[3000]\tvalid_0's quantile: 2.58403\n",
      "Early stopping, best iteration is:\n",
      "[2807]\tvalid_0's quantile: 2.58403\n",
      "0.6\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 2.36786\n",
      "[1000]\tvalid_0's quantile: 2.35343\n",
      "[1500]\tvalid_0's quantile: 2.33476\n",
      "[2000]\tvalid_0's quantile: 2.32426\n",
      "[2500]\tvalid_0's quantile: 2.31984\n",
      "[3000]\tvalid_0's quantile: 2.31736\n",
      "Early stopping, best iteration is:\n",
      "[2871]\tvalid_0's quantile: 2.31729\n",
      "0.7\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.96038\n",
      "[1000]\tvalid_0's quantile: 1.93778\n",
      "[1500]\tvalid_0's quantile: 1.92682\n",
      "[2000]\tvalid_0's quantile: 1.91583\n",
      "[2500]\tvalid_0's quantile: 1.91245\n",
      "Early stopping, best iteration is:\n",
      "[2560]\tvalid_0's quantile: 1.91213\n",
      "0.8\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 1.40785\n",
      "[1000]\tvalid_0's quantile: 1.39944\n",
      "[1500]\tvalid_0's quantile: 1.39438\n",
      "[2000]\tvalid_0's quantile: 1.39224\n",
      "[2500]\tvalid_0's quantile: 1.39124\n",
      "Early stopping, best iteration is:\n",
      "[2308]\tvalid_0's quantile: 1.39107\n",
      "0.9\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[500]\tvalid_0's quantile: 0.780444\n",
      "[1000]\tvalid_0's quantile: 0.777171\n",
      "[1500]\tvalid_0's quantile: 0.775623\n",
      "[2000]\tvalid_0's quantile: 0.775235\n",
      "Early stopping, best iteration is:\n",
      "[1831]\tvalid_0's quantile: 0.774894\n"
     ]
    }
   ],
   "source": [
    "def LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "    \n",
    "    # (a) Modeling  \n",
    "    model = LGBMRegressor(objective='quantile', alpha=q,\n",
    "                         n_estimators=10000, bagging_fraction=0.7, learning_rate=0.027, subsample=0.7)                   \n",
    "                         \n",
    "                         \n",
    "    model.fit(X_train, Y_train, eval_metric = ['quantile'], \n",
    "          eval_set=[(X_valid, Y_valid)], early_stopping_rounds=300, verbose=500)\n",
    "\n",
    "    # (b) Predictions\n",
    "    pred = pd.Series(model.predict(X_test).round(2))\n",
    "    return pred, model\n",
    "\n",
    "def train_data(X_train, Y_train, X_valid, Y_valid, X_test):\n",
    "\n",
    "    LGBM_models=[]\n",
    "    LGBM_actual_pred = pd.DataFrame()\n",
    "\n",
    "    for q in q_lst:\n",
    "        print(q)\n",
    "        pred , model = LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test)\n",
    "        LGBM_models.append(model)\n",
    "        LGBM_actual_pred = pd.concat([LGBM_actual_pred,pred],axis=1)\n",
    "\n",
    "    LGBM_actual_pred.columns=q_lst\n",
    "    \n",
    "    return LGBM_models, LGBM_actual_pred\n",
    "\n",
    "models_1, results_1 = train_data(X_train_1, Y_train_1, X_valid_1, Y_valid_1, df_test)\n",
    "models_2, results_2 = train_data(X_train_2, Y_train_2, X_valid_2, Y_valid_2, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.06666667, 0.79352884,\n",
       "        0.3       ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.075     , 0.80012986,\n",
       "        0.29814815],\n",
       "       [0.04347826, 0.        , 0.        , ..., 0.08333333, 0.78259929,\n",
       "        0.2962963 ],\n",
       "       ...,\n",
       "       [0.95652174, 0.        , 0.        , ..., 0.05833333, 0.63315658,\n",
       "        0.58888889],\n",
       "       [1.        , 0.        , 0.        , ..., 0.05      , 0.64982145,\n",
       "        0.58148148],\n",
       "       [1.        , 0.        , 0.        , ..., 0.05      , 0.66929986,\n",
       "        0.57407407]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(np.array(df_test).reshape(3888, 7, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4352 - val_loss: 1.6001\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4210 - val_loss: 1.5990\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4133 - val_loss: 1.5978\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4169 - val_loss: 1.5973\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4245 - val_loss: 1.6026\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4155 - val_loss: 1.6114\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4168 - val_loss: 1.6229\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4333 - val_loss: 1.5967\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4212 - val_loss: 1.5977\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4344 - val_loss: 1.5949\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4037 - val_loss: 1.5973\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4045 - val_loss: 1.6052\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4141 - val_loss: 1.5923\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3904 - val_loss: 1.5913\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4230 - val_loss: 1.5861\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4168 - val_loss: 1.5852\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4133 - val_loss: 1.5943\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4070 - val_loss: 1.5921\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4084 - val_loss: 1.5814\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4041 - val_loss: 1.5806\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4116 - val_loss: 1.5983\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3786 - val_loss: 1.5820\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3810 - val_loss: 1.5985\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3964 - val_loss: 1.6067\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 2s 3ms/step - loss: 1.3821 - val_loss: 1.5782\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3752 - val_loss: 1.5739\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3892 - val_loss: 1.5908\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3821 - val_loss: 1.5879\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3679 - val_loss: 1.5794\n",
      "Epoch 30/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3761 - val_loss: 1.5771\n",
      "Epoch 31/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3882 - val_loss: 1.5662\n",
      "Epoch 32/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3860 - val_loss: 1.5744\n",
      "Epoch 33/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3651 - val_loss: 1.5708\n",
      "Epoch 34/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3698 - val_loss: 1.6125\n",
      "Epoch 35/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3750 - val_loss: 1.5585\n",
      "Epoch 36/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3900 - val_loss: 1.5754\n",
      "Epoch 37/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3700 - val_loss: 1.5835\n",
      "Epoch 38/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3984 - val_loss: 1.5968\n",
      "Epoch 39/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3885 - val_loss: 1.5658\n",
      "Epoch 40/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3723 - val_loss: 1.5591\n",
      "Epoch 00040: early stopping\n",
      "0.2\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1986 - val_loss: 2.5244\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.1877 - val_loss: 2.5351\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1826 - val_loss: 2.5213\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1643 - val_loss: 2.5160\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1876 - val_loss: 2.5294\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1855 - val_loss: 2.5514\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.1920 - val_loss: 2.5345\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2113 - val_loss: 2.5256\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.1806 - val_loss: 2.5081\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2133 - val_loss: 2.5096\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.1495 - val_loss: 2.5167\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.1388 - val_loss: 2.5098\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.1808 - val_loss: 2.5176\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.1807 - val_loss: 2.5280\n",
      "Epoch 00014: early stopping\n",
      "0.3\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5943 - val_loss: 2.9485\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5752 - val_loss: 2.9549\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5722 - val_loss: 2.9500\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5456 - val_loss: 2.9688\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5828 - val_loss: 2.9719\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5805 - val_loss: 2.9601\n",
      "Epoch 00006: early stopping\n",
      "0.4\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7021 - val_loss: 3.0577\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6730 - val_loss: 3.0713\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6743 - val_loss: 3.1064\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6488 - val_loss: 3.0718\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6840 - val_loss: 3.0866\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6914 - val_loss: 3.0985\n",
      "Epoch 00006: early stopping\n",
      "0.5\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5857 - val_loss: 2.9167\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5582 - val_loss: 2.9072\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5564 - val_loss: 2.9249\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5384 - val_loss: 2.9297\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5609 - val_loss: 2.9730\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5791 - val_loss: 2.9570\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.5902 - val_loss: 2.8967\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6211 - val_loss: 2.9491\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5551 - val_loss: 2.9263\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6126 - val_loss: 2.8973\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4970 - val_loss: 2.9216\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.5072 - val_loss: 2.9575\n",
      "Epoch 00012: early stopping\n",
      "0.6\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2921 - val_loss: 2.6501\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2607 - val_loss: 2.6285\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2568 - val_loss: 2.6953\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2489 - val_loss: 2.6623\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2628 - val_loss: 2.6477\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2827 - val_loss: 2.5939\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2884 - val_loss: 2.5954\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3200 - val_loss: 2.6656\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2611 - val_loss: 2.6261\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3143 - val_loss: 2.5920\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2145 - val_loss: 2.6167\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2195 - val_loss: 2.6442\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2693 - val_loss: 2.6234\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2528 - val_loss: 2.6090\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2446 - val_loss: 2.6010\n",
      "Epoch 00015: early stopping\n",
      "0.7\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8659 - val_loss: 2.1685\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8514 - val_loss: 2.1871\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8486 - val_loss: 2.1821\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8452 - val_loss: 2.1664\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8505 - val_loss: 2.1659\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8686 - val_loss: 2.1249\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8737 - val_loss: 2.1484\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9042 - val_loss: 2.1207\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8452 - val_loss: 2.1859\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.8931 - val_loss: 2.1112\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8106 - val_loss: 2.1346\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8306 - val_loss: 2.1856\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8555 - val_loss: 2.1703\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8466 - val_loss: 2.1246\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.8395 - val_loss: 2.1401\n",
      "Epoch 00015: early stopping\n",
      "0.8\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3619 - val_loss: 1.5558\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3365 - val_loss: 1.5634\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3552 - val_loss: 1.6060\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3491 - val_loss: 1.5664\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3469 - val_loss: 1.6186\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3598 - val_loss: 1.5447\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3643 - val_loss: 1.5683\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3845 - val_loss: 1.5386\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3357 - val_loss: 1.5598\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3789 - val_loss: 1.5380\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3151 - val_loss: 1.5347\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3361 - val_loss: 1.5591\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3401 - val_loss: 1.5744\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3414 - val_loss: 1.5392\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3373 - val_loss: 1.5425\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.3407 - val_loss: 1.5371\n",
      "Epoch 00016: early stopping\n",
      "0.9\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7628 - val_loss: 0.8643\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7468 - val_loss: 0.8624\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7480 - val_loss: 0.8982\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7489 - val_loss: 0.8678\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7456 - val_loss: 0.8739\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7542 - val_loss: 0.8667\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7568 - val_loss: 0.8589\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7608 - val_loss: 0.8709\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7388 - val_loss: 0.8641\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7622 - val_loss: 0.8424\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7282 - val_loss: 0.8455\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7437 - val_loss: 0.8625\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7512 - val_loss: 0.8864\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7455 - val_loss: 0.8502\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7352 - val_loss: 0.8681\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3888, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result7 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model7.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model7.fit(np.array(Day).reshape(52464, 1, 7), np.array(Day7).reshape(52464, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred7 = np.squeeze(model7.predict(np.array(df_test).reshape(3888, 1, 7)))\n",
    "    pred7 = pd.DataFrame(pred7)\n",
    "    result7 = pd.concat([result7, pred7], axis=1)\n",
    "    \n",
    "result7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4554 - val_loss: 1.6486\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4449 - val_loss: 1.6423\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4343 - val_loss: 1.6420\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4265 - val_loss: 1.6404\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4429 - val_loss: 1.6405\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4351 - val_loss: 1.6465\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4443 - val_loss: 1.6487\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4513 - val_loss: 1.6428\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4519 - val_loss: 1.6413\n",
      "Epoch 00009: early stopping\n",
      "0.2\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4134 - val_loss: 2.7459\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3821 - val_loss: 2.7428\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3703 - val_loss: 2.7320\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3590 - val_loss: 2.7315\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3745 - val_loss: 2.7226\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3627 - val_loss: 2.7335\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3815 - val_loss: 2.7135\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3824 - val_loss: 2.7215\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3752 - val_loss: 2.7223\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3941 - val_loss: 2.6942\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3348 - val_loss: 2.7225\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3568 - val_loss: 2.6996\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3541 - val_loss: 2.6885\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3265 - val_loss: 2.7084\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3565 - val_loss: 2.6865\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3365 - val_loss: 2.6840\n",
      "Epoch 17/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3405 - val_loss: 2.7029\n",
      "Epoch 18/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3532 - val_loss: 2.6798\n",
      "Epoch 19/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.3371 - val_loss: 2.6900\n",
      "Epoch 20/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3318 - val_loss: 2.6876\n",
      "Epoch 21/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3293 - val_loss: 2.6816\n",
      "Epoch 22/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3238 - val_loss: 2.6683\n",
      "Epoch 23/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3052 - val_loss: 2.6946\n",
      "Epoch 24/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3321 - val_loss: 2.6527\n",
      "Epoch 25/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3250 - val_loss: 2.6836\n",
      "Epoch 26/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.2987 - val_loss: 2.6612\n",
      "Epoch 27/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3223 - val_loss: 2.6680\n",
      "Epoch 28/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3144 - val_loss: 2.6742\n",
      "Epoch 29/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.2931 - val_loss: 2.6677\n",
      "Epoch 00029: early stopping\n",
      "0.3\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8196 - val_loss: 3.1545\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7936 - val_loss: 3.2312\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7783 - val_loss: 3.2083\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7804 - val_loss: 3.1506\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7826 - val_loss: 3.2645\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7884 - val_loss: 3.1290\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8029 - val_loss: 3.2135\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8024 - val_loss: 3.1319\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7946 - val_loss: 3.1802\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8108 - val_loss: 3.1318\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7432 - val_loss: 3.1172\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7704 - val_loss: 3.1570\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7740 - val_loss: 3.1293\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7395 - val_loss: 3.1250\n",
      "Epoch 15/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7744 - val_loss: 3.1405\n",
      "Epoch 16/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7435 - val_loss: 3.1748\n",
      "Epoch 00016: early stopping\n",
      "0.4\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8978 - val_loss: 3.2265\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8889 - val_loss: 3.3189\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8776 - val_loss: 3.2509\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.8726 - val_loss: 3.2411\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8872 - val_loss: 3.3243\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.8609 - val_loss: 3.2320\n",
      "Epoch 00006: early stopping\n",
      "0.5\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7505 - val_loss: 3.1169\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7430 - val_loss: 3.1196\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7439 - val_loss: 3.1005\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7337 - val_loss: 3.1876\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7500 - val_loss: 3.1717\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7119 - val_loss: 3.0957\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7639 - val_loss: 3.1185\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7540 - val_loss: 3.1188\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7266 - val_loss: 3.0680\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7608 - val_loss: 3.0763\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.6944 - val_loss: 3.0765\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7201 - val_loss: 3.1018\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.7399 - val_loss: 3.0926\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.7228 - val_loss: 3.1464\n",
      "Epoch 00014: early stopping\n",
      "0.6\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4147 - val_loss: 2.7284\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4113 - val_loss: 2.7458\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4095 - val_loss: 2.7866\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4230 - val_loss: 2.7823\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4200 - val_loss: 2.7269\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3878 - val_loss: 2.7445\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4258 - val_loss: 2.7309\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 2.4239 - val_loss: 2.7424\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3941 - val_loss: 2.7217\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4216 - val_loss: 2.7559\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.3747 - val_loss: 2.7547\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4062 - val_loss: 2.7550\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4152 - val_loss: 2.7915\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 2.4064 - val_loss: 2.7985\n",
      "Epoch 00014: early stopping\n",
      "0.7\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9647 - val_loss: 2.2355\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9699 - val_loss: 2.2333\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9609 - val_loss: 2.2331\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9738 - val_loss: 2.2507\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9716 - val_loss: 2.2191\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9439 - val_loss: 2.2186\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9727 - val_loss: 2.2434\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9829 - val_loss: 2.2573\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9489 - val_loss: 2.2112\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9775 - val_loss: 2.2259\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9276 - val_loss: 2.2123\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.9620 - val_loss: 2.2360\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9626 - val_loss: 2.3063\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.9724 - val_loss: 2.2629\n",
      "Epoch 00014: early stopping\n",
      "0.8\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4320 - val_loss: 1.6330\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4306 - val_loss: 1.6301\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 1.4281 - val_loss: 1.6550\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4431 - val_loss: 1.5988\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4264 - val_loss: 1.6107\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4097 - val_loss: 1.6374\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4345 - val_loss: 1.6455\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4341 - val_loss: 1.6225\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4142 - val_loss: 1.5883\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4316 - val_loss: 1.5976\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.3929 - val_loss: 1.6046\n",
      "Epoch 12/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4129 - val_loss: 1.5923\n",
      "Epoch 13/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4118 - val_loss: 1.6206\n",
      "Epoch 14/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 1.4287 - val_loss: 1.6241\n",
      "Epoch 00014: early stopping\n",
      "0.9\n",
      "Epoch 1/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.8045 - val_loss: 0.8836\n",
      "Epoch 2/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.8038 - val_loss: 0.9061\n",
      "Epoch 3/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7938 - val_loss: 0.9004\n",
      "Epoch 4/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7960 - val_loss: 0.8857\n",
      "Epoch 5/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7987 - val_loss: 0.8854\n",
      "Epoch 6/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.7857 - val_loss: 0.8828\n",
      "Epoch 7/100\n",
      "820/820 [==============================] - 2s 2ms/step - loss: 0.8003 - val_loss: 0.9314\n",
      "Epoch 8/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.8038 - val_loss: 0.9122\n",
      "Epoch 9/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7860 - val_loss: 0.8924\n",
      "Epoch 10/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7970 - val_loss: 0.8920\n",
      "Epoch 11/100\n",
      "820/820 [==============================] - 1s 2ms/step - loss: 0.7749 - val_loss: 0.9102\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3888, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8 = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "result8 = pd.DataFrame()\n",
    "\n",
    "for q in q_lst:\n",
    "    print(q)\n",
    "    model8.compile(loss=lambda y,pred: quantile_loss(q,y,pred), optimizer='adam')\n",
    "    model8.fit(np.array(Day).reshape(52464, 1, 7), np.array(Day8).reshape(52464, 1), epochs=epoch, batch_size=48, validation_split=0.25, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, mode='min', monitor='val_loss', verbose=1)])\n",
    "    pred8 = np.squeeze(model8.predict(np.array(df_test).reshape(3888, 1, 7)))\n",
    "    pred8 = pd.DataFrame(pred8)\n",
    "    result8 = pd.concat([result8, pred8], axis=1)\n",
    "    \n",
    "result8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_L0 = pd.DataFrame(results_1.sort_index())\n",
    "res_L0.columns = ['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']\n",
    "res_L1 = pd.DataFrame(results_1.sort_index())\n",
    "res_L1.columns = ['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']\n",
    "\n",
    "res_D0 = pd.DataFrame(results[0].sort_index())\n",
    "res_D0.columns = ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9']\n",
    "res_D1 = pd.DataFrame(results[1].sort_index())\n",
    "res_D1.columns = ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9']\n",
    "\n",
    "res_C0 = pd.DataFrame(result7.sort_index())\n",
    "res_C0.columns = ['C00.1','C00.2','C00.3','C00.4','C00.5','C00.6','C00.7','C00.8','C00.9']\n",
    "res_C1 = pd.DataFrame(result8.sort_index())\n",
    "res_C1.columns = ['C10.1','C10.2','C10.3','C10.4','C10.5','C10.6','C10.7','C10.8','C10.9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0 = pd.concat([res_L0, res_D0, res_C0], axis=1)\n",
    "res_1 = pd.concat([res_L1, res_D1, res_C1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0.loc[res_0[res_0['L00.1'] == 0].index, ['D00.1','D00.2','D00.3','D00.4','D00.5','D00.6','D00.7','D00.8','D00.9'\n",
    "                                            ,'C00.1','C00.2','C00.3','C00.4','C00.5','C00.6','C00.7','C00.8','C00.9']] = 0\n",
    "res_1.loc[res_1[res_1['L10.1'] == 0].index, ['D10.1','D10.2','D10.3','D10.4','D10.5','D10.6','D10.7','D10.8','D10.9'\n",
    "                                            ,'C10.1','C10.2','C10.3','C10.4','C10.5','C10.6','C10.7','C10.8','C10.9']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    res_0[\"L00.\"+str(i)] = 0.25*res_0[\"L00.\"+str(i)] + 0.25*res_0[\"D00.\"+str(i)] + 0.5*res_0[\"C00.\"+str(i)]\n",
    "    res_1[\"L10.\"+str(i)] = 0.25*res_1[\"L10.\"+str(i)] + 0.25*res_1[\"D10.\"+str(i)] + 0.5*res_1[\"C10.\"+str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.csv_Day7_2h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.csv_Day7_3h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.csv_Day7_3h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.csv_Day7_4h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.csv_Day7_4h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.csv_Day7_5h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.csv_Day7_5h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.csv_Day7_6h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.csv_Day7_6h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.csv_Day7_7h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.csv_Day7_7h30m</td>\n",
       "      <td>0.418084</td>\n",
       "      <td>1.311757</td>\n",
       "      <td>1.364666</td>\n",
       "      <td>1.894864</td>\n",
       "      <td>3.088984</td>\n",
       "      <td>3.181783</td>\n",
       "      <td>4.645184</td>\n",
       "      <td>5.733191</td>\n",
       "      <td>8.071223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.csv_Day7_8h00m</td>\n",
       "      <td>1.294383</td>\n",
       "      <td>4.418589</td>\n",
       "      <td>5.263295</td>\n",
       "      <td>6.109350</td>\n",
       "      <td>8.502349</td>\n",
       "      <td>8.924711</td>\n",
       "      <td>11.346642</td>\n",
       "      <td>14.984281</td>\n",
       "      <td>19.015376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.csv_Day7_8h30m</td>\n",
       "      <td>1.939189</td>\n",
       "      <td>6.458027</td>\n",
       "      <td>7.722973</td>\n",
       "      <td>7.419136</td>\n",
       "      <td>10.342248</td>\n",
       "      <td>11.944032</td>\n",
       "      <td>14.965120</td>\n",
       "      <td>18.708272</td>\n",
       "      <td>23.401561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.csv_Day7_9h00m</td>\n",
       "      <td>5.702013</td>\n",
       "      <td>14.419922</td>\n",
       "      <td>18.340764</td>\n",
       "      <td>17.264282</td>\n",
       "      <td>19.099977</td>\n",
       "      <td>20.951260</td>\n",
       "      <td>22.443982</td>\n",
       "      <td>27.215803</td>\n",
       "      <td>34.404105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.csv_Day7_9h30m</td>\n",
       "      <td>7.420425</td>\n",
       "      <td>17.144545</td>\n",
       "      <td>20.198101</td>\n",
       "      <td>20.154273</td>\n",
       "      <td>21.563439</td>\n",
       "      <td>23.021558</td>\n",
       "      <td>24.521596</td>\n",
       "      <td>28.064965</td>\n",
       "      <td>34.690063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.csv_Day7_10h00m</td>\n",
       "      <td>12.584249</td>\n",
       "      <td>24.979677</td>\n",
       "      <td>29.086518</td>\n",
       "      <td>30.017369</td>\n",
       "      <td>31.580824</td>\n",
       "      <td>34.816670</td>\n",
       "      <td>34.062439</td>\n",
       "      <td>33.581355</td>\n",
       "      <td>35.741979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.csv_Day7_10h30m</td>\n",
       "      <td>13.439520</td>\n",
       "      <td>24.798394</td>\n",
       "      <td>31.048225</td>\n",
       "      <td>31.025757</td>\n",
       "      <td>33.298878</td>\n",
       "      <td>36.880938</td>\n",
       "      <td>36.722391</td>\n",
       "      <td>36.687286</td>\n",
       "      <td>36.637264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.csv_Day7_11h00m</td>\n",
       "      <td>15.983388</td>\n",
       "      <td>30.086477</td>\n",
       "      <td>37.146679</td>\n",
       "      <td>38.580327</td>\n",
       "      <td>37.228494</td>\n",
       "      <td>41.613035</td>\n",
       "      <td>41.115335</td>\n",
       "      <td>41.716615</td>\n",
       "      <td>45.725196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.csv_Day7_11h30m</td>\n",
       "      <td>13.885817</td>\n",
       "      <td>27.490487</td>\n",
       "      <td>33.469628</td>\n",
       "      <td>34.610284</td>\n",
       "      <td>33.499061</td>\n",
       "      <td>36.756454</td>\n",
       "      <td>38.437174</td>\n",
       "      <td>37.104394</td>\n",
       "      <td>45.407922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.csv_Day7_12h00m</td>\n",
       "      <td>14.476800</td>\n",
       "      <td>27.586202</td>\n",
       "      <td>35.078012</td>\n",
       "      <td>36.883697</td>\n",
       "      <td>34.247381</td>\n",
       "      <td>37.041204</td>\n",
       "      <td>36.466443</td>\n",
       "      <td>36.598102</td>\n",
       "      <td>41.968743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.csv_Day7_12h30m</td>\n",
       "      <td>15.505414</td>\n",
       "      <td>28.544168</td>\n",
       "      <td>35.886980</td>\n",
       "      <td>38.566766</td>\n",
       "      <td>34.270588</td>\n",
       "      <td>40.140585</td>\n",
       "      <td>39.659719</td>\n",
       "      <td>39.379101</td>\n",
       "      <td>42.591849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.csv_Day7_13h00m</td>\n",
       "      <td>14.209535</td>\n",
       "      <td>25.178239</td>\n",
       "      <td>32.245551</td>\n",
       "      <td>36.582291</td>\n",
       "      <td>31.809328</td>\n",
       "      <td>36.824450</td>\n",
       "      <td>36.893236</td>\n",
       "      <td>36.462261</td>\n",
       "      <td>40.224149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.csv_Day7_13h30m</td>\n",
       "      <td>11.713607</td>\n",
       "      <td>23.009232</td>\n",
       "      <td>30.418395</td>\n",
       "      <td>33.232037</td>\n",
       "      <td>28.733749</td>\n",
       "      <td>33.910563</td>\n",
       "      <td>35.237796</td>\n",
       "      <td>35.260048</td>\n",
       "      <td>45.590324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.csv_Day7_14h00m</td>\n",
       "      <td>10.502457</td>\n",
       "      <td>20.849943</td>\n",
       "      <td>26.630926</td>\n",
       "      <td>29.778007</td>\n",
       "      <td>25.724869</td>\n",
       "      <td>31.567332</td>\n",
       "      <td>30.833007</td>\n",
       "      <td>30.675536</td>\n",
       "      <td>35.973589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.csv_Day7_14h30m</td>\n",
       "      <td>10.847962</td>\n",
       "      <td>21.424289</td>\n",
       "      <td>26.931615</td>\n",
       "      <td>29.956638</td>\n",
       "      <td>25.349210</td>\n",
       "      <td>30.913241</td>\n",
       "      <td>31.788107</td>\n",
       "      <td>31.302884</td>\n",
       "      <td>35.362538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.csv_Day7_15h00m</td>\n",
       "      <td>8.213556</td>\n",
       "      <td>17.241266</td>\n",
       "      <td>21.805353</td>\n",
       "      <td>24.158136</td>\n",
       "      <td>21.701938</td>\n",
       "      <td>26.179041</td>\n",
       "      <td>27.513475</td>\n",
       "      <td>27.648377</td>\n",
       "      <td>30.084755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.csv_Day7_15h30m</td>\n",
       "      <td>6.822864</td>\n",
       "      <td>15.064578</td>\n",
       "      <td>17.748606</td>\n",
       "      <td>19.220458</td>\n",
       "      <td>16.820853</td>\n",
       "      <td>21.179564</td>\n",
       "      <td>23.019844</td>\n",
       "      <td>22.962294</td>\n",
       "      <td>28.420907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.csv_Day7_16h00m</td>\n",
       "      <td>4.260113</td>\n",
       "      <td>7.667303</td>\n",
       "      <td>9.293912</td>\n",
       "      <td>9.767939</td>\n",
       "      <td>9.401492</td>\n",
       "      <td>12.713008</td>\n",
       "      <td>13.271452</td>\n",
       "      <td>14.438062</td>\n",
       "      <td>18.328138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.csv_Day7_16h30m</td>\n",
       "      <td>1.088928</td>\n",
       "      <td>3.384249</td>\n",
       "      <td>3.753455</td>\n",
       "      <td>4.937511</td>\n",
       "      <td>3.627900</td>\n",
       "      <td>6.633394</td>\n",
       "      <td>9.265448</td>\n",
       "      <td>13.172138</td>\n",
       "      <td>16.112758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.csv_Day7_17h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.csv_Day7_17h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.csv_Day7_18h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.csv_Day7_18h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.csv_Day7_19h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.csv_Day7_19h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.csv_Day7_20h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.csv_Day7_20h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.csv_Day7_21h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.csv_Day7_21h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.csv_Day7_22h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.csv_Day7_22h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.csv_Day7_23h00m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.csv_Day7_23h30m</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id      q_0.1      q_0.2      q_0.3      q_0.4      q_0.5  \\\n",
       "0    0.csv_Day7_0h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1    0.csv_Day7_0h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2    0.csv_Day7_1h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3    0.csv_Day7_1h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4    0.csv_Day7_2h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5    0.csv_Day7_2h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "6    0.csv_Day7_3h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7    0.csv_Day7_3h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "8    0.csv_Day7_4h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9    0.csv_Day7_4h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "10   0.csv_Day7_5h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11   0.csv_Day7_5h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "12   0.csv_Day7_6h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "13   0.csv_Day7_6h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "14   0.csv_Day7_7h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "15   0.csv_Day7_7h30m   0.418084   1.311757   1.364666   1.894864   3.088984   \n",
       "16   0.csv_Day7_8h00m   1.294383   4.418589   5.263295   6.109350   8.502349   \n",
       "17   0.csv_Day7_8h30m   1.939189   6.458027   7.722973   7.419136  10.342248   \n",
       "18   0.csv_Day7_9h00m   5.702013  14.419922  18.340764  17.264282  19.099977   \n",
       "19   0.csv_Day7_9h30m   7.420425  17.144545  20.198101  20.154273  21.563439   \n",
       "20  0.csv_Day7_10h00m  12.584249  24.979677  29.086518  30.017369  31.580824   \n",
       "21  0.csv_Day7_10h30m  13.439520  24.798394  31.048225  31.025757  33.298878   \n",
       "22  0.csv_Day7_11h00m  15.983388  30.086477  37.146679  38.580327  37.228494   \n",
       "23  0.csv_Day7_11h30m  13.885817  27.490487  33.469628  34.610284  33.499061   \n",
       "24  0.csv_Day7_12h00m  14.476800  27.586202  35.078012  36.883697  34.247381   \n",
       "25  0.csv_Day7_12h30m  15.505414  28.544168  35.886980  38.566766  34.270588   \n",
       "26  0.csv_Day7_13h00m  14.209535  25.178239  32.245551  36.582291  31.809328   \n",
       "27  0.csv_Day7_13h30m  11.713607  23.009232  30.418395  33.232037  28.733749   \n",
       "28  0.csv_Day7_14h00m  10.502457  20.849943  26.630926  29.778007  25.724869   \n",
       "29  0.csv_Day7_14h30m  10.847962  21.424289  26.931615  29.956638  25.349210   \n",
       "30  0.csv_Day7_15h00m   8.213556  17.241266  21.805353  24.158136  21.701938   \n",
       "31  0.csv_Day7_15h30m   6.822864  15.064578  17.748606  19.220458  16.820853   \n",
       "32  0.csv_Day7_16h00m   4.260113   7.667303   9.293912   9.767939   9.401492   \n",
       "33  0.csv_Day7_16h30m   1.088928   3.384249   3.753455   4.937511   3.627900   \n",
       "34  0.csv_Day7_17h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "35  0.csv_Day7_17h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "36  0.csv_Day7_18h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "37  0.csv_Day7_18h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "38  0.csv_Day7_19h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "39  0.csv_Day7_19h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "40  0.csv_Day7_20h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "41  0.csv_Day7_20h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "42  0.csv_Day7_21h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "43  0.csv_Day7_21h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "44  0.csv_Day7_22h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "45  0.csv_Day7_22h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "46  0.csv_Day7_23h00m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "47  0.csv_Day7_23h30m   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "        q_0.6      q_0.7      q_0.8      q_0.9  \n",
       "0    0.000000   0.000000   0.000000   0.000000  \n",
       "1    0.000000   0.000000   0.000000   0.000000  \n",
       "2    0.000000   0.000000   0.000000   0.000000  \n",
       "3    0.000000   0.000000   0.000000   0.000000  \n",
       "4    0.000000   0.000000   0.000000   0.000000  \n",
       "5    0.000000   0.000000   0.000000   0.000000  \n",
       "6    0.000000   0.000000   0.000000   0.000000  \n",
       "7    0.000000   0.000000   0.000000   0.000000  \n",
       "8    0.000000   0.000000   0.000000   0.000000  \n",
       "9    0.000000   0.000000   0.000000   0.000000  \n",
       "10   0.000000   0.000000   0.000000   0.000000  \n",
       "11   0.000000   0.000000   0.000000   0.000000  \n",
       "12   0.000000   0.000000   0.000000   0.000000  \n",
       "13   0.000000   0.000000   0.000000   0.000000  \n",
       "14   0.000000   0.000000   0.000000   0.000000  \n",
       "15   3.181783   4.645184   5.733191   8.071223  \n",
       "16   8.924711  11.346642  14.984281  19.015376  \n",
       "17  11.944032  14.965120  18.708272  23.401561  \n",
       "18  20.951260  22.443982  27.215803  34.404105  \n",
       "19  23.021558  24.521596  28.064965  34.690063  \n",
       "20  34.816670  34.062439  33.581355  35.741979  \n",
       "21  36.880938  36.722391  36.687286  36.637264  \n",
       "22  41.613035  41.115335  41.716615  45.725196  \n",
       "23  36.756454  38.437174  37.104394  45.407922  \n",
       "24  37.041204  36.466443  36.598102  41.968743  \n",
       "25  40.140585  39.659719  39.379101  42.591849  \n",
       "26  36.824450  36.893236  36.462261  40.224149  \n",
       "27  33.910563  35.237796  35.260048  45.590324  \n",
       "28  31.567332  30.833007  30.675536  35.973589  \n",
       "29  30.913241  31.788107  31.302884  35.362538  \n",
       "30  26.179041  27.513475  27.648377  30.084755  \n",
       "31  21.179564  23.019844  22.962294  28.420907  \n",
       "32  12.713008  13.271452  14.438062  18.328138  \n",
       "33   6.633394   9.265448  13.172138  16.112758  \n",
       "34   0.000000   0.000000   0.000000   0.000000  \n",
       "35   0.000000   0.000000   0.000000   0.000000  \n",
       "36   0.000000   0.000000   0.000000   0.000000  \n",
       "37   0.000000   0.000000   0.000000   0.000000  \n",
       "38   0.000000   0.000000   0.000000   0.000000  \n",
       "39   0.000000   0.000000   0.000000   0.000000  \n",
       "40   0.000000   0.000000   0.000000   0.000000  \n",
       "41   0.000000   0.000000   0.000000   0.000000  \n",
       "42   0.000000   0.000000   0.000000   0.000000  \n",
       "43   0.000000   0.000000   0.000000   0.000000  \n",
       "44   0.000000   0.000000   0.000000   0.000000  \n",
       "45   0.000000   0.000000   0.000000   0.000000  \n",
       "46   0.000000   0.000000   0.000000   0.000000  \n",
       "47   0.000000   0.000000   0.000000   0.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = res_0[['L00.1','L00.2','L00.3','L00.4','L00.5','L00.6','L00.7','L00.8','L00.9']].values\n",
    "submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = res_1[['L10.1','L10.2','L10.3','L10.4','L10.5','L10.6','L10.7','L10.8','L10.9']].values\n",
    "submission[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission/submission_20210120-3..csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
